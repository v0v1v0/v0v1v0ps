<div class="container">

<table style="width: 100%;"><tr>
<td>MakeForest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create an Rcpp_Forest Object</h2>

<h3>Description</h3>

<p>Make an object of type <code>Rcpp_Fores</code>t, which can be used to embed a soft
BART model into other models. Some examples are given in the package
vignette.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MakeForest(hypers, opts, warn = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>hypers</code></td>
<td>
<p>A list of hyperparameter values obtained from <code>Hypers()</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opts</code></td>
<td>
<p>A list of MCMC chain settings obtained from <code>Opts()</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warn</code></td>
<td>
<p>If <code>TRUE</code>, reminds the user to normalize their design matrix when interacting with a forest object.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns an object of type <code>Rcpp_Forest</code>. If <code>forest</code> is an
<code>Rcpp_Forest</code> object then it has the following methods.
</p>

<ul>
<li> <p><code>forest$do_gibbs(X, Y, X_test, i)</code> runs <code>i</code> iterations of
the Bayesian backfitting algorithm and predicts on the test set
<code>X_test</code>. The state of forest is also updated.
</p>
</li>
<li> <p><code>forest$do_gibbs_weighted(X, Y, weights X_test, i)</code> runs <code>i</code>
iterations of the Bayesian backfitting algorithm and predicts on the test
set <code>X_test</code>; assumes that <code>Y</code> is heteroskedastic with known weights. The state
of forest is also updated.
</p>
</li>
<li> <p><code>forest$do_predict(X)</code> returns the predictions from a matrix <code>X</code>
of predictors.
</p>
</li>
<li> <p><code>forest$get_counts()</code> returns the number of times each variable
has been used in a splitting rule at the current state of <code>forest</code>.
</p>
</li>
<li> <p><code>forest$get_s()</code> returns the splitting probabilities of the
forest.
</p>
</li>
<li> <p><code>forest$get_sigma()</code> returns the error standard deviation of the
forest.
</p>
</li>
<li> <p><code>forest$get_sigma_mu()</code> returns the standard deviation of the
leaf node parameters.
</p>
</li>
<li> <p><code>forest$get_tree_counts()</code> returns a matrix with a column for
each predictor and a row for each tree that counts the number of times each
predictor is used in each tree at the current state of <code>forest</code>.
</p>
</li>
<li> <p><code>forest$predict_iteration(X, i)</code> returns the predictions from a
matrix <code>X</code> of predictors at iteration <code>i</code>. Requires that <code>opts$cache_trees =
  TRUE</code> in <code>MakeForest(hypers, opts)</code>.
</p>
</li>
<li> <p><code>forest$set_s(s)</code> sets the splitting probabilities of the forest
to <code>s</code>.
</p>
</li>
<li> <p><code>forest$set_sigma(x)</code> sets the error standard deviation of the
forest to <code>x</code>.
</p>
</li>
<li> <p><code>forest$num_gibbs</code> returns the number of iterations in total
that the Gibbs sampler has been run.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">
X &lt;- matrix(runif(100 * 10), nrow = 100, ncol = 10)
Y &lt;- rowSums(X) + rnorm(100)
my_forest &lt;- MakeForest(Hypers(X,Y), Opts())
mu_hat &lt;- my_forest$do_gibbs(X,Y,X,200)

</code></pre>


</div>