<div class="container">

<table style="width: 100%;"><tr>
<td>get_ECB_speeches</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Download and pre-process speeches from the European Central Bank</h2>

<h3>Description</h3>

<p>This helper function automatically retrieve the full data set of
speeches made available by the ECB. In addition, it implements a number of
pre-processing steps that may be turned on or off as needed.
</p>


<h3>Usage</h3>

<pre><code class="language-R">get_ECB_speeches(
  filter_english = TRUE,
  clean_footnotes = TRUE,
  compute_sentiment = TRUE,
  tokenize_w_POS = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>filter_english</code></td>
<td>
<p>if <code>TRUE</code>, attempts to select English speeches only
using <code>textcat::textcat()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clean_footnotes</code></td>
<td>
<p>if <code>TRUE</code>, attempts to clean footnotes from speeches
texts using some regex patterns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compute_sentiment</code></td>
<td>
<p>if <code>TRUE</code>, computes the sentiment of each speech
using <code>sentometrics::compute_sentiment()</code> with the the Loughran &amp; McDonald
lexicon.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenize_w_POS</code></td>
<td>
<p>if <code>TRUE</code>, tokenizes and apply Part-Of-Speech tagging
with <code>spacyr::spacy_parse()</code>. Nouns, adjectives and proper nouns are then
extracted from the parsed speeches to form a <code>tokens</code> object.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Depending on the arguments, returns either a data.frame or a
quanteda::tokens object containing speeches of the ECB.
</p>


</div>