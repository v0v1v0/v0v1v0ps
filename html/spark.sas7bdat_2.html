<div class="container">

<table style="width: 100%;"><tr>
<td>spark_read_sas</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Read in SAS datasets in .sas7bdat format into Spark by using the spark-sas7bdat Spark package.</h2>

<h3>Description</h3>

<p>Read in SAS datasets in .sas7bdat format into Spark by using the spark-sas7bdat Spark package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_read_sas(sc, path, table)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p>Connection to Spark local instance or remote cluster. See the example</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>full path to the SAS file either on HDFS (hdfs://), S3 (s3n://), as well as the local file system (file://). 
Mark that files on the local file system need to be specified using the full path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>table</code></td>
<td>
<p>character string with the name of the Spark table where the SAS dataset will be put into</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>an object of class <code>tbl_spark</code>, which is a reference to a Spark DataFrame based on which
dplyr functions can be executed. See <a href="https://github.com/sparklyr/sparklyr">https://github.com/sparklyr/sparklyr</a>
</p>


<h3>References</h3>

<p><a href="https://spark-packages.org/package/saurfang/spark-sas7bdat">https://spark-packages.org/package/saurfang/spark-sas7bdat</a>, <a href="https://github.com/saurfang/spark-sas7bdat">https://github.com/saurfang/spark-sas7bdat</a>, <a href="https://github.com/sparklyr/sparklyr">https://github.com/sparklyr/sparklyr</a>
</p>


<h3>See Also</h3>

<p><code>spark_connect</code>, <code>sdf_register</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## If you haven't got a Spark cluster, you can install Spark locally like this
library(sparklyr)
spark_install(version = "2.0.1")

## Define the SAS .sas7bdat file, connect to the Spark cluster to read + process the data
myfile &lt;- system.file("extdata", "iris.sas7bdat", package = "spark.sas7bdat")
myfile

library(spark.sas7bdat)
sc &lt;- spark_connect(master = "local")
x &lt;- spark_read_sas(sc, path = myfile, table = "sas_example")
x

library(dplyr)
x %&gt;% group_by(Species) %&gt;%
  summarise(count = n(), length = mean(Sepal_Length), width = mean(Sepal_Width))

## End(Not run)
</code></pre>


</div>