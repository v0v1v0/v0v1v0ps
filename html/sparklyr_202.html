<div class="container">

<table style="width: 100%;"><tr>
<td>ft_word2vec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Feature Transformation â€“ Word2Vec (Estimator)</h2>

<h3>Description</h3>

<p>Word2Vec transforms a word into a code for further natural language processing or machine learning process.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ft_word2vec(
  x,
  input_col = NULL,
  output_col = NULL,
  vector_size = 100,
  min_count = 5,
  max_sentence_length = 1000,
  num_partitions = 1,
  step_size = 0.025,
  max_iter = 1,
  seed = NULL,
  uid = random_string("word2vec_"),
  ...
)

ml_find_synonyms(model, word, num)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>input_col</code></td>
<td>
<p>The name of the input column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_col</code></td>
<td>
<p>The name of the output column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vector_size</code></td>
<td>
<p>The dimension of the code that you want to transform from words. Default: 100</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>The minimum number of times a token must appear to be included in
the word2vec model's vocabulary. Default: 5</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_sentence_length</code></td>
<td>
<p>(Spark 2.0.0+) Sets the maximum length (in words) of each sentence
in the input data. Any sentence longer than this threshold will be divided into
chunks of up to <code>max_sentence_length</code> size. Default: 1000</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_partitions</code></td>
<td>
<p>Number of partitions for sentences of words. Default: 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p>Param for Step size to be used for each iteration of optimization (&gt; 0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>uid</code></td>
<td>
<p>A character string used to uniquely identify the feature transformer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A fitted <code>Word2Vec</code> model, returned by <code>ft_word2vec()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>word</code></td>
<td>
<p>A word, as a length-one character vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num</code></td>
<td>
<p>Number of words closest in similarity to the given word to find.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator
fits against <code>x</code> to obtain a transformer, returning a <code>tbl_spark</code>.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> or a
<code>ml_estimator</code> object. If it is a <code>ml_pipeline</code>, it will return
a pipeline with the transformer or estimator appended to it. If a
<code>tbl_spark</code>, it will return a <code>tbl_spark</code> with the transformation
applied to it.
</p>
<p><code>ml_find_synonyms()</code> returns a DataFrame of synonyms and cosine similarities
</p>


<h3>See Also</h3>

<p>Other feature transformers: 
<code>ft_binarizer()</code>,
<code>ft_bucketizer()</code>,
<code>ft_chisq_selector()</code>,
<code>ft_count_vectorizer()</code>,
<code>ft_dct()</code>,
<code>ft_elementwise_product()</code>,
<code>ft_feature_hasher()</code>,
<code>ft_hashing_tf()</code>,
<code>ft_idf()</code>,
<code>ft_imputer()</code>,
<code>ft_index_to_string()</code>,
<code>ft_interaction()</code>,
<code>ft_lsh</code>,
<code>ft_max_abs_scaler()</code>,
<code>ft_min_max_scaler()</code>,
<code>ft_ngram()</code>,
<code>ft_normalizer()</code>,
<code>ft_one_hot_encoder()</code>,
<code>ft_one_hot_encoder_estimator()</code>,
<code>ft_pca()</code>,
<code>ft_polynomial_expansion()</code>,
<code>ft_quantile_discretizer()</code>,
<code>ft_r_formula()</code>,
<code>ft_regex_tokenizer()</code>,
<code>ft_robust_scaler()</code>,
<code>ft_sql_transformer()</code>,
<code>ft_standard_scaler()</code>,
<code>ft_stop_words_remover()</code>,
<code>ft_string_indexer()</code>,
<code>ft_tokenizer()</code>,
<code>ft_vector_assembler()</code>,
<code>ft_vector_indexer()</code>,
<code>ft_vector_slicer()</code>
</p>


</div>