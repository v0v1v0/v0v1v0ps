<div class="container">

<table style="width: 100%;"><tr>
<td>spark_read</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Read file(s) into a Spark DataFrame using a custom reader</h2>

<h3>Description</h3>

<p>Run a custom R function on Spark workers to ingest data from one or more files
into a Spark DataFrame, assuming all files follow the same schema.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_read(sc, paths, reader, columns, packages = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paths</code></td>
<td>
<p>A character vector of one or more file URIs (e.g.,
c("hdfs://localhost:9000/file.txt", "hdfs://localhost:9000/file2.txt"))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reader</code></td>
<td>
<p>A self-contained R function that takes a single file URI as
argument and returns the data read from that file as a data frame.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>columns</code></td>
<td>
<p>a named list of column names and column types of the resulting
data frame (e.g., list(column_1 = "integer", column_2 = "character")), or a
list of column names only if column types should be inferred from the data
(e.g., list("column_1", "column_2"), or NULL if column types should be
inferred and resulting data frame can have arbitrary column names</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>packages</code></td>
<td>
<p>A list of R packages to distribute to Spark workers</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code>collect_from_rds()</code>,
<code>spark_insert_table()</code>,
<code>spark_load_table()</code>,
<code>spark_read_avro()</code>,
<code>spark_read_binary()</code>,
<code>spark_read_csv()</code>,
<code>spark_read_delta()</code>,
<code>spark_read_image()</code>,
<code>spark_read_jdbc()</code>,
<code>spark_read_json()</code>,
<code>spark_read_libsvm()</code>,
<code>spark_read_orc()</code>,
<code>spark_read_parquet()</code>,
<code>spark_read_source()</code>,
<code>spark_read_table()</code>,
<code>spark_read_text()</code>,
<code>spark_save_table()</code>,
<code>spark_write_avro()</code>,
<code>spark_write_csv()</code>,
<code>spark_write_delta()</code>,
<code>spark_write_jdbc()</code>,
<code>spark_write_json()</code>,
<code>spark_write_orc()</code>,
<code>spark_write_parquet()</code>,
<code>spark_write_source()</code>,
<code>spark_write_table()</code>,
<code>spark_write_text()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

library(sparklyr)
sc &lt;- spark_connect(
  master = "yarn",
  spark_home = "~/spark/spark-2.4.5-bin-hadoop2.7"
)

# This is a contrived example to show reader tasks will be distributed across
# all Spark worker nodes
spark_read(
  sc,
  rep("/dev/null", 10),
  reader = function(path) system("hostname", intern = TRUE),
  columns = c(hostname = "string")
) %&gt;% sdf_collect()

## End(Not run)

</code></pre>


</div>