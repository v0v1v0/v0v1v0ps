<div class="container">

<table style="width: 100%;"><tr>
<td>mice.impute.SuperLearner</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>SuperLearner method for <code>mice</code> package.</h2>

<h3>Description</h3>

<p>Method for the <code>mice</code> package that uses SuperLearner as the predctive
algorithm.  Model fitting is done using the <code>SuperLearner</code> package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mice.impute.SuperLearner(
  y,
  ry,
  x,
  wy = NULL,
  SL.library,
  kernel = c("gaussian", "uniform", "triangular"),
  bw = c(0.1, 0.2, 0.25, 0.3, 0.5, 1, 2.5, 5, 10, 20),
  bw.update = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector to be imputed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ry</code></td>
<td>
<p>Logical vector of length <code>length(y)</code> indicating the the subset
<code>y[ry]</code> of elements in y to which the imputation model is fitted.
The <code>ry</code> generally distinguishes the observed (<code>TRUE</code>) and missing
values (<code>FALSE</code>) in <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Numeric design matrix with <code>length(y)</code> rows with predictors
for <code>y</code>.
Matrix <code>x</code> may have no missing values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wy</code></td>
<td>
<p>Logical vector of length <code>length(y)</code>. A <code>TRUE</code> value indicates
locations in <code>y</code> for which imputations are created.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>SL.library</code></td>
<td>
<p>For SuperLearner: Either a character vector of prediction
algorithms or list containing character vectors as specified by the
SuperLearner package.  See details below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>One of "gaussian", "uniform", "triangular".  Kernel function
used to compute weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bw</code></td>
<td>
<p>NULL or numeric value for bandwidth of kernel function (as standard deviations of the kernel).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bw.update</code></td>
<td>
<p>logical indicating whether bandwidths should be computed
every iteration or only on the first iteration.  Default is <code>TRUE</code>,
but <code>FALSE</code> may speed up the run time at the cost of accuracy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments passed to <code>SuperLearner</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>mice.impute.SuperLearner()</code> is a method for use with the mice() function that
implements the ensemble predictive model, SuperLearner (van der Laan, 2011),
into the mice (van Buuren, 2011) multiple imputation procedure. This function
is never called directly, instead a user that wishes to use SuperLearner
in MICE simply needs to set the argument <code>method = "SuperLearner"</code> in the
call to <code>mice()</code>. Arguments for the <code>SuperLearner()</code>
function are passed from mice as extra arguments in the <code>mice()</code> call.
</p>
<p>All MICE methods randomly generate imputed values for a number of data sets.
The approach of SuperMICE is to estimate parameters for a normal distribution
centered at the point estimate for an imputed value predicted by a SuperLearner model.
The point estimates are obtained by fitting a selection of different
predictive models on complete cases and determining an optimal weighted average
of candidate models to predict the missing cases. SuperMICE uses the implementation
of SuperLearner found in the SuperLearner package.
The models to be used with <code>SuperLearner()</code> are supplied by the user as a
character vector. For a full list of available methods see
<code>listWrappers()</code>.
</p>
<p>SuperLearner models do not produce standard errors for estimates, so instead
we use a kernel based estimate of local variance around each point estimate
as the variance parameter in the normal distribution used to randomly sample values.
The kernel can be set by the user with the <code>kernel</code> argument as either
a gaussian kernel, uniform kernel, or triangular kernel. The user must also
supply a list of candidate bandwidths in the <code>bw</code> argument as a numeric
vector.  For more information on the variance and bandwidth selection
see Laqueur, et. al (2021). In every iteration the mice procedure, the optimal
bandwidth is reselected. This may be changed to select the bandwidth only
on the first iteration to speed up the total run time of the imputation by
changing <code>bw.update</code> to <code>FALSE</code>; however this may bias your results.
Note that this only applies to continuous response variables.  In the binary
case the variance is a function of the SuperLearner estimate.
</p>


<h3>Value</h3>

<p>Vector with imputed data, same type as <code>y</code>, and of length <code>sum(wy)</code>
</p>


<h3>References</h3>

<p>Laqueur, H. S., Shev, A. B., Kagawa, R. M. C. (2021). SuperMICE: An Ensemble
Machine Learning Approach to Multiple Imputation by Chained Equations.
American Journal of Epidemiology, kwab271,
<a href="https://doi.org/10.1093/aje/kwab271">doi:10.1093/aje/kwab271</a>.
</p>
<p>Van Buuren, S., Groothuis-Oudshoorn, K. (2011). <code>mice</code>: Multivariate
Imputation by Chained Equations in <code>R</code>. Journal of Statistical Software,
<strong>45</strong>(3), 1-67. <a href="https://doi.org/10.18637/jss.v045.i03">doi:10.18637/jss.v045.i03</a>.
</p>
<p>van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner,
Statistical Applications of Genetics and Molecular Biology, 6, article 25.
</p>


<h3>See Also</h3>

<p><code>mice()</code>, <code>SuperLearner()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
  #Multiple imputation with missingness on a continuous variable.

  #Randomly generated data with missingness in x2. The probability of x2
  #  being missing increases with with value of x1.
  n &lt;- 20
  pmissing &lt;- 0.10
  x1 &lt;- runif(n, min = -3, max = 3)
  x2 &lt;- x1^2 + rnorm(n, mean = 0, sd = 1)
  error &lt;- rnorm(n, mean = 0, sd = 1)
  y &lt;- x1 + x2 + error
  f &lt;- ecdf(x1)
  x2 &lt;- ifelse(runif(x2) &lt; (f(x1) * 2 * pmissing), NA, x2)
  dat &lt;- data.frame(y, x1, x2)

  #Create vector of SuperLearner method names
  #  Note: see SuperLearner::listWrappers() for a full list of methods
  #    available.
  SL.lib &lt;- c("SL.mean", "SL.glm")

  #Run mice().
  #  Note 1: m &gt;= 30 and maxit &gt;= 10 are recommended outside of this
  #    toy example
  #  Note 2: a denser bandwidth grid is recommended, see default for bw
  #    argument for example.
  imp.SL &lt;- mice::mice(dat, m = 2, maxit = 2,
                       method = "SuperLearner",
                       print = TRUE, SL.library = SL.lib,
                       kernel = "gaussian",
                       bw = c(0.25, 1, 5))


</code></pre>


</div>