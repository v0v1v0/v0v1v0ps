<div class="container">

<table style="width: 100%;"><tr>
<td>shannon-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Computation of Entropy Measures and Relative Loss
</h2>

<h3>Description</h3>

<p>The functions allow for the numerical evaluation of some commonly used entropy measures, such as Shannon entropy, Rényi entropy, Havrda and Charvat entropy, and Arimoto entropy, at selected parametric values from several well-known and widely used probability distributions. Moreover, the functions also compute the relative loss of these entropies using the truncated distributions. Let <code class="reqn">X</code> be an absolutely continuous random variable having the probability density function <code class="reqn">f(x)</code>. Then, the Shahnon entropy is as follows:
</p>
<p style="text-align: center;"><code class="reqn">
H(X)=-\intop_{-\infty}^{+\infty}f(x)\log f(x)dx.
</code>
</p>

<p>The  Rényi entropy is as follows:
</p>
<p style="text-align: center;"><code class="reqn">
H_{\delta}(X)=\frac{1}{1-\delta}\log\intop_{-\infty}^{+\infty}f(x)^{\delta}dx;\qquad\delta&gt;0,\delta\ne1.
</code>
</p>

<p>The Havrda and Charvat entropy is as follows:
</p>
<p style="text-align: center;"><code class="reqn">
H_{\delta}(X)=\frac{1}{2^{1-\delta}-1}\left(\intop_{-\infty}^{+\infty}f(x)^{\delta}dx-1\right);\qquad\delta&gt;0,\delta\ne1.
</code>
</p>

<p>The Arimoto entropy is as follows:
</p>
<p style="text-align: center;"><code class="reqn">
H_{\delta}(X)=\frac{\delta}{1-\delta}\left[\left(\intop_{-\infty}^{+\infty}f(x)^{\delta}dx\right)^{\frac{1}{\delta}}-1\right];\qquad\delta&gt;0,\delta\ne1.
</code>
</p>

<p>Let <code class="reqn">D(X)</code> be an entropy, and <code class="reqn">D_p(X)</code> be its truncated integral version at <code class="reqn">p</code>, i.e., defined with the truncated version of <code class="reqn">f(x)</code> over the interval <code class="reqn">(-\infty,p)</code>. Then we define the corresponding relative loss entropy is defined by
</p>
<p style="text-align: center;"><code class="reqn">
S_D(p)= \frac{D(X)-D_p(X)}{D(X)}.
</code>
</p>



<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> shannon</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 0.2.0 </td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2024-08-21</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>Maintainers</h3>

<p>Muhammad Imran &lt;imranshakoor84@yahoo.com&gt;
</p>


<h3>Author(s)</h3>

<p>Muhammad Imran <a href="mailto:imranshakoor84@yahoo.com">imranshakoor84@yahoo.com</a>,
Christophe Chesneau
<a href="mailto:christophe.chesneau@unicaen.fr">christophe.chesneau@unicaen.fr</a>
and Farrukh Jamal <a href="mailto:farrukh.jamal@iub.edu.pk">farrukh.jamal@iub.edu.pk</a>.
</p>


<h3>References</h3>

<p>Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.
</p>
<p>Rényi, A. (1961). On measures of entropy and information,
Hungarian Academy of Sciences, Budapest, Hungary, 547-
561.
</p>
<p>Havrda, J., &amp; Charvat, F. (1967). Quantification method of classification processes. Concept of structural <code class="reqn">\alpha</code>-entropy. Kybernetika, 3(1), 30-35.
</p>
<p>Arimoto, S. (1971). Information-theoretical considerations on estimation problems. Information and control, 19(3), 181-194.
</p>
<p>Awad, A. M., &amp; Alawneh, A. J. (1987). Application of entropy to a life-time model. IMA Journal of Mathematical Control and Information, 4(2), 143-148.
</p>


</div>