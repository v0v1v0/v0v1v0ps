<div class="container">

<table style="width: 100%;"><tr>
<td>pit_sample</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Probability Integral Transformation (sample-based version)</h2>

<h3>Description</h3>

<p>Uses a Probability Integral Transformation (PIT) (or a
randomised PIT for integer forecasts) to
assess the calibration of predictive Monte Carlo samples. Returns a
p-values resulting from an Anderson-Darling test for uniformity
of the (randomised) PIT as well as a PIT histogram if specified.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pit_sample(true_values, predictions, n_replicates = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_replicates</code></td>
<td>
<p>the number of draws for the randomised PIT for
integer predictions.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Calibration or reliability of forecasts is the ability of a model to
correctly identify its own uncertainty in making predictions. In a model
with perfect calibration, the observed data at each time point look as if
they came from the predictive probability distribution at that time.
</p>
<p>Equivalently, one can inspect the probability integral transform of the
predictive distribution at time t,
</p>
<p style="text-align: center;"><code class="reqn">
u_t = F_t (x_t)
</code>
</p>

<p>where <code class="reqn">x_t</code> is the observed data point at time <code class="reqn">t \textrm{ in } t_1,
â€¦, t_n</code>, n being the number of forecasts, and <code class="reqn">F_t</code> is
the (continuous) predictive cumulative probability distribution at time t. If
the true probability distribution of outcomes at time t is <code class="reqn">G_t</code> then the
forecasts <code class="reqn">F_t</code> are said to be ideal if <code class="reqn">F_t = G_t</code> at all times t.
In that case, the probabilities <code class="reqn">u_t</code> are distributed uniformly.
</p>
<p>In the case of discrete outcomes such as incidence counts,
the PIT is no longer uniform even when forecasts are ideal.
In that case a randomised PIT can be used instead:
</p>
<p style="text-align: center;"><code class="reqn">
u_t = P_t(k_t) + v * (P_t(k_t) - P_t(k_t - 1) )
</code>
</p>

<p>where <code class="reqn">k_t</code> is the observed count, <code class="reqn">P_t(x)</code> is the predictive
cumulative probability of observing incidence k at time t,
<code class="reqn">P_t (-1) = 0</code> by definition and v is standard uniform and independent
of k. If <code class="reqn">P_t</code> is the true cumulative
probability distribution, then <code class="reqn">u_t</code> is standard uniform.
</p>
<p>The function checks whether integer or continuous forecasts were provided.
It then applies the (randomised) probability integral and tests
the values <code class="reqn">u_t</code> for uniformity using the
Anderson-Darling test.
</p>
<p>As a rule of thumb, there is no evidence to suggest a forecasting model is
miscalibrated if the p-value found was greater than a threshold of p &gt;= 0.1,
some evidence that it was miscalibrated if 0.01 &lt; p &lt; 0.1, and good
evidence that it was miscalibrated if p &lt;= 0.01. However, the AD-p-values
may be overly strict and there actual usefulness may be questionable.
In this context it should be noted, though, that uniformity of the
PIT is a necessary but not sufficient condition of calibration.
</p>


<h3>Value</h3>

<p>A vector with PIT-values. For continuous forecasts, the vector will
correspond to the length of <code>true_values</code>. For integer forecasts, a
randomised PIT will be returned of length
<code>length(true_values) * n_replicates</code>
</p>


<h3>References</h3>

<p>Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe,
Rosalind M. Eggo, W. John Edmunds (2019) Assessing the performance of
real-time epidemic forecasts: A case study of Ebola in the Western Area
region of Sierra Leone, 2014-15, <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>See Also</h3>

<p><code>pit()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">

## continuous predictions
true_values &lt;- rnorm(20, mean = 1:20)
predictions &lt;- replicate(100, rnorm(n = 20, mean = 1:20))
pit &lt;- pit_sample(true_values, predictions)
plot_pit(pit)

## integer predictions
true_values &lt;- rpois(50, lambda = 1:50)
predictions &lt;- replicate(2000, rpois(n = 50, lambda = 1:50))
pit &lt;- pit_sample(true_values, predictions, n_replicates = 30)
plot_pit(pit)
</code></pre>


</div>