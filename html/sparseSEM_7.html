<div class="container">

<table style="width: 100%;"><tr>
<td>enSEM_stability_selection_parallel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Parallel Stability Selection for the Elastic Net penalized SEM  
</h2>

<h3>Description</h3>

<p>Fit the elastic-net penalized structureal Equation Models (SEM) with input data (X, Y): Y = BY + fX + e.
Perform Stability Selection (STS) on the input dataset. This function implements STS described in
Meinshausen N. and Buhlmann P (2010) and Shah R. and Samworth R (2013). 
</p>
<p>Underlying the function, the program obtains the performs n rounds of boostraping each with half of the original 
sample size, and run the selection path of hyperparameter (alpha, lambda). The following stability selection scores are 
calculated: <br>
1. E(v): the upper bound of the expected number of falsely selected variables  <br>
2. pre-comparison error rate = E(v)/p where p is the total number of model parameters (in SEM, p = M*M -M) <br>
3. E(v)_ShaR the expected number of falsely selected variables described in Shah R. and Samworth R (2013) <br>
4. FDR: False discovery rate = E(v)/nSelected <br>
5. FDR_ShaR: FDR described in Shah R. and Samworth R (2013) <br></p>
<p>The final output is based on Scores described in described in Shah R. and Samworth R (2013), and original scores
described in Meinshausen N. and Buhlmann P (2010) are provided for reference.
</p>
<p>This function 'enSEM_stability_selection_parallel' performs the same computation as that in function 'enSEM_stability_selection' 
with the only difference of setting up the bootstrapping in parallel leveraging the 'parallel' package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">enSEM_stability_selection_parallel(Y,X, Missing,B,
                                    alpha_factors, 
                                    lambda_factors, 
                                    kFold,
                                    nBootstrap,
                                    verbose,
                                    clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>

<p>The observed node response data with dimension of M (nodes) by N (samples). Y is normalized inside the function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>The network node attribute matrix with dimension of M by N. Theoretically, X can be L by N matrix, with L being the total
node attributes. In current implementation, each node only allows one and only one attribute. <br>
If you have more than one attributes for some nodes,  please consider selecting the top one by either
correlation or principal component methods.  <br>
If for some nodes there is no attribute available, fill in the rows with all zeros.  See the yeast data 'yeast.rda' for example. <br>
X is normalized inside the function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Missing</code></td>
<td>

<p>Optional M by N matrix corresponding to elements of Y. 0 denotes not missing, and 1 denotes missing.
If a node i in sample j has the label missing (Missing[i,j] = 1), then Y[i,j] is set to 0.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>

<p>Optional input. For a network with M nodes, B is the M by M adjacency matrix.
If data is simulated/with known true network topology (i.e., known adjacency matrix), the Power
of detection (PD) and False Discovery Rate (FDR) is computed in the output parameter 'statistics'.
</p>
<p>If the true network topology is unknown, B is optional, and the PD/FDR in output parameter
'statistics' should be ignored.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha_factors</code></td>
<td>

<p>The set of candidate alpha values.  Default is seq(start = 0.95, to = 0.05, step = -0.05)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda_factors</code></td>
<td>

<p>The set of candidate lambda values. Default is 10^seq(start =1, to = 0.001, step = -0.2)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kFold</code></td>
<td>

<p>k-fold cross validation, default k=3.  Note STS result is not based on CV. However, fitting l1/l2 regularized SEM will
run the first step described in elasticNetSEM() function: 
Step 1. SEM-ridge regression (L2 penalty) with k-fold CV: this step find the optimal ridge hyperparameter rho to provide an initial values for l1/l2 regularized SEM. <br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nBootstrap</code></td>
<td>

<p>bootstrapping parameter. default nBootstrap = 100.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>describe the information output from -1 - 10, larger number means more output
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clusters</code></td>
<td>

<p>snow clusters
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>the function perform STS
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>STS</code></td>
<td>

<p>The stable effects are those effects selected by STS, i.e., the non-zero values in matrix B.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>statistics</code></td>
<td>

<p>the final STS scores with components of: <br>
1. threshold: denoted as pi in  Meinshausen N. and Buhlmann P (2010) <br>
2. pre-comparison error rate   <br>
3. E(v)   <br>
4. E(v)_ShahR <br>
5. nSTS: final number of stable effects with pi that leads to minimum FDR  <br>
6. FDR <br>
7. FDR_ShahR <br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>STS data</code></td>
<td>
<p>Bootstrapping details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call that produced this object</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Anhui Huang</p>


<h3>References</h3>

<p>[1]: Meinshausen, N. and Buhlmann, P., 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4), pp.417-473.
</p>
<p>[2] Shah, R.D. and Samworth, R.J., 2013. Variable selection with error control: another look at stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(1), pp.55-80.
</p>


<h3>Examples</h3>

<pre><code class="language-R">	library(sparseSEM)
	library(parallel)
	data(B);
	data(Y);
	data(X);
	data(Missing);
	#Example
	

  cl&lt;-makeCluster(2)
  clusterEvalQ(cl,{library(sparseSEM)})
  output = enSEM_stability_selection_parallel(Y,X, Missing,B,
                                            alpha_factors = seq(1,0.05, -0.05), 
                                            lambda_factors =10^seq(-0.2,-4,-0.2), 
                                            kFold = 3,
                                            nBootstrap = 100,
                                            verbose = -1,
                                            clusters = cl)
  stopCluster(cl)	
  
</code></pre>


</div>