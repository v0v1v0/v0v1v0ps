<div class="container">

<table style="width: 100%;"><tr>
<td>SMME</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Soft Maximin Estimation for Large Scale Heterogenous Data</h2>

<h3>Description</h3>

<p>Efficient procedure for solving the Lasso or SCAD penalized soft
maximin problem for large scale_y data. This software implements two proximal
gradient based algorithms (NPG and FISTA) to solve different forms of the soft
maximin problem from <cite>Lund et al., 2022</cite>. 1) For general group specific
design the soft maximin problem is solved using the NPG algorithm.
2) For fixed identical d-array-tensor design across groups, where <code class="reqn">d = 1, 2, 3</code>, the
estimation procedure uses either the FISTA algorithm or the NPG algorithm and
is implemented for the following two cases; i) For a tensor design matrix the
algorithms use array arithmetic to speed up design matrix multiplications
using only the tensor components ii) For a wavelet design matrix the algorithms use
the pyramid algorithm to completely avoid the design matrix and speed up
design matrix multiplications.
Multi-threading is possible when openMP is available for R.
</p>
<p>Note this package SMME replaces the SMMA package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">softmaximin(x,
            y,
            zeta,
            penalty = c("lasso", "scad"),
            alg = c("npg", "fista"),
            nlambda = 30,
            lambda.min.ratio = 1e-04,
            lambda = NULL,
            scale_y = 1,
            penalty.factor = NULL,
            reltol = 1e-05,
            maxiter = 1000,
            steps = 1,
            btmax = 100,
            c = 0.0001,
            tau = 2,
            M = 4,
            nu = 1,
            Lmin = 0,
            lse = TRUE,
            nthreads = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Either a list containing the G group specific design matrices of sizes
<code class="reqn">n_i \times p_i</code> (general model),  a list containing the <code class="reqn">d</code>
(<code class="reqn">d \in \{ 1, 2, 3\}</code>) tensor components (tensor array model) or a string
indicating which wavelet design to use (wavelet array model), see wt
for options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>list containing the G group specific response vectors of sizes
<code class="reqn">n_i \times 1</code>. Alternatively for a model with identical tensor design
across G groups, <code>y</code>is an array of size <code class="reqn">n_1 \times\cdots\times n_d \times G</code>
(<code class="reqn">d \in \{ 1, 2, 3\}</code>) containing the response values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zeta</code></td>
<td>
<p>vector of strictly positive floats controlling  the softmaximin
approximation accuracy. When <code>length(zeta) &gt; 1</code> the procedure will distribute
the computations using the <code>nthreads</code> parameter below when openMP is available.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>string specifying the penalty type. Possible values are
<code>"lasso", "scad"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alg</code></td>
<td>
<p>string specifying the optimization algorithm. Possible values are
<code>"npg", "fista"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>positive integer giving the number of <code>lambda</code> values.
Used when lambda is not specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>strictly positive float giving the smallest value for
<code>lambda</code>, as a fraction of <code class="reqn">\lambda_{max}</code>; the (data dependent)
smallest value for which all coefficients are zero. Used when lambda is not
specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A sequence of strictly positive floats used  as penalty parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale_y</code></td>
<td>
<p>strictly positive number that the response <code>y</code> is multiplied with.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>a length <code class="reqn">p</code> vector of positive floats that are
multiplied with each element in <code>lambda</code> to allow differential penalization
on the coefficients. For tensor models an array of size <code class="reqn">p_1 \times \cdots \times p_d</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reltol</code></td>
<td>
<p>strictly positive float giving the convergence tolerance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>positive integer giving the maximum number of  iterations
allowed for each <code>lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>steps</code></td>
<td>
<p>strictly positive integer giving the number of steps used in the
multi-step adaptive lasso algorithm for non-convex penalties. Automatically
set to 1 when <code>penalty = "lasso"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>btmax</code></td>
<td>
<p>strictly positive integer giving the maximum number of backtracking
steps allowed in each iteration. Default is <code>btmax = 100</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c</code></td>
<td>
<p>strictly positive float used in the NPG algorithm. Default is
<code>c = 0.0001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>strictly positive float used to control the stepsize for NPG.
Default is <code>tau = 2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>positive integer giving the look back for the NPG. Default is <code>M = 4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>strictly positive float used to control the stepsize. A  value less
that 1 will decrease the stepsize and a value larger than one will increase it.
Default is <code>nu = 1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Lmin</code></td>
<td>
<p>non-negative float used by the NPG algorithm to control the
stepsize. For the default  <code>Lmin = 0</code> the maximum step size is the same
as for the FISTA algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lse</code></td>
<td>
<p>logical variable indicating whether to use the log-sum-exp-loss.  TRUE is
default and yields the loss below and  FALSE yields the exponential of this.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nthreads</code></td>
<td>
<p>integer giving the number of threads to use when  openMP
is available. Default is 2.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Consider modeling heterogeneous data <code class="reqn">y_1,\ldots, y_n</code> by dividing
it into <code class="reqn">G</code> groups <code class="reqn">\mathbf{y}_g = (y_1, \ldots, y_{n_g})</code>,
<code class="reqn">g \in \{ 1,\ldots, G\}</code> and then using a linear model
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{y}_g = \mathbf{X}_gb_g + \epsilon_g, \quad g \in \{1,\ldots, G\},
</code>
</p>

<p>to model the group response. Then <code class="reqn">b_g</code> is a group specific <code class="reqn">p\times 1</code>
coefficient, <code class="reqn">\mathbf{X}_g</code> an <code class="reqn">n_g\times p</code> group design matrix and
<code class="reqn">\epsilon_g</code> an <code class="reqn">n_g\times 1</code> error term. The objective is to estimate
a common coefficient <code class="reqn">\beta</code> such that <code class="reqn">\mathbf{X}_g\beta</code> is a robust
and good approximation to <code class="reqn">\mathbf{X}_gb_g</code> across groups.
</p>
<p>Following <cite>Lund et al., 2022</cite>, this objective may be accomplished by
solving the soft maximin estimation problem
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\beta}\frac{1}{\zeta}\log\bigg(\sum_{g = 1}^G \exp(-\zeta \hat V_g(\beta))\bigg)
 + \lambda  \Vert\beta\Vert_1, \quad \zeta &gt; 0,\lambda \geq 0.
</code>
</p>

<p>Here <code class="reqn">\zeta</code> essentially controls the amount of pooling across groups
(<code class="reqn">\zeta \sim 0</code> effectively ignores grouping and pools observations) and
</p>
<p style="text-align: center;"><code class="reqn">
\hat V_g(\beta):=\frac{1}{n_g}(2\beta^\top \mathbf{X}_g^\top
\mathbf{y}_g-\beta^\top \mathbf{X}_g^\top \mathbf{X}_g\beta),
</code>
</p>

<p>is the empirical explained variance, see <cite>Lund et al., 2022</cite> for more
details and references.
</p>
<p>The function <code>softmaximin</code> solves the soft maximin estimation problem in
large scale settings for a sequence of penalty parameters
<code class="reqn">\lambda_{max}&gt;\ldots &gt;\lambda_{min}&gt;0</code> and a sequence of strictly positive
softmaximin  parameters <code class="reqn">\zeta_1, \zeta_2,\ldots</code>.
</p>
<p>The implementation also solves the
problem above with the penalty given by the SCAD penalty, using the multiple
step adaptive lasso procedure to loop over the inner proximal algorithm.
</p>
<p>Two optimization algorithms  are implemented in the SMME packages;
a non-monotone proximal gradient (NPG) algorithm and a fast iterative soft
thresholding algorithm (FISTA).
</p>
<p>The implementation is particularly efficient for models where the design is
identical across groups i.e. <code class="reqn">\mathbf{X}_g = \mathbf{X}</code>
<code class="reqn">\forall g \in \{1, \ldots, G\}</code> in the following two cases:
i) first if <code class="reqn">\mathbf{X}</code> has tensor structure i.e.
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{X} = \bigotimes_{i=1}^d \mathbf{M}_i
</code>
</p>

<p>for marginal <code class="reqn">n_i\times p_i</code> design matrices <code class="reqn">\mathbf{M}_1,\ldots, \mathbf{M}_d</code>
, <code class="reqn">d \in \{ 1, 2, 3\}</code>, <code>y</code> is a <code class="reqn">d + 1</code> dimensional response array
and  <code>x</code> is a list containing the <code class="reqn">d</code> marginal matrices
<code class="reqn">\mathbf{M}_1,\ldots, \mathbf{M}_d</code>. In this case <code>softmaximin</code> solves
the soft maximin problem using minimal memory by way of tensor optimized
arithmetic, see also <code>RH</code>.
ii) second, if the design matrix <code class="reqn">\mathbf{X}</code> is the inverse matrix of an
orthogonal wavelet transform <code>softmaximin</code>  solves the soft maximin problem
given the <code class="reqn">d + 1</code> dimensional response array <code>y</code> and
<code>x</code> the name of the wavelet family <code>wt</code>,  using the
pyramid algorithm to compute multiplications
involving <code class="reqn">\mathbf{X}</code>.
</p>
<p>Note that when multiple values for <code class="reqn">\zeta</code> is provided it is  possible to
distribute the computations across CPUs if openMP is available.
</p>


<h3>Value</h3>

<p>An object with S3 Class "SMME".
</p>
<table>
<tr style="vertical-align: top;">
<td><code>spec</code></td>
<td>
<p>A string indicating the array dimension (1, 2 or 3) and the penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coef</code></td>
<td>
<p>A  <code>length(zeta)</code>-list of <code class="reqn">p \times</code> <code>nlambda</code>
matrices containing the estimates of the model coefficients (<code class="reqn">\beta</code>) for
each <code>lambda</code>-value for which the procedure converged.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A <code>length(zeta)</code>-list vectors containing the sequence of
penalty values used in the estimation procedure for which the procedure converged.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>A  <code>length(zeta)</code>-list of vectors indicating the nonzero model
coefficients for each value of <code>lambda</code> for which the procedure converged.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dimcoef</code></td>
<td>
<p>An integer giving the number <code class="reqn">p</code> of model parameters.
For array data a vector giving the dimension of the model coefficient array <code class="reqn">\beta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dimobs</code></td>
<td>
<p>An integer giving the number of observations. For array data a
vector giving the dimension of the observation (response) array <code>Y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>Integer indicating the dimension of of the array model. Equal to 1
for non array.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wf</code></td>
<td>
<p>A string indicating the wavelet name if used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diagnostics</code></td>
<td>
<p>A list of length 3. Item <code>iter</code> is a <code>length(zeta)</code>-list
of vectors containing  the number of   iterations for each <code>lambda</code> value
for which the algorithm converged. Item <code>bt_iter</code>  is a  <code>length(zeta)</code>
vector with total number of backtracking steps performed across all (converged)
<code>lambda</code> values for given  <code>zeta</code> value. Key <code>bt_enter</code> is a
<code>length(zeta)</code> vector with  total number of times backtracking is initiated
across all (converged)  <code>lambda</code> values for given <code>zeta</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>endmod</code></td>
<td>
<p>Vector of length <code>length(zeta)</code> with the number of
models fitted for each <code>zeta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Stops</code></td>
<td>
<p>Convergence indicators.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Adam Lund
</p>
<p>Maintainer: Adam Lund, <a href="mailto:adam.lund@math.ku.dk">adam.lund@math.ku.dk</a>
</p>


<h3>References</h3>

<p>Lund, A., S. W. Mogensen and N. R. Hansen (2022). Soft Maximin Estimation for
Heterogeneous Data. <em>Scandinavian Journal of Statistics</em>, vol. 49, no. 4,
pp. 1761-1790.
url = https://doi.org/10.1111/sjos.12580
</p>


<h3>Examples</h3>

<pre><code class="language-R">#Non-array data

##size of example
set.seed(42)
G &lt;- 10; n &lt;- sample(100:500, G); p &lt;- 60
x &lt;- y &lt;- list()

##group design matrices
for(g in 1:G){x[[g]] &lt;- matrix(rnorm(n[g] * p), n[g], p)}

##common features and effects
common_features &lt;- rbinom(p, 1, 0.1) #sparsity of comm. feat.
common_effects &lt;- rnorm(p) * common_features

##group response
for(g in 1:G){
bg &lt;- rnorm(p, 0, 0.5) * (1 - common_features) + common_effects
mu &lt;- x[[g]] %*% bg
y[[g]] &lt;- rnorm(n[g]) + mu
}

##fit model for range of lambda and zeta
system.time(fit &lt;- softmaximin(x, y, zeta = c(0.1, 1), penalty = "lasso", alg = "npg"))
betahat &lt;- fit$coef

##estimated common effects for specific lambda and zeta
zetano &lt;- 2
modelno &lt;- dim(betahat[[zetano]])[2]
m &lt;- min(betahat[[zetano]][ , modelno], common_effects)
M &lt;- max(betahat[[zetano]][ , modelno], common_effects)
plot(common_effects, type = "p", ylim = c(m, M), col = "red")
lines(betahat[[zetano]][ , modelno], type = "h")

#Array data
##size of example
set.seed(42)
G &lt;- 50; n &lt;- c(30, 20, 10); p &lt;- c(7, 5, 4)

##marginal design matrices (Kronecker components)
x &lt;- list()
for(i in 1:length(n)){x[[i]] &lt;- matrix(rnorm(n[i] * p[i]), n[i], p[i])}

##common features and effects
common_features &lt;- rbinom(prod(p), 1, 0.1) #sparsity of comm. feat.
common_effects &lt;- rnorm(prod(p),0,0.1) * common_features

##group response
 y &lt;- array(NA, c(n, G))
for(g in 1:G){
bg &lt;- rnorm(prod(p), 0, .1) * (1 - common_features) + common_effects
Bg &lt;- array(bg, p)
mu &lt;- RH(x[[3]], RH(x[[2]], RH(x[[1]], Bg)))
y[,,, g] &lt;- array(rnorm(prod(n)), dim = n) + mu
}

##fit model for range of lambda and zeta
system.time(fit &lt;- softmaximin(x, y, zeta = c(1, 10, 100), penalty = "lasso",
            alg = "npg"))
betahat &lt;- fit$coef

##estimated common effects for specific lambda and zeta
zetano &lt;- 1
modelno &lt;- dim(betahat[[zetano]])[2]
m &lt;- min(betahat[[zetano]][, modelno], common_effects)
M &lt;- max(betahat[[zetano]][, modelno], common_effects)
plot(common_effects, type = "p", ylim = c(m, M), col = "red")
lines(betahat[[zetano]][ , modelno], type = "h")

#Array data and wavelets
##size of example
set.seed(42)
G &lt;- 50; p &lt;- n &lt;- c(2^3, 2^4, 2^5);

##common features and effects
common_features &lt;- rbinom(prod(p), 1, 0.1) #sparsity of comm. feat.
common_effects &lt;- rnorm(prod(p), 0, 1) * common_features

##group response
y &lt;- array(NA, c(n, G))
for(g in 1:G){
bg &lt;- rnorm(prod(p), 0, 0.1) * (1 - common_features) + common_effects
Bg &lt;- array(bg, p)
mu &lt;- iwt(Bg)
y[,,, g] &lt;- array(rnorm(prod(n), 0, 0.5), dim = n) + mu
}

##fit model for range of lambda and zeta
system.time(fit &lt;- softmaximin(x = "la8", y, zeta = c(0.1, 1, 10),
                                penalty = "lasso", alg = "fista"))
betahat &lt;- fit$coef

##estimated common effects for specific lambda and zeta
zetano &lt;- 3
modelno &lt;- dim(betahat[[zetano]])[2]
m &lt;- min(betahat[[zetano]][, modelno], common_effects)
M &lt;- max(betahat[[zetano]][, modelno], common_effects)
plot(common_effects, type = "p", ylim = c(m, M), col = "red")
lines(betahat[[zetano]][ , modelno], type = "h")
</code></pre>


</div>