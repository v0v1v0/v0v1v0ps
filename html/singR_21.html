<div class="container">

<table style="width: 100%;"><tr>
<td>singR</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>SImultaneous Non-Gaussian Component analysis for data integration.</h2>

<h3>Description</h3>

<p>This function combines all steps from the <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-15/issue-3/Simultaneous-non-Gaussian-component-analysis-SING-for-data-integration-in/10.1214/21-AOAS1466.full">SING paper</a>
</p>


<h3>Usage</h3>

<pre><code class="language-R">singR(
  dX,
  dY,
  n.comp.X = NULL,
  n.comp.Y = NULL,
  df = 0,
  rho_extent = c("small", "medium", "large"),
  Cplus = TRUE,
  tol = 1e-10,
  stand = FALSE,
  distribution = "JB",
  maxiter = 1500,
  individual = FALSE,
  whiten = c("sqrtprec", "eigenvec", "none"),
  restarts.dbyd = 0,
  restarts.pbyd = 20
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>dX</code></td>
<td>
<p>original dataset for decomposition, matrix of n x px.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dY</code></td>
<td>
<p>original dataset for decomposition, matrix of n x py.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.comp.X</code></td>
<td>
<p>the number of non-Gaussian components in dataset X. If null, will estimate the number using ICtest::FOBIasymp.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.comp.Y</code></td>
<td>
<p>the number of non-Gaussian components in dataset Y. If null, will estimate the number using ICtest::FOBIasymp.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>default value=0 when use JB, if df&gt;0, estimates a density for the loadings using a tilted Gaussian (non-parametric density estimate).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho_extent</code></td>
<td>
<p>Controls similarity of the scores in the two datasets. Numerical value and three options in character are acceptable. small, medium or large is defined from the JB statistic. Try "small" and see if the loadings are equal, then try others if needed. If numeric input, it will multiply the input by JBall to get the rho.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Cplus</code></td>
<td>
<p>whether to use C code (faster) in curvilinear search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>difference tolerance in curvilinear search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stand</code></td>
<td>
<p>whether to use standardization, if true, it will make the column and row means to 0 and columns sd to 1. If false, it will only make the row means to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>
<p>"JB" or "tiltedgaussian"; "JB" is much faster. In SING, this refers to the "density" formed from the vector of loadings. "tiltedgaussian" with large df can potentially model more complicated patterns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>the max iteration number for the curvilinear search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>individual</code></td>
<td>
<p>whether to return the individual non-Gaussian components, default value = F.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>whiten</code></td>
<td>
<p>whitening method used in lngca. Defaults to "svd" which uses the n left eigenvectors divided by sqrt(px-1) by 'eigenvec'. Optionally uses the square root of the n x n "precision" matrix by 'sqrtprec'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts.dbyd</code></td>
<td>
<p>default = 0. These are d x d initial matrices padded with zeros, which results in initializations from the principal subspace. Can speed up convergence but may miss low variance non-Gaussian components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts.pbyd</code></td>
<td>
<p>default = 20. Generates p x d random orthogonal matrices. Use a large number for large datasets. Note: it is recommended that you run lngca twice with different seeds and compare the results, which should be similar when a sufficient number of restarts is used. In practice, stability with large datasets and a large number of components can be challenging.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Function outputs a list including the following:
</p>

<dl>
<dt><code>Sjx</code></dt>
<dd>
<p>variable loadings for joint NG components in dataset X with matrix rj x px.</p>
</dd>
<dt><code>Sjy</code></dt>
<dd>
<p>variable loadings for joint NG components in dataset Y with matrix rj x py.</p>
</dd>
<dt><code>Six</code></dt>
<dd>
<p>variable loadings for individual NG components in dataset X with matrix riX x px.</p>
</dd>
<dt><code>Siy</code></dt>
<dd>
<p>variable loadings for individual NG components in dataset Y with matrix riX x py.</p>
</dd>
<dt><code>Mix</code></dt>
<dd>
<p>scores of individual NG components in X with matrix n x riX.</p>
</dd>
<dt><code>Miy</code></dt>
<dd>
<p>scores of individual NG components in Y with matrix n x riY.</p>
</dd>
<dt><code>est.Mjx</code></dt>
<dd>
<p>Estimated subject scores for joint components in dataset X with matrix n x rj.</p>
</dd>
<dt><code>est.Mjy</code></dt>
<dd>
<p>Estimated subject scores for joint components in dataset Y with matrix n x rj.</p>
</dd>
<dt><code>est.Mj</code></dt>
<dd>
<p>Average of est.Mjx and est.Mjy as the subject scores for joint components in both datasets with matrix n x rj.</p>
</dd>
<dt><code>C_plus</code></dt>
<dd>
<p>whether to use C version of curvilinear search.</p>
</dd>
<dt><code>rho_extent</code></dt>
<dd>
<p>the weight of rho in search</p>
</dd>
<dt><code>df</code></dt>
<dd>
<p>degree of freedom, = 0 when use JB, &gt;0 when use tiltedgaussian.</p>
</dd>
</dl>
<h3>Examples</h3>

<pre><code class="language-R">
#get simulation data
data(exampledata)

# use JB stat to compute with singR
output_JB=singR(dX=exampledata$dX,dY=exampledata$dY,
df=0,rho_extent="small",distribution="JB",individual=TRUE)

# use tiltedgaussian distribution to compute with singR.
# tiltedgaussian may be more accurate but is considerably slower,
# and is not recommended for large datasets.
output_tilted=singR(dX=exampledata$dX,dY=exampledata$dY,
df=5,rho_extent="small",distribution="tiltedgaussian",individual=TRUE)

# use pmse to measure difference from the truth
pmse(M1 = t(output_JB$est.Mj),M2 = t(exampledata$mj),standardize = TRUE)

pmse(M1 = t(output_tilted$est.Mj),M2 = t(exampledata$mj),standardize = TRUE)


</code></pre>


</div>