<div class="container">

<table style="width: 100%;"><tr>
<td>build_pytorch_net</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Build a Pytorch Multilayer Perceptron</h2>

<h3>Description</h3>

<p>Utility function to build an MLP with a choice of activation function and weight
initialization with optional dropout and batch normalization.
</p>


<h3>Usage</h3>

<pre><code class="language-R">build_pytorch_net(
  n_in,
  n_out,
  nodes = c(32, 32),
  activation = "relu",
  act_pars = list(),
  dropout = 0.1,
  bias = TRUE,
  batch_norm = TRUE,
  batch_pars = list(eps = 1e-05, momentum = 0.1, affine = TRUE),
  init = "uniform",
  init_pars = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>n_in</code></td>
<td>
<p><code>(integer(1))</code><br> Number of input features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_out</code></td>
<td>
<p><code>(integer(1))</code><br> Number of targets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nodes</code></td>
<td>
<p><code>(numeric())</code><br> Hidden nodes in network, each element in vector represents number
of hidden nodes in respective layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p><code>(character(1)|list())</code><br> Activation function, can either be a single
character and the same function is used in all layers, or a list of length <code>length(nodes)</code>. See
get_pycox_activation for options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>act_pars</code></td>
<td>
<p><code>(list())</code><br> Passed to get_pycox_activation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p><code>(numeric())</code><br> Optional dropout layer, if <code>NULL</code> then no dropout layer added
otherwise either a single numeric which will be added to all layers or a vector of differing
drop-out amounts.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias</code></td>
<td>
<p><code>(logical(1))</code><br> If <code>TRUE</code> (default) then a bias parameter is added to all linear
layers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_norm</code></td>
<td>
<p><code>(logical(1))</code><br> If <code>TRUE</code> (default) then batch normalisation is applied
to all layers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_pars</code></td>
<td>
<p><code>(list())</code><br> Parameters for batch normalisation, see
<code>reticulate::py_help(torch$nn$BatchNorm1d)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p><code>(character(1))</code><br> Weight initialization method. See
get_pycox_init for options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init_pars</code></td>
<td>
<p><code>(list())</code><br> Passed to get_pycox_init.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function is a helper for R users with less Python experience. Currently it is
limited to simple MLPs. More advanced networks will require manual creation with
<a href="https://CRAN.R-project.org/package=reticulate"><span class="pkg">reticulate</span></a>.
</p>


<h3>Value</h3>

<p>No return value.
</p>


</div>