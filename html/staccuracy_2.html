<div class="container">

<table style="width: 100%;"><tr>
<td>aucroc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Area under the ROC curve</h2>

<h3>Description</h3>

<p>Returns the area under the ROC curve based on comparing the predicted scores to the actual binary values. Tied predictions are handled by calculating the optimistic AUC (positive cases sorted first, resulting in higher AUC) and the pessimistic AUC (positive cases sorted last, resulting in lower AUC) and then returning the average of the two. For the ROC, a "tie" means at least one pair of <code>pred</code> predictions whose value is identical yet their corresponding values of <code>actual</code> are different. (If the value of <code>actual</code> are the same for identical predictions, then these are unproblematic and are not considered "ties".)
</p>


<h3>Usage</h3>

<pre><code class="language-R">aucroc(
  actual,
  pred,
  na.rm = FALSE,
  binary_true_value = NULL,
  sample_size = 10000,
  seed = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>actual</code></td>
<td>
<p>any atomic vector. Actual label values from a dataset. They must be binary; that is, there must be exactly two distinct values (other than missing values, which are allowed). The "true" or "positive" class is determined by coercing <code>actual</code> to logical <code>TRUE</code> and <code>FALSE</code> following the rules of <code>as.logical()</code>. If this is not the intended meaning of "positive", then specify which of the two values should be considered <code>TRUE</code> with the argument <code>binary_true_value</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>numeric vector. Predictions corresponding to each respective element in <code>actual</code>. Any numeric value (not only probabilities) are permissible.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>logical(1). <code>TRUE</code> if missing values should be removed; <code>FALSE</code> if they should be retained. If <code>TRUE</code>, then if any element of either <code>actual</code> or <code>pred</code> is missing, its paired element will be also removed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>binary_true_value</code></td>
<td>
<p>any single atomic value. The value of <code>actual</code> that is considered <code>TRUE</code>; any other value of <code>actual</code> is considered <code>FALSE</code>. For example, if <code>2</code> means <code>TRUE</code> and <code>1</code> means <code>FALSE</code>, then set <code>binary_true_value = 2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_size</code></td>
<td>
<p>single positive integer. To keep the computation relatively rapid, when <code>actual</code> and <code>pred</code> are longer than <code>sample_size</code> elements, then a random sample of <code>sample_size</code> of <code>actual</code> and <code>pred</code> will be selected and the ROC and AUC will be calculated on this sample. To disable random sampling for long inputs, set <code>sample_size = NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>numeric(1). Random seed used only if <code>length(actual) &gt; sample_size</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>List with the following elements:
</p>

<ul>
<li> <p><code>roc_opt</code>: tibble with optimistic ROC data. "Optimistic" means that when predictions are tied, the TRUE/positive actual values are ordered before the FALSE/negative ones.
</p>
</li>
<li> <p><code>roc_pess</code>: tibble with pessimistic ROC data. "Pessimistic" means that when predictions are tied, the FALSE/negative actual values are ordered before the TRUE/positive ones. Note that this difference is not merely in the sort order: when there are ties, the way that true positives, true negatives, etc. are counted is different for optimistic and pessimistic approaches. If there are no tied predictions, then <code>roc_opt</code> and <code>roc_pess</code> are identical.
</p>
</li>
<li> <p><code>auc_opt</code>: area under the ROC curve for optimistic ROC.
</p>
</li>
<li> <p><code>auc_pess</code>: area under the ROC curve for pessimistic ROC.
</p>
</li>
<li> <p><code>auc</code>: mean of <code>auc_opt</code> and <code>auc_pess</code>. If there are no tied predictions, then <code>auc_opt</code>, <code>auc_pess</code>, and <code>auc</code> are identical.
</p>
</li>
<li> <p><code>ties</code>: <code>TRUE</code> if there are two or more tied predictions; <code>FALSE</code> if there are no ties.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">set.seed(0)
# Generate some simulated "actual" data
a &lt;- sample(c(TRUE, FALSE), 50, replace = TRUE)

# Generate some simulated predictions
p &lt;- runif(50) |&gt; round(2)
p[c(7, 8, 22, 35, 40, 41)] &lt;- 0.5

# Calculate AUCROC with its components
ar &lt;- aucroc(a, p)
ar$auc

</code></pre>


</div>