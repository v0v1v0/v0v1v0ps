<div class="container">

<table style="width: 100%;"><tr>
<td>scalreg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Scaled sparse linear regression</h2>

<h3>Description</h3>

<p>The algorithm gives the scaled Lasso solution with given penalty constants for a sparse linear regression. When the response vector is not set, the algorithm estimates the precision matrix of predictors. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">scalreg(X, y, lam0 = NULL, LSE = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>predictors, an n by p matrix with n &gt; 1 and p &gt; 1.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>response, an n-vector with n &gt; 1. If NULL, the algorithm computes the precision matrix of predictors.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam0</code></td>
<td>

<p>penalty constant; c("univ","quantile") or other specified numerical value.
If p &lt; 10^6, default is "quantile"; otherwise, default is "univ".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LSE</code></td>
<td>

<p>If TRUE, compute least squares estimates after scaled Lasso selection.
Default is FALSE.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Scaled sparse linear regression jointly estimates the regression coefficients and noise level in a linear model, described in details in Sun and Zhang (2012).  It alternates between estimating the noise level via the mean residual square and scaling the penalty in proportion to the estimated noise level. The theoretical performance of scaled Lasso with lam0="univ" was proven in Sun and Zhang (2012), while the quantile-based penalty level (lam0="quantile") was introduced and studied in Sun and Zhang (2013).
</p>
<p>Precision matrix estimation was described in details in Sun and Zhang (2013). The algorithm first estimates each column of the matrix by scaled sparse linear regression and then adjusts the matrix estimator to be symmetric.
</p>


<h3>Value</h3>

<p>A "scalreg" object is returned.
If it is a linear regression solution, some significant components of the object are:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>"regression".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hsigma</code></td>
<td>
<p>the estimated noise level.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>the estimated coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p>the fitted mean values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals</code></td>
<td>
<p>the residuals, that is response minus fitted values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lse</code></td>
<td>
<p>the object of least square estimation after the selection, which includes the similar values as "scalreg" (e.g. hsigma, coefficients, fitted.values, residual).</p>
</td>
</tr>
</table>
<p>If it estimates a precition matrix, some significant components of the object are:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>"precision matrix".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precision</code></td>
<td>
<p>the estimated precision matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hsigma</code></td>
<td>
<p>the estimated noise level for the linear regression problem of each column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lse</code></td>
<td>
<p>the object of least square estimation, containing values of precision and hsigma.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Tingni Sun &lt;tingni@wharton.upenn.edu&gt;
</p>


<h3>References</h3>

<p>Sun, T. and Zhang, C.-H. (2012) Scaled sparse linear regression. Biometrika, 99 (4), 879-898.
</p>
<p>Sun, T. and Zhang, C.-H. (2013) Sparse matrix inversion with scaled Lasso. Journal of Machine Learning Research, 14, 3385-3418.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(sp500)
attach(sp500)
x = sp500.percent[,3: (dim(sp500.percent)[2])]
y = sp500.percent[,1]

object = scalreg(x,y)
##print(object)

object = scalreg(x,y,LSE=TRUE)
print(object$hsigma)
print(object$lse$hsigma)

detach(sp500)
</code></pre>


</div>