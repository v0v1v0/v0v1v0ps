<div class="container">

<table style="width: 100%;"><tr>
<td>RFcv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross validation, n-fold for random forest (RF)</h2>

<h3>Description</h3>

<p>This function is a cross validation function for random forest.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RFcv(
  trainx,
  trainy,
  cv.fold = 10,
  mtry = if (!is.null(trainy) &amp;&amp; !is.factor(trainy)) max(floor(ncol(trainx)/3), 1) else
    floor(sqrt(ncol(trainx))),
  ntree = 500,
  predacc = "ALL",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>trainx</code></td>
<td>
<p>a dataframe or matrix contains columns of predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trainy</code></td>
<td>
<p>a vector of response, must have length equal to the number of
rows in trainx.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.fold</code></td>
<td>
<p>integer; number of folds in the cross-validation. if &gt; 1,
then apply n-fold cross validation; the default is 10, i.e., 10-fold cross
validation that is recommended.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtry</code></td>
<td>
<p>a function of number of remaining predictor variables to use as
the mtry parameter in the randomForest call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p>number of trees to grow. This should not be set to too small a
number, to ensure that every input row gets predicted at least a few times.
By default, 500 is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predacc</code></td>
<td>
<p>can be either "VEcv" for vecv or "ALL" for all measures
in function pred.acc.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>other arguments passed on to randomForest.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with the following components:
for numerical data: me, rme, mae, rmae, mse, rmse, rrmse, vecv and e1; or vecv.
for categorical data: correct classification rate (ccr), kappa (kappa), sensitivity (sens),
specificity (spec) and true skill statistic (tss)
</p>


<h3>Note</h3>

<p>This function is largely based on rf.cv (see Li et al. 2013) and
rfcv in randomForest.
</p>


<h3>Author(s)</h3>

<p>Jin Li
</p>


<h3>References</h3>

<p>Li, J., J. Siwabessy, M. Tran, Z. Huang, and A. Heap. 2013.
Predicting Seabed Hardness Using Random Forest in R. Pages 299-329 in Y.
Zhao and Y. Cen, editors. Data Mining Applications with R. Elsevier.
</p>
<p>Li, J. 2013. Predicting the spatial distribution of seabed gravel content
using random forest, spatial interpolation methods and their hybrid methods.
Pages 394-400  The International Congress on Modelling and Simulation
(MODSIM) 2013, Adelaide.
</p>
<p>Liaw, A. and M. Wiener (2002). Classification and Regression by
randomForest. R News 2(3), 18-22.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
data(hard)
data(petrel)

rfcv1 &lt;- RFcv(petrel[, c(1,2, 6:9)], petrel[, 5], predacc = "ALL")
rfcv1

n &lt;- 20 # number of iterations, 60 to 100 is recommended.
VEcv &lt;- NULL
for (i in 1:n) {
rfcv1 &lt;- RFcv(petrel[, c(1,2,6:9)], petrel[, 5], predacc = "VEcv")
VEcv [i] &lt;- rfcv1
}
plot(VEcv ~ c(1:n), xlab = "Iteration for RF", ylab = "VEcv (%)")
points(cumsum(VEcv) / c(1:n) ~ c(1:n), col = 2)
abline(h = mean(VEcv), col = 'blue', lwd = 2)

n &lt;- 20 # number of iterations, 60 to 100 is recommended.
measures &lt;- NULL
for (i in 1:n) {
rfcv1 &lt;- RFcv(hard[, c(4:6)], hard[, 17])
measures &lt;- rbind(measures, rfcv1$ccr) # for kappa, replace ccr with kappa
}
plot(measures ~ c(1:n), xlab = "Iteration for RF", ylab = "Correct
classification rate  (%)")
points(cumsum(measures) / c(1:n) ~ c(1:n), col = 2)
abline(h = mean(measures), col = 'blue', lwd = 2)

## End(Not run)

</code></pre>


</div>