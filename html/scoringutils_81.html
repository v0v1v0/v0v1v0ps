<div class="container">

<table style="width: 100%;"><tr>
<td>summarise_scores</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Summarise scores as produced by <code>score()</code>
</h2>

<h3>Description</h3>

<p>Summarise scores as produced by <code>score()</code>
</p>


<h3>Usage</h3>

<pre><code class="language-R">summarise_scores(
  scores,
  by = NULL,
  across = NULL,
  fun = mean,
  relative_skill = FALSE,
  relative_skill_metric = "auto",
  metric = deprecated(),
  baseline = NULL,
  ...
)

summarize_scores(
  scores,
  by = NULL,
  across = NULL,
  fun = mean,
  relative_skill = FALSE,
  relative_skill_metric = "auto",
  metric = deprecated(),
  baseline = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>scores</code></td>
<td>
<p>A data.table of scores as produced by <code>score()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>by</code></td>
<td>
<p>character vector with column names to summarise scores by. Default
is <code>NULL</code>, meaning that the only summary that takes is place is summarising
over samples or quantiles (in case of quantile-based forecasts), such that
there is one score per forecast as defined by the <em>unit of a single forecast</em>
(rather than one score for every sample or quantile).
The <em>unit of a single forecast</em> is determined by the columns present in the
input data that do not correspond to a metric produced by <code>score()</code>, which
indicate indicate a grouping of forecasts (for example there may be one
forecast per day, location and model). Adding additional, unrelated, columns
may alter results in an unpredictable way.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>across</code></td>
<td>
<p>character vector with column names from the vector of variables
that define the <em>unit of a single forecast</em> (see above) to summarise scores
across (meaning that the specified columns will be dropped). This is an
alternative to specifying <code>by</code> directly. If <code>NULL</code> (default), then <code>by</code> will
be used or inferred internally if also not specified. Only  one of <code>across</code>
and <code>by</code>  may be used at a time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun</code></td>
<td>
<p>a function used for summarising scores. Default is <code>mean</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>relative_skill</code></td>
<td>
<p>logical, whether or not to compute relative
performance between models based on pairwise comparisons.
If <code>TRUE</code> (default is <code>FALSE</code>), then a column called
'model' must be present in the input data. For more information on
the computation of relative skill, see <code>pairwise_comparison()</code>.
Relative skill will be calculated for the aggregation level specified in
<code>by</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>relative_skill_metric</code></td>
<td>
<p>character with the name of the metric for which
a relative skill shall be computed. If equal to 'auto' (the default), then
this will be either interval score, CRPS or Brier score (depending on which
of these is available in the input data)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt="[Deprecated]"></a> Deprecated in 1.1.0. Use
<code>relative_skill_metric</code> instead.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>baseline</code></td>
<td>
<p>character string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (<code>NULL</code>), relative skill will not be scaled with
respect to a baseline model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters that can be passed to the summary function
provided to <code>fun</code>. For more information see the documentation of the
respective function.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">
library(magrittr) # pipe operator

scores &lt;- score(example_continuous)
summarise_scores(scores)


# summarise over samples or quantiles to get one score per forecast
scores &lt;- score(example_quantile)
summarise_scores(scores)

# get scores by model
summarise_scores(scores,by = "model")

# get scores by model and target type
summarise_scores(scores, by = c("model", "target_type"))

# Get scores summarised across horizon, forecast date, and target end date
summarise_scores(
 scores, across = c("horizon", "forecast_date", "target_end_date")
)

# get standard deviation
summarise_scores(scores, by = "model", fun = sd)

# round digits
summarise_scores(scores,by = "model") %&gt;%
  summarise_scores(fun = signif, digits = 2)

# get quantiles of scores
# make sure to aggregate over ranges first
summarise_scores(scores,
  by = "model", fun = quantile,
  probs = c(0.25, 0.5, 0.75)
)

# get ranges
# summarise_scores(scores, by = "range")
</code></pre>


</div>