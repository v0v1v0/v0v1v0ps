<div class="container">

<table style="width: 100%;"><tr>
<td>svd.als</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>compute a low rank soft-thresholded svd by alternating orthogonal
ridge regression
</h2>

<h3>Description</h3>

<p>fit a low-rank svd to a complete matrix by alternating
orthogonal ridge regression. Special sparse-matrix classes available for very
large matrices, including "SparseplusLowRank" versions for row and
column centered sparse matrices.
</p>


<h3>Usage</h3>

<pre><code class="language-R">svd.als(x, rank.max = 2, lambda = 0, thresh = 1e-05, maxit = 100,
        trace.it = FALSE, warm.start = NULL, final.svd = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>An m by n matrix. Large matrices can be in "sparseMatrix" format, as
well as "SparseplusLowRank". The latter arise after centering sparse
matrices, for example with <code>biScale</code>, as well as in applications
such as <code>softImpute</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rank.max</code></td>
<td>

<p>The maximum rank for the solution. This is also the dimension of the
left and right matrices of orthogonal singular vectors. 'rank.max' should be no
bigger than 'min(dim(x)'.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>The regularization parameter. <code>lambda=0</code> corresponds to an
accelerated version of the orthogonal QR-algorithm. With <code>lambda&gt;0</code>
the algorithm amounts to alternating orthogonal ridge regression.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thresh</code></td>
<td>

<p>convergence threshold, measured as the relative changed in the Frobenius
norm between two successive estimates.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>

<p>maximum number of iterations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace.it</code></td>
<td>

<p>with <code>trace.it=TRUE</code>, convergence progress is reported.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warm.start</code></td>
<td>

<p>an svd object can be supplied as a warm start. If the solution requested
has higher rank than the warm start, the additional subspace is
initialized with random Gaussians (and then orthogonalized wrt the rest).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>final.svd</code></td>
<td>

<p>Although in theory, this algorithm converges to the solution to a
nuclear-norm regularized low-rank matrix approximation problem,
with potentially some singular values equal to zero, in practice only
near-zeros are achieved. This final step does one more iteration with
<code>lambda=0</code>,
followed by soft-thresholding.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This algorithm solves the problem
</p>
<p style="text-align: center;"><code class="reqn">\min ||X-M||_F^2 +\lambda ||M||_*</code>
</p>
<p> subject to <code class="reqn">rank(M)\leq
 r</code>, where <code class="reqn">||M||_*</code> is the nuclear norm of <code class="reqn">M</code> (sum of singular values).
It achieves this by solving the related problem
</p>
<p style="text-align: center;"><code class="reqn">\min ||X-AB'||_F^2 +\lambda/2 (||A||_F^2+||B||_F^2)</code>
</p>
<p> subject to
<code class="reqn">rank(A)=rank(B)\leq r</code>. The solution is a rank-restricted,
soft-thresholded SVD of <code class="reqn">X</code>.
</p>


<h3>Value</h3>

<p>An svd object is returned, with components "u", "d", and "v".
</p>
<table>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>an m by <code>rank.max</code> matrix with the left orthogonal singular
vectors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>a vector of length <code>rank.max</code> of soft-thresholded singular values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>an n by <code>rank.max</code> matrix with the right orthogonal singular
vectors</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Trevor Hastie, Rahul Mazumder<br>
Maintainer: Trevor Hastie  <a href="mailto:hastie@stanford.edu">hastie@stanford.edu</a>
</p>


<h3>References</h3>

<p>Rahul Mazumder, Trevor Hastie and Rob Tibshirani (2010)
<em>Spectral Regularization Algorithms for Learning Large Incomplete
Matrices</em>,
<a href="https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf">https://web.stanford.edu/~hastie/Papers/mazumder10a.pdf</a><br><em> Journal of Machine Learning Research 11 (2010) 2287-2322</em>
</p>


<h3>See Also</h3>

<p><code>biScale</code>, <code>softImpute</code>, <code>Incomplete</code>,
<code>lambda0</code>, <code>impute</code>, <code>complete</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">#create a matrix and run the algorithm
set.seed(101)
n=100
p=50
J=25
np=n*p
x=matrix(rnorm(n*J),n,J)%*%matrix(rnorm(J*p),J,p)+matrix(rnorm(np),n,p)/5
fit=svd.als(x,rank=25,lambda=50)
fit$d
pmax(svd(x)$d-50,0)
# now create a sparse matrix and do the same
nnz=trunc(np*.3)
inz=sample(seq(np),nnz,replace=FALSE)
i=row(x)[inz]
j=col(x)[inz]
x=rnorm(nnz)
xS=sparseMatrix(x=x,i=i,j=j)
fit2=svd.als(xS,rank=20,lambda=7)
fit2$d
pmax(svd(as.matrix(xS))$d-7,0)
</code></pre>


</div>