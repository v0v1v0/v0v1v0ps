<div class="container">

<table style="width: 100%;"><tr>
<td>sieve_solver</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate the coefficients for the basis functions</h2>

<h3>Description</h3>

<p>This is the main function that performs sieve estimation. It calculate the coefficients by solving a penalized lasso type problem.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sieve_solver(
  model,
  Y,
  l1 = TRUE,
  family = "gaussian",
  lambda = NULL,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a list. Typically, it is the output of Sieve::sieve_preprocess.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>a vector. The outcome variable. The length of Y equals to the training sample size, which should also match the row number of X in model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l1</code></td>
<td>
<p>a logical variable. TRUE means calculating the coefficients by sovling a l1-penalized empirical risk minimization problem. FALSE means solving a least-square problem. Default is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>a string. 'gaussian', mean-squared-error regression problem.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>same as the lambda of glmnet::glmnet.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>a number. Number of penalization hyperparameter used when solving the lasso-type problem. Default is 100.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a list. In addition to the preprocessing information, it also has the fitted value.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Phi</code></td>
<td>
<p>a matrix. This is the design matrix directly used by the next step model fitting. The (i,j)-th element of this matrix is the evaluation of i-th sample's feature at the j-th basis function. The dimension of this matrix is sample size x basisN.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a matrix. This is the rescaled original feature/predictor matrix. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_hat</code></td>
<td>
<p>a matrix. Dimension is basisN x nlambda. The j-th column corresponds to the fitted regression coeffcients using the j-th hyperparameter in lambda.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>a string. The type of basis funtion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>index_matrix</code></td>
<td>
<p>a matrix. It specifies what are the product basis functions used when constructing the design matrix Phi. It has a dimension basisN x dimension of original features. There are at most interaction_order many non-1 elements in each row.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>basisN</code></td>
<td>
<p>a number. Number of sieve basis functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm_para</code></td>
<td>
<p>a matrix. It records how each dimension of the feature/predictor is rescaled, which is useful when rescaling the testing sample's predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a vector. It records the penalization hyperparameter used when solving the lasso problems. Default has a length of 100, meaning the algorithm tried 100 different penalization hyperparameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>a string. 'gaussian', continuous numerical outcome, regression probelm; 'binomial', binary outcome, classification problem.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
#use 50 cosine basis functions
type &lt;- 'cosine'
basisN &lt;- 50 
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y)

###if the outcome is binary, 
###need to solve a nonparametric logistic regression problem
xdim &lt;- 1
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, y.type = 'binary', frho = 'nonlinear_binary')
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y,
                         family = 'binomial')
</code></pre>


</div>