<div class="container">

<table style="width: 100%;"><tr>
<td>spark_write_csv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Write a Spark DataFrame to a CSV</h2>

<h3>Description</h3>

<p>Write a Spark DataFrame to a tabular (typically, comma-separated) file.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_write_csv(
  x,
  path,
  header = TRUE,
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  mode = NULL,
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the ‘<span class="samp">⁠"hdfs://"⁠</span>’, ‘<span class="samp">⁠"s3a://"⁠</span>’ and ‘<span class="samp">⁠"file://"⁠</span>’ protocols.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>header</code></td>
<td>
<p>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delimiter</code></td>
<td>
<p>The character used to delimit each column, defaults to <code>,</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quote</code></td>
<td>
<p>The character used as a quote. Defaults to ‘<span class="samp">⁠'"'⁠</span>’.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>escape</code></td>
<td>
<p>The character used to escape other characters, defaults to <code>\</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>charset</code></td>
<td>
<p>The character set, defaults to <code>"UTF-8"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>null_value</code></td>
<td>
<p>The character to use for default values, defaults to <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
ignore. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partition_by</code></td>
<td>
<p>A <code>character</code> vector. Partitions the output by the given columns on the file system.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code>collect_from_rds()</code>,
<code>spark_insert_table()</code>,
<code>spark_load_table()</code>,
<code>spark_read()</code>,
<code>spark_read_avro()</code>,
<code>spark_read_binary()</code>,
<code>spark_read_csv()</code>,
<code>spark_read_delta()</code>,
<code>spark_read_image()</code>,
<code>spark_read_jdbc()</code>,
<code>spark_read_json()</code>,
<code>spark_read_libsvm()</code>,
<code>spark_read_orc()</code>,
<code>spark_read_parquet()</code>,
<code>spark_read_source()</code>,
<code>spark_read_table()</code>,
<code>spark_read_text()</code>,
<code>spark_save_table()</code>,
<code>spark_write_avro()</code>,
<code>spark_write_delta()</code>,
<code>spark_write_jdbc()</code>,
<code>spark_write_json()</code>,
<code>spark_write_orc()</code>,
<code>spark_write_parquet()</code>,
<code>spark_write_source()</code>,
<code>spark_write_table()</code>,
<code>spark_write_text()</code>
</p>


</div>