<div class="container">

<table style="width: 100%;"><tr>
<td>alvm.fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Auxiliary Linear Variance Model</h2>

<h3>Description</h3>

<p>Fits an Auxiliary Linear Variance Model (ALVM) to estimate the error
variances of a heteroskedastic linear regression model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">alvm.fit(
  mainlm,
  M = NULL,
  model = c("cluster", "spline", "linear", "polynomial", "basic", "homoskedastic"),
  varselect = c("none", "hettest", "cv.linear", "cv.cluster", "qgcv.linear",
    "qgcv.cluster"),
  lambda = c("foldcv", "qgcv"),
  nclust = c("elbow.swd", "elbow.mwd", "elbow.both", "foldcv"),
  clustering = NULL,
  polypen = c("L2", "L1"),
  d = 2L,
  solver = c("auto", "quadprog", "quadprogXT", "roi", "osqp"),
  tsk = NULL,
  tsm = NULL,
  constol = 1e-10,
  cvoption = c("testsetols", "partitionres"),
  nfolds = 5L,
  reduce2homosked = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mainlm</code></td>
<td>
<p>Either an object of <code>class</code> <code>"lm"</code>
(e.g., generated by <code>lm</code>), or
a list of two objects: a response vector and a design matrix. The objects
are assumed to be in that order, unless they are given the names
<code>"X"</code>  and <code>"y"</code> to distinguish them. The design matrix passed
in a list must begin with a column of ones if an intercept is to be
included in the linear model. The design matrix passed in a list should
not contain factors, as all columns are treated 'as is'. For tests that
use ordinary least squares residuals, one can also pass a vector of
residuals in the list, which should either be the third object or be
named <code>"e"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>An <code class="reqn">n\times n</code> annihilator matrix. If <code>NULL</code>
(the default), this will be calculated from the <code>mainlm</code> object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A character corresponding to the type of ALVM to be fitted:
<code>"cluster"</code> for the clustering ALVM, <code>"spline"</code> for the
thin-plate spline ALVM, <code>"linear"</code> for the linear ALVM,
<code>"polynomial"</code> for the penalised polynomial ALVM, <code>"basic"</code> for
the basic or naive ALVM, and <code>"homoskedastic"</code> for the
homoskedastic error variance estimator, <code class="reqn">e'e/(n-p)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>varselect</code></td>
<td>
<p>Either a character indicating how variable selection should
be conducted, or an integer vector giving indices of columns of the
predictor matrix (<code>model.matrix</code> of <code>mainlm</code>)
to select. The vector must include <code>1L</code> for the intercept to be
selected. If a character, it must be one of the following:
</p>

<ul>
<li> <p><code>"none"</code>: No variable selection is conducted;
</p>
</li>
<li> <p><code>"hettest"</code>: Variable selection is conducted by applying a
heteroskedasticity test with each feature in turn serving as the
‘deflator’ variable
</p>
</li>
<li> <p><code>"cv.linear"</code>: Variable selection is conducted by best subset
selection on the auxiliary linear variance model (linear specification),
using squared-error loss computed under <code class="reqn">K</code>-fold cross-validation
</p>
</li>
<li> <p><code>"cv.cluster"</code>: Variable selection is conducted by best subset
selection on the auxiliary linear variance model (clustering
specification), using squared-error loss computed under <code class="reqn">K</code>-fold
cross-validation
</p>
</li>
<li> <p><code>"qgcv.linear"</code>: Variable selection is conducted by best subset
selection on the auxiliary linear variance model (linear specification),
using squared-error loss computed under quasi-generalised
cross-validation
</p>
</li>
<li> <p><code>"qgcv.cluster"</code>: Variable selection is conducted by best subset
selection on the auxiliary linear variance model (clustering
specification), using squared-error loss computed under
quasi-generalised cross-validation
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Either a double of length 1 indicating the value of the
penalty hyperparameter <code class="reqn">\lambda</code>, or a character specifying the
tuning method for choosing <code class="reqn">\lambda</code>: <code>"foldcv"</code> for
<code class="reqn">K</code>-fold cross-validation (the default) or <code>"qgcv"</code> for
quasi-generalised cross-validation. This argument is ignored if
<code>model</code> is neither <code>"polynomial"</code> nor <code>"spline"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nclust</code></td>
<td>
<p>Either an integer of length 1 indicating the value of the
number of clusters <code class="reqn">n_c</code>, or a character specifying the tuning method
for choosing <code class="reqn">n_c</code>: <code>"elbow.swd"</code> for the elbow method using a
sum of within-cluster distances criterion,  <code>"elbow.mwd"</code> for the
elbow method using a maximum within-cluster distances criterion,
<code>"elbow.both"</code> for rounded average of the results of the previous
two, and <code>"foldcv"</code> for <code class="reqn">K</code>-fold cross-validation. This argument
is ignored if <code>model</code> is not <code>"cluster"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clustering</code></td>
<td>
<p>A list object of class <code>"doclust"</code>. If set to
<code>NULL</code> (the default), such an object is generated (ignored if
<code>cluster</code> is <code>FALSE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>polypen</code></td>
<td>
<p>A character, either <code>"L2"</code> or <code>"L1"</code>, indicating
whether an <code class="reqn">L_2</code> norm penalty (ridge regression) or
<code class="reqn">L_1</code> norm penalty (LASSO) should be used with the polynomial model.
This argument is ignored if <code>model</code> is not <code>"polynomial"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>An integer specifying the degree of polynomial to use in the
penalised polynomial ALVM; defaults to <code>2L</code>. Ignored if
<code>model</code> is other than <code>"polynomial"</code>. Setting <code>d</code> to
<code>1L</code> is not identical to setting <code>model</code> to <code>"linear"</code>,
because the linear ALVM does not have a penalty term in the objective
function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>solver</code></td>
<td>
<p>A character, indicating which Quadratic Programming solver
function to use to estimate <code class="reqn">\gamma</code>. The options are
<code>"quadprog"</code>, corresponding to
<code>solve.QP.compact</code> from the <span class="pkg">quadprog</span> package;
package; <code>"quadprogXT"</code>, corresponding to
<code>buildQP</code> from the <span class="pkg">quadprogXT</span> package;
<code>"roi"</code>, corresponding to the <code>qpoases</code> solver implemented in
<code>ROI_solve</code> from the <span class="pkg">ROI</span> package with
<span class="pkg">ROI.plugin.qpoases</span> add-on; and <code>"osqp"</code>, corresponding to
<code>solve_osqp</code> from the <span class="pkg">osqp</span> package.
Alternatively, the user can specify <code>"auto"</code> (the default), in which
case the function will select the solver that seems to work best for the
chosen model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tsk</code></td>
<td>
<p>An integer corresponding to the basis dimension <code>k</code> to be
passed to the <code>[mgcv]{s}</code> function for fitting of a thin-plate
spline ALVM; see <code>[mgcv]{choose.k}</code> for more details about
choosing the parameter and defaults. Ignored if <code>model</code> is not
<code>"spline"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tsm</code></td>
<td>
<p>An integer corresponding to the order <code>m</code> of the penalty
to be passed to the <code>[mgcv]{s}</code> function for fitting of a thin-plate
spline ALVM. If left as the default (<code>NULL</code>), it will be set to
2, corresponding to 2nd derivative penalties for a cubic spline.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constol</code></td>
<td>
<p>A double corresponding to the boundary value for the
constraint on error variances. Of course, the error variances must be
non-negative, but setting the constraint boundary to 0 can result in
zero estimates that then result in infinite weights for Feasible
Weighted Least Squares. The boundary value should thus be positive, but
small enough not to bias estimation of very small variances. Defaults to
<code>1e-10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cvoption</code></td>
<td>
<p>A character, either <code>"testsetols"</code> or
<code>"partitionres"</code>, indicating how to obtain the observed response
values for each test fold when performing <code class="reqn">K</code>-fold cross-validation
on an ALVM. The default technique, <code>"testsetols"</code>, entails fitting
a linear regression model to the test fold of observations from the
original response vector <code class="reqn">y</code> and predictor matrix <code class="reqn">X</code>. The
squared residuals from this regression are the observed
responses that are predicted from the trained model to compute the
cross-validated squared error loss function. Under the other technique,
<code>"partitionres"</code>, the squared residuals from the full
linear regression model are partitioned into training and test folds and
the squared residuals in the test fold are the observed responses that
are predicted for computation of the cross-validated loss.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>An integer specifying the number of folds <code class="reqn">K</code> to use for
cross-validation, if the <code class="reqn">\lambda</code> and/or <code class="reqn">n_c</code> hyperparameters
are to be tuned using cross-validation. Defaults to <code>5L</code>. One must
ensure that each test fold contains at least <code class="reqn">p+1</code> observations if
the <code>"testsetols"</code> technique is used with cross-validation, so that
there are enough degrees of freedom to fit a linear model to the test
fold.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduce2homosked</code></td>
<td>
<p>A logical indicating whether the homoskedastic
error variance estimator <code class="reqn">e'e/(n-p)</code> should be used if the
variable selection procedure does not select any variables. Defaults to
<code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other arguments that can be passed to (non-exported) helper
functions, namely:
</p>

<ul>
<li> <p><code>greedy</code>, a logical passed to the functions implementing best subset
selection, indicating whether or not to use a greedy search rather than
exhaustive search for the best subset. Defaults to <code>FALSE</code>, but
coerced to <code>TRUE</code> unconditionally if <code class="reqn">p&gt;9</code>.
</p>
</li>
<li> <p><code>distmetric</code>, a character specifying the distance metric to use in
computing distance for the clustering algorithm. Corresponds to the
<code>method</code> argument of <code>dist</code> and defaults to
<code>"euclidean"</code>
</p>
</li>
<li> <p><code>linkage</code>, a character specifying the linkage rule to use in
agglomerative hierarchical clustering. Corresponds to the <code>method</code>
argument of <code>hclust</code> and defaults to
<code>"complete"</code>
</p>
</li>
<li> <p><code>nclust2search</code>, an integer vector specifying the values of
<code class="reqn">n_c</code> to try when tuning <code class="reqn">n_c</code> by cross-validation. Defaults to
<code>1L:20L</code>
</p>
</li>
<li> <p><code>alpha</code>, a double specifying the significance level threshold to
use when applying heteroskedasticity test for the purpose of feature
selection in an ALVM; defaults to <code>0.1</code>
</p>
</li>
<li> <p><code>testname</code>, a character corresponding to the name of a function
that performs a heteroskedasticity test. The function must either be one
that takes a <code>deflator</code> argument or <code>breusch_pagan</code>.
Defaults to <code>evans_king</code>
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The ALVM model equation is
</p>
<p style="text-align: center;"><code class="reqn">e\circ e = (M \circ M)L \gamma + u</code>
</p>
<p>,
where <code class="reqn">e</code> is the Ordinary Least Squares residual vector, <code class="reqn">M</code> is
the annihilator matrix <code class="reqn">M=I-X(X'X)^{-1}X'</code>, <code class="reqn">L</code> is a linear
predictor matrix, <code class="reqn">u</code> is a random error vector, <code class="reqn">\gamma</code> is a
<code class="reqn">p</code>-vector of unknown parameters, and <code class="reqn">\circ</code> denotes the
Hadamard (elementwise) product. The construction of <code class="reqn">L</code> depends on
the method used to model or estimate the assumed heteroskedastic
function <code class="reqn">g(\cdot)</code>, a continuous, differentiable function that is
linear in <code class="reqn">\gamma</code> and by which the error variances <code class="reqn">\omega_i</code>
of the main linear model are related to the predictors <code class="reqn">X_{i\cdot}</code>.
This method has been developed as part of the author's doctoral research
project.
</p>
<p>Depending on the model used, the estimation method could be
Inequality-Constrained Least Squares or Inequality-Constrained Ridge
Regression. However, these are both special cases of Quadratic
Programming. Therefore, all of the models are fitted using Quadratic
Programming.
</p>
<p>Several techniques are available for feature selection within the model.
The LASSO-type model handles feature selection via a shrinkage penalty.
For this reason, if the user calls the polynomial model with
<code class="reqn">L_1</code>-norm penalty, it is not necessary to specify a variable
selection method, since this is handled automatically. Another feature
selection technique is to use a heteroskedasticity test that tests for
heteroskedasticity linked to a particular predictor variable (the
‘deflator’). This test can be conducted with each features in turn
serving as the deflator. Those features for which the null hypothesis of
homoskedasticity is rejected at a specified significance level
<code>alpha</code> are selected. A third feature selection technique is best
subset selection, where the model is fitted with all possible subsets of
features. The models are scored in terms of some metric, and the
best-performing subset of features is selected. The metric could be
squared-error loss computed under <code class="reqn">K</code>-fold cross-validation or using
quasi-generalised cross-validation. (The <em>quasi-</em> prefix refers to
the fact that generalised cross-validation is, properly speaking, only
applicable to a linear fitting method, as defined by
Hastie et al. (2009). ALVMs are not linear fitting
methods due to the inequality constraint). Since best subset selection
requires fitting <code class="reqn">2^{p-1}</code> models (where <code class="reqn">p-1</code> is the number of
candidate features), it is infeasible for large <code class="reqn">p</code>. A greedy search
technique can therefore be used as an alternative, where one begins with
a null model and adds the feature that leads to the best improvement in
the metric, stopping when no new feature leads to an improvement.
</p>
<p>The polynomial and thin-plate spline ALVMs have a penalty hyperparameter
<code class="reqn">\lambda</code> that must either be specified or tuned. <code class="reqn">K</code>-fold
cross-validation or quasi-generalised cross-validation can be used for
tuning. The clustering ALVM has a hyperparameter <code class="reqn">n_c</code>, the number of
clusters into which to group the observations (where error variances
are assumed to be equal within each cluster). <code class="reqn">n_c</code> can be specified
or tuned. The available tuning methods are an elbow method (using a
sum of within-cluster distances criterion, a maximum
within-cluster distance criterion, or a combination of the two) and
<code class="reqn">K</code>-fold cross-validation.
</p>


<h3>Value</h3>

<p>An object of class <code>"alvm.fit"</code>, containing the following:
</p>

<ul>
<li> <p><code>coef.est</code>, a vector of parameter estimates, <code class="reqn">\hat{\gamma}</code>
</p>
</li>
<li> <p><code>var.est</code>, a vector of estimates <code class="reqn">\hat{\omega}</code> of the error
variances for all observations
</p>
</li>
<li> <p><code>method</code>, a character corresponding to the <code>model</code> argument
</p>
</li>
<li> <p><code>ols</code>, the <code>lm</code> object corresponding to the original linear
regression model
</p>
</li>
<li> <p><code>fitinfo</code>, a list containing four named objects: <code>Msq</code> (the
elementwise-square of the annihilator matrix <code class="reqn">M</code>), <code>L</code> (the
linear predictor matrix <code class="reqn">L</code>), <code>clustering</code> (a list object
with results of the clustering procedure), and <code>gam.object</code>, an
object of class <code>"gam"</code> (see <code>gamObject</code>). The
last two are set to <code>NA</code> unless the clustering ALVM or thin-plate
spline ALVM is used, respectively
</p>
</li>
<li> <p><code>hyperpar</code>, a named list of hyperparameter values,
<code>lambda</code>, <code>nclust</code>, <code>tsk</code>, and <code>d</code>, and tuning
methods, <code>lambdamethod</code> and <code>nclustmethod</code>. Values
corresponding to unused hyperparameters are set to <code>NA</code>.
</p>
</li>
<li> <p><code>selectinfo</code>, a list containing two named objects,
<code>varselect</code> (the value of the eponymous argument), and
<code>selectedcols</code> (a numeric vector with column indices of <code class="reqn">X</code>
that were selected, with <code>1</code> denoting the intercept column)
</p>
</li>
<li> <p><code>pentype</code>, a character corresponding to the <code>polypen</code>
argument
</p>
</li>
<li> <p><code>solver</code>, a character corresponding to the <code>solver</code>
argument (or specifying the QP solver actually used, if <code>solver</code>
was set to <code>"auto"</code>)
</p>
</li>
<li> <p><code>constol</code>, a double corresponding to the <code>constol</code> argument
</p>
</li>
</ul>
<h3>References</h3>

<p>Hastie T, Tibshirani R, Friedman JH (2009).
<em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>, 2nd edition.
Springer, New York.
</p>


<h3>See Also</h3>

<p><code>alvm.fit</code>, <code>avm.ci</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">mtcars_lm &lt;- lm(mpg ~ wt + qsec + am, data = mtcars)
myalvm &lt;- alvm.fit(mtcars_lm, model = "cluster")

</code></pre>


</div>