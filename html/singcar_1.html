<div class="container">

<table style="width: 100%;"><tr>
<td>BSDT</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Standardised Difference Test</h2>

<h3>Description</h3>

<p>A test on the discrepancy between two tasks in a single case, by comparison
to the discrepancy of means in the same two tasks in a control sample. Can
take both tasks measured on the same scale with the same underlying
distribution or tasks measured on different scales by setting
<code>unstandardised</code> to <code>TRUE</code> or <code>FALSE</code> (default). Calculates a
standardised effects size of task discrepancy as well as a point estimate of
the proportion of the control population that would be expected to show a
more extreme discrepancy as well as relevant credible intervals. This test
is based on random number generation which means that results may vary
between runs. This is by design and the reason for not using <code>set.seed()</code>
to reproduce results inside the function is to emphasise the randomness of
the test. To get more accurate and stable results please increase the number
of iterations by increasing <code>iter</code> whenever feasible. Developed by
Crawford and Garthwaite (2007).
</p>


<h3>Usage</h3>

<pre><code class="language-R">BSDT(
  case_a,
  case_b,
  controls_a,
  controls_b,
  sd_a = NULL,
  sd_b = NULL,
  sample_size = NULL,
  r_ab = NULL,
  alternative = c("two.sided", "greater", "less"),
  int_level = 0.95,
  iter = 10000,
  unstandardised = FALSE,
  calibrated = TRUE,
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>case_a</code></td>
<td>
<p>Case's score on task A.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>case_b</code></td>
<td>
<p>Case's score on task B.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>controls_a</code></td>
<td>
<p>Controls' scores on task A. Takes either a vector of
observations or a single value interpreted as mean. <em>Note</em>: you can
supply a vector as input for task A while mean and SD for task B.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>controls_b</code></td>
<td>
<p>Controls' scores on task A. Takes either a vector of
observations or a single value interpreted as mean. <em>Note</em>: you can
supply a vector as input for task B while mean and SD for task A.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sd_a</code></td>
<td>
<p>If single value for task A is given as input you must
supply the standard deviation of the sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sd_b</code></td>
<td>
<p>If single value for task B is given as input you must
supply the standard deviation of the sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_size</code></td>
<td>
<p>If A or B is given as mean and SD you must supply the
sample size. If controls_a is given as vector and controls_b as mean and
SD, sample_size must equal the number of observations in controls_a.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r_ab</code></td>
<td>
<p>If A or B is given as mean and SD you must supply the
correlation between the tasks.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis,
must be one of <code>"two.sided"</code> (default), <code>"greater"</code> or
<code>"less"</code>. You can specify just the initial letter. Since the direction
of the expected effect depends on which task is set as A and which is set
as B, be very careful if changing this parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>int_level</code></td>
<td>
<p>Level of confidence for credible intervals, defaults to 95%.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of iterations, defaults to 10000. Greater number gives better
estimation but takes longer to calculate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unstandardised</code></td>
<td>
<p>Estimate z-value based on standardised or
unstandardised task scores. Set to <code>TRUE</code> only if tasks are measured on the
same scale with the same underlying distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>calibrated</code></td>
<td>
<p><code>TRUE</code> is default. Whether or not to use the standard theory (Jeffreys) prior
distribution (if set to <code>FALSE</code>) or a calibrated prior examined by
Berger and Sun (2008). The sample estimation of the covariance matrix is
based on the sample size being n - 1 when the calibrated prior is used. See
Crawford et al. (2011) for further information. Calibrated prior is
recommended.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>Remove <code>NA</code>s from controls.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Uses random generation of inverse wishart distributions from the
CholWishart package (Geoffrey Thompson, 2019).
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>

<table>
<tr>
<td style="text-align: left;"> <code>statistic</code>   </td>
<td style="text-align: left;"> the mean z-value over <code>iter</code>
  number of iterations. </td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"> <code>parameter</code> </td>
<td style="text-align: left;"> the degrees of freedom
  used to specify the posterior distribution. </td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"> <code>p.value</code>    </td>
<td style="text-align: left;">
  the mean p-value over <code>iter</code> number of iterations. </td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">
  <code>estimate</code> </td>
<td style="text-align: left;"> case scores expressed as z-scores on task A and B.
  Standardised effect size (Z-DCC) of task difference between case and
  controls and point estimate of the proportion of the control population
  estimated to show a more extreme task difference. </td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">  <code>null.value</code>
  </td>
<td style="text-align: left;"> the value of the difference under the null hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">
  <code>alternative</code>     </td>
<td style="text-align: left;"> a character string describing the alternative
  hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"> <code>method</code> </td>
<td style="text-align: left;"> a character string indicating what
  type of test was performed.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"> <code>data.name</code> </td>
<td style="text-align: left;"> a character string
  giving the name(s) of the data</td>
</tr>
</table>
<h3>References</h3>

<p>Berger, J. O., &amp; Sun, D. (2008). Objective Priors for the Bivariate Normal
Model. <em>The Annals of Statistics, 36</em>(2), 963-982. JSTOR.
</p>
<p>Crawford, J. R., &amp; Garthwaite, P. H. (2007). Comparison of a single case to a
control or normative sample in neuropsychology: Development of a Bayesian
approach. <em>Cognitive Neuropsychology, 24</em>(4), 343-372.
<a href="https://doi.org/10.1080/02643290701290146">doi:10.1080/02643290701290146</a>
</p>
<p>Crawford, J. R., Garthwaite, P. H., &amp; Ryan, K. (2011). Comparing a single
case to a control sample: Testing for neuropsychological deficits and
dissociations in the presence of covariates. <em>Cortex, 47</em>(10),
1166-1178. <a href="https://doi.org/10.1016/j.cortex.2011.02.017">doi:10.1016/j.cortex.2011.02.017</a>
</p>
<p>Geoffrey Thompson (2019). CholWishart: Cholesky Decomposition of the Wishart
Distribution. R package version 1.1.0.
<a href="https://CRAN.R-project.org/package=CholWishart">https://CRAN.R-project.org/package=CholWishart</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">BSDT(-3.857, -1.875, controls_a = 0, controls_b = 0, sd_a = 1,
sd_b = 1, sample_size = 20, r_ab = 0.68, iter = 100)

BSDT(case_a = size_weight_illusion[1, "V_SWI"], case_b = size_weight_illusion[1, "K_SWI"],
 controls_a = size_weight_illusion[-1, "V_SWI"],
 controls_b = size_weight_illusion[-1, "K_SWI"], iter = 100)

</code></pre>


</div>