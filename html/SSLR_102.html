<div class="container">

<table style="width: 100%;"><tr>
<td>SSLRDecisionTree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>General Interface Decision Tree model</h2>

<h3>Description</h3>

<p>Decision Tree is a simple and effective semi-supervised
learning method.
Based on the article "Semi-supervised classification trees".
It also offers many parameters to modify the behavior of this method.
It is the same as the traditional Decision Tree
algorithm, but the difference is how the gini coefficient is calculated (classification).
In regression we use SSE metric (different from the original investigation)
It can be used in classification or regression. If Y is numeric is for regression, classification in another case
</p>


<h3>Usage</h3>

<pre><code class="language-R">SSLRDecisionTree(
  max_depth = 30,
  w = 0.5,
  min_samples_split = 20,
  min_samples_leaf = ceiling(min_samples_split/3)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>max_depth</code></td>
<td>
<p>A number from 1 to Inf.
Is the maximum number of depth in Decision Tree
Default is 30</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>weight parameter ranging from 0 to 1. Default is 0.5</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_samples_split</code></td>
<td>
<p>the minimum number of observations to do split. Default is 20</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_samples_leaf</code></td>
<td>
<p>the minimum number of any terminal leaf node. Default is ceiling(min_samples_split/3)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In this model we can make predictions with prob type
</p>


<h3>References</h3>

<p>Jurica Levati, Michelangelo Ceci, Dragi Kocev, Saso Dzeroski.<br><em>Semi-supervised classification trees.</em><br>
Published online: 25 March 2017
Â© Springer Science Business Media New York 2017
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(tidyverse)
library(caret)
library(SSLR)
library(tidymodels)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(wine$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA


m &lt;- SSLRDecisionTree(min_samples_split = round(length(labeled.index) * 0.25),
                      w = 0.3,
                      ) %&gt;% fit(Wine ~ ., data = train)


#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)


#For probabilities
predict(m,test, type = "prob")

</code></pre>


</div>