<div class="container">

<table style="width: 100%;"><tr>
<td>textmodel_seqlda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sequential Latent Dirichlet allocation</h2>

<h3>Description</h3>

<p>Implements Sequential Latent Dirichlet allocation (Sequential LDA).
<code>textmodel_seqlda()</code> allows the users to classify sentences of texts. It
considers the topics of previous document in inferring the topics of currency
document. <code>textmodel_seqlda()</code> is a shortcut equivalent to
<code>textmodel_lda(gamma = 0.5)</code>. Seeded Sequential LDA is
<code>textmodel_seededlda(gamma = 0.5)</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textmodel_seqlda(
  x,
  k = 10,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  batch_size = 1,
  model = NULL,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a fitted LDA model; if provided, <code>textmodel_lda()</code> inherits
parameters from an existing model. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The same as <code>textmodel_lda()</code>
</p>


<h3>References</h3>

<p>Du, Lan et al. (2012). "Sequential Latent Dirichlet Allocation".
doi.org/10.1007/s10115-011-0425-1. <em>Knowledge and Information Systems</em>.
</p>
<p>Watanabe, Kohei &amp; Baturo, Alexander. (2023). "Seeded Sequential LDA:
A Semi-supervised Algorithm for Topic-specific Analysis of Sentences".
doi:10.1177/08944393231178605. <em>Social Science Computer Review</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500) %&gt;%
    corpus_reshape()
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.01, docfreq_type = "prop")

lda_seq &lt;- textmodel_seqlda(dfmt, k = 6, max_iter = 500) # 6 topics
terms(lda_seq)
topics(lda_seq)

</code></pre>


</div>