<div class="container">

<table style="width: 100%;"><tr>
<td>ProxADMM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Solving penalized Frobenius problem.</h2>

<h3>Description</h3>

<p>This function solves the optimization problem
</p>


<h3>Usage</h3>

<pre><code class="language-R">ProxADMM(A, del, lam, P, rho = 0.1, tol = 1e-06, maxiters = 100, verb = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>A symmetric matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>del</code></td>
<td>
<p>A non-negative scalar. Lower bound on eigenvalues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>A non-negative scalar. L1 penalty parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>P</code></td>
<td>
<p>Matrix with non-negative elements and dimension of A. Allows for
differing L1 penalty parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>ADMM parameter.  Can affect rate of convergence a lot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence threshold.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiters</code></td>
<td>
<p>Maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p>Controls whether to be verbose.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Minimize_X (1/2)||X - A||_F^2 + lam||P*X||_1 s.t. X &gt;= del * I.
</p>
<p>This is the prox function for the generalized gradient descent of Bien &amp;
Tibshirani 2011 (see full reference below).
</p>
<p>This is the R implementation of the algorithm in Appendix 3 of Bien, J., and
Tibshirani, R. (2011), "Sparse Estimation of a Covariance Matrix,"
Biometrika. 98(4). 807–820.  It uses an ADMM approach to solve the problem
</p>
<p>Minimize_X (1/2)||X - A||_F^2 + lam||P*X||_1 s.t. X &gt;= del * I.
</p>
<p>Here, the multiplication between P and X is elementwise.  The inequality in
the constraint is a lower bound on the minimum eigenvalue of the matrix X.
</p>
<p>Note that there are two variables X and Z that are outputted.  Both are
estimates of the optimal X.  However, Z has exact zeros whereas X has
eigenvalues at least del.  Running the ADMM algorithm long enough, these two
are guaranteed to converge.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Estimate of optimal X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z</code></td>
<td>
<p>Estimate of optimal X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj</code></td>
<td>
<p>Objective values.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jacob Bien and Rob Tibshirani
</p>


<h3>References</h3>

<p>Bien, J., and Tibshirani, R. (2011), "Sparse Estimation of a
Covariance Matrix," Biometrika. 98(4). 807–820.
</p>


<h3>See Also</h3>

<p>spcov
</p>


<h3>Examples</h3>

<pre><code class="language-R">
set.seed(1)
n &lt;- 100
p &lt;- 200
# generate a covariance matrix:
model &lt;- GenerateCliquesCovariance(ncliques=4, cliquesize=p / 4, 1)

# generate data matrix with x[i, ] ~ N(0, model$Sigma):
x &lt;- matrix(rnorm(n * p), ncol=p) %*% model$A
S &lt;- var(x)

# compute sparse, positive covariance estimator:
P &lt;- matrix(1, p, p)
diag(P) &lt;- 0
lam &lt;- 0.1
aa &lt;- ProxADMM(S, 0.01, lam, P)

</code></pre>


</div>