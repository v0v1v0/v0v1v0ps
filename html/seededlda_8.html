<div class="container">

<table style="width: 100%;"><tr>
<td>textmodel_lda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Unsupervised Latent Dirichlet allocation</h2>

<h3>Description</h3>

<p>Implements unsupervised Latent Dirichlet allocation (LDA). Users can run
Seeded LDA by setting <code>gamma &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textmodel_lda(
  x,
  k = 10,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  gamma = 0,
  adjust_alpha = 0,
  model = NULL,
  update_model = FALSE,
  batch_size = 1,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>a parameter to determine change of topics between sentences or
paragraphs. When <code>gamma &gt; 0</code>, Gibbs sampling of topics for the current
document is affected by the previous document's topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjust_alpha</code></td>
<td>
<p>[experimental] if <code>adjust_alpha &gt; 0</code>, automatically adjust
<code>alpha</code> by the size of the topics. The smallest value of adjusted <code>alpha</code>
will be <code>alpha * (1 - adjust_alpha)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a fitted LDA model; if provided, <code>textmodel_lda()</code> inherits
parameters from an existing model. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>update_model</code></td>
<td>
<p>if <code>TRUE</code>, update the terms of <code>model</code> to recognize unseen
words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>auto_iter = TRUE</code>, the iteration stops even before <code>max_iter</code>
when <code>delta &lt;= 0</code>. <code>delta</code> is computed to measure the changes in the number
of words whose topics are updated by the Gibbs sampler in every 100
iteration as shown in the verbose message.
</p>
<p>If <code>batch_size &lt; 1.0</code>, the corpus is partitioned into sub-corpora of
<code>ndoc(x) * batch_size</code> documents for Gibbs sampling in sub-processes with
synchronization of parameters in every 10 iteration. Parallel processing is
more efficient when <code>batch_size</code> is small (e.g. 0.01). The algorithm is the
Approximate Distributed LDA proposed by Newman et al. (2009). User can
changed the number of sub-processes used for the parallel computing via
<code>options(seededlda_threads)</code>.
</p>
<p><code>set.seed()</code> should be called immediately before <code>textmodel_lda()</code> or
<code>textmodel_seededlda()</code> to control random topic assignment. If the random
number seed is the same, the serial algorithm produces identical results;
the parallel algorithm produces non-identical results because it
classifies documents in different orders using multiple processors.
</p>
<p>To predict topics of new documents (i.e. out-of-sample), first, create a
new LDA model from a existing LDA model passed to <code>model</code> in
<code>textmodel_lda()</code>; second, apply <code>topics()</code> to the new model. The <code>model</code>
argument takes objects created either by <code>textmodel_lda()</code> or
<code>textmodel_seededlda()</code>.
</p>


<h3>Value</h3>

<p>Returns a list of model parameters:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>last_iter</code></td>
<td>
<p>the number of iterations in Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>the maximum number of iterations in Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auto_iter</code></td>
<td>
<p>the use of <code>auto_iter</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjust_alpha</code></td>
<td>
<p>the value of <code>adjust_alpha</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the smoothing parameter for <code>theta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the smoothing parameter for <code>phi</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>the amount of adjustment for <code>adjust_alpha</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>the gamma parameter for Sequential LDA.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phi</code></td>
<td>
<p>the distribution of words over topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>the distribution of topics over documents.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>words</code></td>
<td>
<p>the raw frequency count of words assigned to topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>the original input of <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the command used to execute the function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>version</code></td>
<td>
<p>the version of the seededlda package.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Newman, D., Asuncion, A., Smyth, P., &amp; Welling, M. (2009). Distributed
Algorithms for Topic Models. The Journal of Machine Learning Research, 10,
1801â€“1828.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500)
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.1, docfreq_type = "prop")

lda &lt;- textmodel_lda(dfmt, k = 6, max_iter = 500) # 6 topics
terms(lda)
topics(lda)

</code></pre>


</div>