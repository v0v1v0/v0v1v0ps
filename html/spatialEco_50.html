<div class="container">

<table style="width: 100%;"><tr>
<td>kl.divergence</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler divergence (relative entropy)</h2>

<h3>Description</h3>

<p>Calculates the Kullback-Leibler divergence (relative entropy)
</p>


<h3>Usage</h3>

<pre><code class="language-R">kl.divergence(object, eps = 10^-4, overlap = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Matrix or dataframe object with &gt;=2 columns</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Probabilities below this threshold are replaced by this 
threshold for numerical stability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overlap</code></td>
<td>
<p>Logical, do not determine the KL divergence for those 
pairs where for each point at least one of the densities 
has a value smaller than eps.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Calculates the Kullback-Leibler divergence (relative entropy) between 
unweighted theoretical component distributions. Divergence is calculated 
as: int [f(x) (log f(x) - log g(x)) dx] for distributions with densities 
f() and g().
</p>


<h3>Value</h3>

<p>pairwise Kullback-Leibler divergence index (matrix)
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Evans  &lt;jeffrey_evans@tnc.org&gt;
</p>


<h3>References</h3>

<p>Kullback S., and R. A. Leibler (1951) On information and sufficiency.  
The Annals of Mathematical Statistics 22(1):79-86
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- seq(-3, 3, length=200)
y &lt;- cbind(n=dnorm(x), t=dt(x, df=10))
  matplot(x, y, type='l')
    kl.divergence(y)
   
# extract value for last column
  kl.divergence(y[,1:2])[3:3]

</code></pre>


</div>