<div class="container">

<table style="width: 100%;"><tr>
<td>diagDA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Diagonal Discriminant Analysis</h2>

<h3>Description</h3>

<p>This function implements a simple Gaussian maximum likelihood
discriminant rule, for diagonal class covariance matrices.
</p>
<p>In machine learning lingo, this is called “Naive Bayes” (for
continuous predictors).  Note that naive Bayes is more general, as it
models discrete predictors as multinomial, i.e., binary predictor
variables as Binomial / Bernoulli.
</p>


<h3>Usage</h3>

<pre><code class="language-R">dDA(x, cll, pool = TRUE)
## S3 method for class 'dDA'
predict(object, newdata, pool = object$pool, ...)
## S3 method for class 'dDA'
print(x, ...)

diagDA(ls, cll, ts, pool = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x,ls</code></td>
<td>
<p>learning set data matrix, with rows corresponding to
cases (e.g., mRNA samples) and columns to predictor variables
(e.g., genes).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cll</code></td>
<td>
<p>class labels for learning set, must be consecutive integers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>object of class <code>dDA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ts, newdata</code></td>
<td>
<p>test set (prediction) data matrix, with rows corresponding
to cases and columns to predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pool</code></td>
<td>
<p>logical flag.  If true (by default), the covariance matrices
are assumed to be constant across classes and the discriminant rule
is linear in the data.  Otherwise (<code>pool= FALSE</code>), the
covariance matrices may vary across classes and the discriminant
rule is quadratic in the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments passed to and from methods.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>dDA()</code> returns an object of class <code>dDA</code> for which there are
<code>print</code> and <code>predict</code> methods.  The latter
returns the same as <code>diagDA()</code>:
</p>
<p><code>diagDA()</code> returns an integer vector of class predictions for the
test set.
</p>


<h3>Author(s)</h3>


<p>Sandrine Dudoit, <a href="mailto:sandrine@stat.berkeley.edu">sandrine@stat.berkeley.edu</a>  and<br>
Jane Fridlyand, <a href="mailto:janef@stat.berkeley.edu">janef@stat.berkeley.edu</a> originally wrote
<code>stat.diag.da()</code> in CRAN package <a href="https://CRAN.R-project.org/package=sma"><span class="pkg">sma</span></a> which was modified
for speedup by Martin Maechler <a href="mailto:maechler@R-project.org">maechler@R-project.org</a>
who also introduced <code>dDA</code> etc.
</p>


<h3>References</h3>

<p>S. Dudoit, J. Fridlyand, and T. P. Speed. (2000)
Comparison of Discrimination Methods for the Classification of Tumors
Using Gene Expression Data.
(Statistics, UC Berkeley, June 2000, Tech Report #576)
</p>


<h3>See Also</h3>

<p><code>lda</code> and <code>qda</code> from the
<a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> package;
<code>naiveBayes</code> from <a href="https://CRAN.R-project.org/package=e1071"><span class="pkg">e1071</span></a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## two artificial examples by Andreas Greutert:
d1 &lt;- data.frame(x = c(1, 5, 5, 5, 10, 25, 25, 25, 25, 29),
                 y = c(4, 1, 2, 4,  4,  4,     6:8,     7))
n.plot(d1)
library(cluster)
(cl1P &lt;- pam(d1,k=4)$cluster) # 4 surprising clusters
with(d1, points(x+0.5, y, col = cl1P, pch =cl1P))

i1 &lt;- c(1,3,5,6)
tr1 &lt;- d1[-i1,]
cl1. &lt;- c(1,2,1,2,1,3)
cl1  &lt;- c(2,2,1,1,1,3)
plot(tr1, cex=2, col = cl1, pch = 20+cl1)
(dd.&lt;- diagDA(tr1, cl1., ts = d1[ i1,]))# ok
(dd &lt;- diagDA(tr1, cl1 , ts = d1[ i1,]))# ok, too!
points(d1[ i1,], pch = 10, cex=3, col = dd)

## use new fit + predict instead :
(r1 &lt;- dDA(tr1, cl1))
(r1.&lt;- dDA(tr1, cl1.))
stopifnot(dd == predict(r1,  new = d1[ i1,]),
          dd.== predict(r1., new = d1[ i1,]))

plot(tr1, cex=2, col = cl1, bg = cl1, pch = 20+cl1,
     xlim=c(1,30), ylim= c(0,10))
xy &lt;- cbind(x= runif(500, min=1,max=30), y = runif(500, min=0, max=10))
points(xy, cex= 0.5, col = predict(r1, new = xy))
abline(v=c( mean(c(5,25)), mean(c(25,29))))

## example where one variable xj has  Var(xj) = 0:
x4 &lt;- matrix(c(2:4,7, 6,8,5,6,  7,2,3,1, 7,7,7,7), ncol=4)
y &lt;- c(2,2, 1,1)
m4.1 &lt;- dDA(x4, y, pool = FALSE)
m4.2 &lt;- dDA(x4, y, pool = TRUE)
xx &lt;- matrix(c(3,7,5,7), ncol=4)
predict(m4.1, xx)## gave integer(0) previously
predict(m4.2, xx)
</code></pre>


</div>