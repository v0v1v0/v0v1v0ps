<div class="container">

<table style="width: 100%;"><tr>
<td>profanity</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compute Profanity Rate</h2>

<h3>Description</h3>

<p>Detect the rate of profanity at the sentence level.  This method uses a simple
dictionary lookup to find profane words and then compute the rate per sentence.
The <code>profanity</code> score ranges between 0 (no profanity used) and 1 (all
words used were profane).  Note that a single profane phrase would count as 
just one in the <code>profanity_count</code> column but would count as two words in
the <code>word_count</code> column.
</p>


<h3>Usage</h3>

<pre><code class="language-R">profanity(
  text.var,
  profanity_list = unique(tolower(lexicon::profanity_alvarez)),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text.var</code></td>
<td>
<p>The text variable.  Can be a <code>get_sentences</code> object or
a raw character vector though <code>get_sentences</code> is preferred as it avoids
the repeated cost of doing sentence boundary disambiguation every time
<code>sentiment</code> is run.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>profanity_list</code></td>
<td>
<p>A atomic character vector of profane words.  The 
<span class="pkg">lexicon</span> package has lists that can be used, including: 
</p>

<ul>
<li> <p><code>unique(tolower(lexicon::profanity_alvarez))</code>
</p>
</li>
<li> <p><code>lexicon::profanity_arr_bad</code>
</p>
</li>
<li> <p><code>lexicon::profanity_banned</code>
</p>
</li>
<li> <p><code>lexicon::profanity_zac_anger</code>
</p>
</li>
<li> <p><code>lexicon::profanity_racist</code>
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>ignored.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns a <span class="pkg">data.table</span> of:
</p>

<ul>
<li>
<p>  element_id - The id number of the original vector passed to <code>profanity</code>
</p>
</li>
<li>
<p>  sentence_id - The id number of the sentences within each <code>element_id</code>
</p>
</li>
<li>
<p>  word_count - Word count
</p>
</li>
<li>
<p>  profanity_count - Count of the number of profane words
</p>
</li>
<li>
<p> profanity - A score of the percentage of profane words
</p>
</li>
</ul>
<h3>See Also</h3>

<p>Other profanity functions: 
<code>profanity_by()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
bw &lt;- sample(unique(tolower(lexicon::profanity_alvarez)), 4)
mytext &lt;- c(
   sprintf('do you like this %s?  It is %s. But I hate really bad dogs', bw[1], bw[2]),
   'I am the best friend.',
   NA,
   sprintf('I %s hate this %s', bw[3], bw[4]),
   "Do you really like it?  I'm not happy"
)

## works on a character vector but not the preferred method avoiding the 
## repeated cost of doing sentence boundary disambiguation every time 
## `profanity` is run
profanity(mytext)

## preferred method avoiding paying the cost 
mytext2 &lt;- get_sentences(mytext)
profanity(mytext2)

plot(profanity(mytext2))

brady &lt;- get_sentences(crowdflower_deflategate)
brady_swears &lt;- profanity(brady)
brady_swears

## Distribution of profanity proportion for all comments
hist(brady_swears$profanity)
sum(brady_swears$profanity &gt; 0)

## Distribution of proportions for those profane comments
hist(brady_swears$profanity[brady_swears$profanity &gt; 0])

combo &lt;- combine_data()
combo_sentences &lt;- get_sentences(crowdflower_deflategate)
racist &lt;- profanity(combo_sentences, profanity_list = lexicon::profanity_racist)
combo_sentences[racist$profanity &gt; 0, ]$text
extract_profanity_terms(
    combo_sentences[racist$profanity &gt; 0, ]$text, 
    profanity_list = lexicon::profanity_racist
)

## Remove jerry, que, and illegal from the list
library(textclean)

racist2 &lt;- profanity(
    combo_sentences, 
    profanity_list = textclean::drop_element_fixed(
        lexicon::profanity_racist, 
        c('jerry', 'illegal', 'que')
    )
)
combo_sentences[racist2$profanity &gt; 0, ]$text

## End(Not run)
</code></pre>


</div>