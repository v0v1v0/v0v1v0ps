<div class="container">

<table style="width: 100%;"><tr>
<td>sensiHSIC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sensitivity Indices based on the Hilbert-Schmidt Independence Criterion (HSIC)</h2>

<h3>Description</h3>

 
<p><code>sensiHSIC</code> allows to conduct <b>global sensitivity analysis (<abbr><span class="acronym">GSA</span></abbr>)</b> in many different contexts thanks to several sensitivity measures based on the <b>Hilbert-Schmidt independence criterion (<abbr><span class="acronym">HSIC</span></abbr>)</b>. The so-called <abbr><span class="acronym">HSIC</span></abbr> sensitivity indices depend on the kernels which are affected to the input variables <code class="reqn">Xi</code> as well as on the kernel which is affected to the output object <code class="reqn">Y</code>. For each random entity, a reproducing kernel Hilbert space (<abbr><span class="acronym">RKHS</span></abbr>) is associated to the chosen kernel and allows to represent probability distributions in an appropriate function space. The influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> is then measured through the distance between the joint probability distribution (true impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code> in the numerical model) and the product of marginal distributions (no impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code>) after embedding those distributions into a bivariate <abbr><span class="acronym">RKHS</span></abbr>. Such a GSA approach has three main advantages:
</p>

<ul>
<li>
<p> The input variables <code class="reqn">Xi</code> may be correlated.
</p>
</li>
<li>
<p> Any kind of mathematical object is supported (provided that a kernel function is available).
</p>
</li>
<li>
<p> Accurate estimation is possible even in presence of very few data (no more than a hundred of input-output samples).
</p>
</li>
</ul>
<p>In <code>sensiHSIC</code>, each input variable <code class="reqn">Xi</code> is expected to be scalar (either discrete or continous). On the contrary, a much wider collection of mathematical objects are supported for the output variable <code class="reqn">Y</code>. In particular, <code class="reqn">Y</code> may be:
</p>

<ul>
<li>
<p> A <b>scalar output</b> (either discrete or continous). If so, one single kernel family is selected among the kernel collection.
</p>
</li>
<li>
<p> A <b>low-dimensional vector output</b>. If so, a kernel is selected for each output variable and the final output kernel is built by <b>tensorization</b>.
</p>
</li>
<li>
<p> A <b>high-dimensional vector output</b> or a <b>functional output</b>. In this case, the output data must be seen as time series observations. Three different methods are proposed.
</p>

<ol>
<li>
<p> A preliminary <b>dimension reduction</b> may be performed. In order to achieve this, a <b>principal component analysis (<abbr><span class="acronym">PCA</span></abbr>)</b> based on the empirical covariance matrix helps identify the first terms of the Kharunen-Loeve expansion. The final output kernel is then built in the reduced subspace where the functional data are projected.
</p>
</li>
<li>
<p> The <b>dynamic time warping (<abbr><span class="acronym">DTW</span></abbr>)</b> algorithm may be combined with a translation-invariant kernel. The resulting <abbr><span class="acronym">DTW</span></abbr>-based output kernel is well-adapted to measure similarity between two given time series.
</p>
</li>
<li>
<p> The <b>global alignment kernel (<abbr><span class="acronym">GAK</span></abbr>)</b> may be directly applied on the functional data. Unlike the <abbr><span class="acronym">DTW</span></abbr> kernel, it was specifically designed to deal with time series and to measure the impact of one input variable on the shape of the output curve.
</p>
</li>
</ol>
</li>
</ul>
<p>Many variants of the original <abbr><span class="acronym">HSIC</span></abbr> indices are now available in <code>sensiHSIC</code>.
</p>

<ul>
<li> <p><b>Normalized <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">R2-HSIC</span></abbr>)</b> <br>
The original <abbr><span class="acronym">HSIC</span></abbr> indices defined in Gretton et al. (2005) may be hard to interpret because they do not admit a universal upper bound. A first step to overcome this difficulty was enabled by Da Veiga (2015) with the definition of the <abbr><span class="acronym">R2-HSIC</span></abbr> indices. The resulting sensitivity indices can no longer be greater than <code class="reqn">1</code>.
</p>
</li>
<li> <p><b>Target <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">T-HSIC</span></abbr>)</b> <br>
They were thought by Marrel and Chabridon (2021) to meet the needs of <b>target sensitivity analysis (<abbr><span class="acronym">TSA</span></abbr>)</b>. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on a specific part of the output distribution (for example the upper tail). To achieve this, a weight function <code class="reqn">w</code> is applied on <code class="reqn">Y</code> before computing <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
</li>
<li> <p><b>Conditional <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">C-HSIC</span></abbr>)</b> <br>
They were thought by Marrel and Chabridon (2021) to meet the needs of <b>conditional sensitivity analysis (<abbr><span class="acronym">CSA</span></abbr>)</b>. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on <code class="reqn">Y</code> when a specific event occurs. This conditioning event is detected on the output variable <code class="reqn">Y</code> and its amplitude is taken into account thanks to a weight function <code class="reqn">w</code>.
</p>
</li>
<li> <p><b><abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices</b> <br>
To improve the interpretability of <abbr><span class="acronym">HSIC</span></abbr> indices, Da Veiga (2021) came up with an <b>ANOVA-like decomposition</b> that allows to establish a strict separation of main effects and interaction effects in the <abbr><span class="acronym">HSIC</span></abbr> paradigm. The first-order and total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are then defined just as the first-order and total-order Sobol' indices. However, this framework only holds if two major assumptions are verified: the input variables <code class="reqn">Xi</code> must be mutually independent and all input kernels must belong to the very restrained class of <abbr><span class="acronym">ANOVA</span></abbr> kernels. 
</p>
</li>
</ul>
<p>As most sensitivity measures, <abbr><span class="acronym">HSIC</span></abbr> indices allow to rank the input variables <code class="reqn">Xi</code> according to the influence they have on the output variable <code class="reqn">Y</code>. They can also be used for a screening purpose, that is to distinguish between truly influential input variables and non-influential input variables. The user who is interested in this topic is invited to consult the documentation of the function <code>testHSIC</code>. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">sensiHSIC(model = NULL, X, target = NULL, cond = NULL, 
          kernelX = "rbf", paramX = NA,
          kernelY = "rbf", paramY = NA,
          estimator.type = "V-stat",
          nboot = 0, conf = 0.95,
          anova = list(obj = "no", is.uniform = TRUE),
          sensi = NULL, 
          save.GM = list(KX = TRUE, KY = TRUE), ...)
          
## S3 method for class 'sensiHSIC'
tell(x, y = NULL, ...)

## S3 method for class 'sensiHSIC'
print(x, ...)

## S3 method for class 'sensiHSIC'
plot(x, ylim = c(0, 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A function, or a statistical model with a <code>predict</code> method. It defines the input-output model that needs to be studied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A <code class="reqn">n</code>-by-<code class="reqn">p</code> matrix containing all input samples. It comprises <code class="reqn">n</code> joint observations of the <code class="reqn">p</code> input variables.
</p>

<ul>
<li>
<p> If the user is only wanting to estimate <abbr><span class="acronym">HSIC</span></abbr> indices or <abbr><span class="acronym">R2-HSIC</span></abbr> indices, the input variables can be correlated.
</p>
</li>
<li>
<p> If the user is also wanting to estimate <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices, the input variables have to be mutually independent.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>target</code></td>
<td>
<p>A list of options to perform TSA. At least, <code>target</code> must contain an option named <code>"c"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>type</code> is a string specifying the weight function. Available choices include <code>"indicTh"</code>, <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>. Default value is <code>"exp1side"</code>.
</p>

<ul>
<li> <p><code>"indicTh"</code> and <code>"zeroTh"</code> only depend on a threshold parameter. 
</p>
</li>
<li> <p><code>"logistic"</code> and <code>"exp1side"</code> depend on both a threshold parameter and a smoothness parameter.
</p>
</li>
</ul>
</li>
<li> <p><code>c</code> is a scalar value specifying the threshold parameter.
</p>
</li>
<li> <p><code>upper</code> is a boolean indicating whether the target region is located above (<code>TRUE</code>) or below (<code>FALSE</code>) the threshold parameter <code>c</code>. Only relevant when <code>type</code> is <code>"indicTh"</code>, <code>"zeroTh"</code> or <code>"exp1side"</code>. Default value is <code>TRUE</code>. 
</p>
</li>
<li> <p><code>param</code> is a scalar value specifying the smoothness parameter. Only relevant when <code>type</code> is <code>"logistic"</code> or <code>"exp1side"</code>. Default value is <code>1</code>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cond</code></td>
<td>
<p>A list of options to perform CSA. At least, <code>cond</code> must contain an option named <code>"c"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>type</code> is a string specifying the weight function. Available choices include <code>"indicTh"</code>, <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>. Default value is <code>"exp1side"</code>.
</p>

<ul>
<li> <p><code>"indicTh"</code> and <code>"zeroTh"</code> only depend on a threshold parameter. 
</p>
</li>
<li> <p><code>"logistic"</code> and <code>"exp1side"</code> depend on both a threshold parameter and a smoothness parameter.
</p>
</li>
</ul>
</li>
<li> <p><code>c</code> is a scalar value specifying the threshold parameter.
</p>
</li>
<li> <p><code>upper</code> is a boolean indicating whether the conditioning region is located above (<code>TRUE</code>) or below (<code>FALSE</code>) the threshold parameter <code>c</code>. Only relevant when <code>type</code> is <code>"indicTh"</code>, <code>"zeroTh"</code> or <code>"exp1side"</code>. Default value is <code>TRUE</code>. 
</p>
</li>
<li> <p><code>param</code> is a scalar value specifying the smoothness parameter. Only relevant if <code>type</code> is <code>"logistic"</code> or <code>"exp1side"</code>. Default value is <code>1</code>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernelX</code></td>
<td>
<p>A string or a vector of <code class="reqn">p</code> strings that specifies how to choose input kernels.
</p>

<ul>
<li>
<p> If only one string is provided, the associated kernel is affected to all inputs.
</p>
</li>
<li>
<p> For dimension-wise kernel selection, a vector of <code class="reqn">p</code> strings must be provided.
</p>
</li>
</ul>
<p>For each input variable, available choices include <code>"categ"</code> (categorical kernel), <code>"dcov"</code> (covariance kernel of the fractional Brownian motion), <code>"invmultiquad"</code> (inverse multiquadratic kernel), <code>"laplace"</code> (exponential kernel), <code>"linear"</code> (dot-product kernel), <code>"matern3"</code> (Matern <code class="reqn">3/2</code> kernel), <code>"matern5"</code> (Matern <code class="reqn">5/2</code> kernel), <code>"raquad"</code> (rationale quadratic kernel), <code>"rbf"</code> (Gaussian kernel), <code>"sobolev1"</code> (Sobolev kernel with smoothness parameter <code class="reqn">r=1</code>) and <code>"sobolev2"</code> (Sobolev kernel with smoothness parameter <code class="reqn">r=2</code>).
</p>
<p>In addition, let us assume that all input variables are uniformly distributed on <code class="reqn">[0,1]</code>. Under this assumption, the kernels <code>"laplace"</code>, <code>"matern3"</code>, <code>"matern5"</code> and <code>"rbf"</code> can be easily transformed into <abbr><span class="acronym">ANOVA</span></abbr> kernels. The resulting kernels are respectively called <code>"laplace_anova"</code>, <code>"matern3_anova"</code>, <code>"matern5_anova"</code> and <code>"rbf_anova"</code>.
</p>

<ul>
<li>
<p> One-parameter kernels: <code>"categ"</code>, <code>"dcov"</code>, <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"laplace_anova"</code>, <code>"matern3"</code>, <code>"matern3_anova"</code>, <code>"matern5"</code>, <code>"matern5_anova"</code>, <code>"raquad"</code>, <code>"rbf"</code> and <code>"rbf_anova"</code>.
</p>
</li>
<li>
<p> Parameter-free kernels: <code>"linear"</code>, <code>"sobolev1"</code> and <code>"sobolev2"</code>. 
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramX</code></td>
<td>
<p>A scalar value or a vector of <code class="reqn">p</code> values with input kernel parameters.
</p>

<ul>
<li>
<p> If <code>paramX=NA</code>, input kernel parameters are computed automatically with rules of thumb.
</p>
</li>
<li>
<p> If <code>paramX</code> is a scalar value, it is affected to all input kernels.
</p>
</li>
<li>
<p> For dimension-wise kernel parametrization, a vector of <code class="reqn">p</code> values must be provided. If <code>kernelX</code> combines one-parameter kernels and parameter-free kernels, <code>NA</code> must be specified for parameter-free kernels.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernelY</code></td>
<td>
<p>A string, a vector of <code class="reqn">q</code> strings or a list of options that specifies how to construct the output kernel. Regardless of its mathematical nature, the model output must be envisioned as a <code class="reqn">q</code>-dimensional random vector.
</p>
<p>To deal with a <b>scalar output</b> or a <b>low-dimensional vector output</b>, it is advised to select one kernel per output dimension and to tensorize all selected kernels. In this case, <code>kernelY</code> must be a string or a vector of <code class="reqn">q</code> strings.
</p>

<ul>
<li>
<p> If only one string is provided, the associated kernel is repeated <code class="reqn">q</code> times.
</p>
</li>
<li>
<p> For dimension-wise kernel selection, a vector of <code class="reqn">q</code> strings must be provided.
</p>
</li>
</ul>
<p>Have a look at <code>kernelX</code> for an exhaustive list of available kernels.
</p>
<p>To deal with a <b>high-dimensional vector output</b> or a <b>functional output</b>, it is advised to reduce dimension or to use a dedicated kernel. In this case, <code>kernelY</code> must be specified as a list of options. At least, <code>kernelY</code> must contain an option named <code>"method"</code>. For other options, there exist default assignments.
</p>

<ul><li> <p><code>method</code> is a string indicating the strategy used to construct the output kernel. Available choices include <code>"PCA"</code> (dimension reduction through principal component analysis), <code>"DTW"</code> (dynamic type warping) and <code>"GAK"</code> (global alignment kernel).
</p>
</li></ul>
<ol>
<li>
<p> If <code>method="PCA"</code>, the following options may also be specified:
</p>

<ul>
<li> <p><code>data.centering</code> is a boolean indicating whether the input samples must be centered before performing the preliminary PCA. Default value is <code>TRUE</code>.
</p>
</li>
<li> <p><code>data.scaling</code> is a boolean indicating whether the input samples must be scaled before performing the preliminary PCA. Default value is <code>TRUE</code>.
</p>
</li>
<li> <p><code>fam</code> is a string specifying the input kernel which is applied on principal components. Available choices only include <code>"dcov"</code>, <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"linear"</code>, <code>"matern3"</code>, <code>"matern5"</code>, <code>"raquad"</code> and <code>"rbf"</code>. Default value is <code>"rbf"</code>.
</p>
</li>
<li> <p><code>expl.var</code> is a scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the expected percentage of output variance that must be explained by <abbr><span class="acronym">PCA</span></abbr>. Default value is <code>0.95</code>.
</p>
</li>
<li> <p><code>PC</code> is the expected number of principal components in <abbr><span class="acronym">PCA</span></abbr>. Default value is <code>NA</code>.
</p>
</li>
<li> <p><code>combi</code> is a string indicating how the final output kernel is built in the reduced subspace. Available options include <code>"sum"</code> or <code>"prod"</code>. The chosen kernel in <code>fam</code> is applied on all principal components before summation (if <code>"sum"</code>) or tensorization (if <code>"prod"</code>).
</p>
</li>
<li> <p><code>position</code> is a string indicating whether weights have to be involved in the construction of the final output kernel in the reduced subspace. Available choices include <code>"nowhere"</code> (no weights), <code>"intern"</code> (weights applied on principal components) or <code>"extern"</code> (weights applied on kernels). Default value is <code>"intern"</code>.
</p>
</li>
</ul>
<p><b>Remark:</b> <code>expl.var</code> and <code>PC</code> are conflicting options. Only one of both needs to be specified and the other one must be set to <code>NA</code>. If both are specified, <code>expl.var</code> is prioritized. If both are set to <code>NA</code>, <code>expl.var</code> is then set to its default value.
</p>
</li>
<li>
<p> If <code>method="DTW"</code>, the following option may also be specified:
</p>

<ul><li> <p><code>fam</code> is a string specifying the translation-invariant kernel which is combined with <abbr><span class="acronym">DTW</span></abbr>. Available choices only include <code>"invmultiquad"</code>, <code>"laplace"</code>, <code>"matern3"</code>, <code>"matern5"</code>, <code>"raquad"</code> and <code>"rbf"</code>. Default value is <code>"rbf"</code>.
</p>
</li></ul>
</li>
<li>
<p> If <code>method="GAK"</code>, there is no other option to specify.
</p>
</li>
</ol>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramY</code></td>
<td>
<p>A scalar value or a vector of values with output kernel parameters.
</p>

<ul><li>
<p> If <code>paramY=NA</code>, output kernel parameters are computed automatically with rules of thumb.
</p>
</li></ul>
<p>In other cases, <code>paramY</code> must be specified in agreement with <code>kernelY</code>. 
</p>
<p><b>Case 1:</b> <code>kernelY</code> is a string or a vector of <code class="reqn">q</code> strings.
</p>
<p><code>paramY</code> must be a scalar value or a vector of <code class="reqn">q</code> values with output kernel parameters.
</p>

<ul>
<li>
<p> If <code>paramY</code> is a scalar value, it is affected to all output kernels.
</p>
</li>
<li>
<p> For dimension-wise kernel parametrization, a vector of <code class="reqn">q</code> values must be provided. If <code>kernelY</code> combines one parameter kernels and parameter-free kernels, <code>NA</code> must be specified for parameter-free kernels.
</p>
</li>
</ul>
<p><b>Case 2:</b> <code>kernelY</code> is a list of options with <code>method="PCA"</code>.
</p>
<p><code>paramY</code> must be set to <code>NA</code> because the parameters involved in the final output kernel are computed automatically. Their optimal tuning depends on the reduced subspace given by <abbr><span class="acronym">PCA</span></abbr>.
</p>
<p><b>Case 3:</b> <code>kernelY</code> is a list of options with <code>method="DTW"</code>.
</p>
<p><code>paramY</code> must be set to <code>NA</code>.
</p>
<p><b>Case 4:</b> <code>kernelY</code> is a list of options with <code>method="GAK"</code>.
</p>
<p><code>paramY</code> must be a vector of <code class="reqn">2</code> values. If the user only wants to specify one parameter, the other one must be set to <code>NA</code>. The two parameters correspond to the arguments <code>sigma</code> and <code>window.size</code> in the function <code>gak</code> from the package <code>dtwclust</code>. However, automatical computation (specified by <code>paramY=NA</code>) is strongly advised for this kind of output kernel.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimator.type</code></td>
<td>
<p>A string specifying the kind of estimator used for HSIC indices. Available choices include <code>"U-stat"</code> (U-stastics) and <code>"V-stat"</code> (V-statistics). U-statistics are unbiased estimators. V-statistics are biased estimators but they become unbiased asymptotically. In the specific case of <abbr><span class="acronym">HSIC</span></abbr> indices, V-statistics are non-negative estimators and they offer more flexibility for further test procedures (see <code>testHSIC</code>). Both kinds of estimators can be computed with complexity <code class="reqn">O(n^2)</code> where <code class="reqn">n</code> denotes the sample size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nboot</code></td>
<td>
<p>Number of bootstrap replicates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf</code></td>
<td>
<p>A scalar value (between <code class="reqn">0</code> and <code class="reqn">1</code>) specifying the level of confidence intervals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>anova</code></td>
<td>
<p>A list of parameters to achieve an <abbr><span class="acronym">ANOVA</span></abbr>-like decomposition based on HSIC indices. At least, <code>anova</code> must contain an option named <code>"obj"</code>. For other options, there exist default assignments.
</p>

<ul>
<li> <p><code>obj</code> is a string specifying which kinds of <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are expected. Available choices include <code>"no"</code> (<code>anova</code> is disabled), <code>"FO"</code> (first-order only), <code>"TO"</code> (total-order only) and <code>"both"</code> (first-order and total-order). 
</p>
</li>
<li> <p><code>is.uniform</code> is a boolean indicating whether the samples stored in <code>X</code> come from random variables that are uniformly distributed on <code class="reqn">[0,1]</code>. Let us recall that <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can only be computed by means of <abbr><span class="acronym">ANOVA</span></abbr> kernels. Among available kernels, only <code>"laplace_anova"</code>, <code>"matern3_anova"</code>, <code>"matern5_anova"</code>, <code>"rbf_anova"</code>, <code>"sobolev1"</code> and <code>"sobole2"</code> verify this constraint (provided that all input variables are uniformly distributed on <code class="reqn">[0,1]</code>).
</p>

<ul>
<li>
<p> If <code>is.uniform=TRUE</code>, it is checked that each input data stored in <code class="reqn">X</code> actually lies in <code class="reqn">[0,1]</code>. If this condition is not verified, an error is returned.
</p>
</li>
<li>
<p> If <code>is.uniform=FALSE</code>, non-parametric rescaling (based on empirical distribution functions) is operated. 
</p>
</li>
</ul>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sensi</code></td>
<td>
<p>An object of class <code>"sensiHSIC"</code> resulting from a prior call to the hereby function. If an object of class <code>"sensiHSIC"</code> is indeed provided, the following happens:
</p>

<ul>
<li>
<p> If <code>sensi</code> contains an object named <code>"KX"</code>, it is extracted from <code>sensi</code> and the input Gram matrices (required to estimate <abbr><span class="acronym">HSIC</span></abbr> indices) are not built from <code>X</code>, <code>kernelX</code> and <code>paramX</code>.
</p>
</li>
<li>
<p> If <code>sensi</code> contains an object named <code>"KY"</code>, it is extracted from <code>sensi</code> and the output Gram matrix (required to estimate <abbr><span class="acronym">HSIC</span></abbr> indices) is not built from <code>model</code>, <code>kernelY</code> and <code>paramY</code>. 
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save.GM</code></td>
<td>
<p>A list of parameters indicating whether Gram matrices have to be saved. The list <code>save.GM</code> must contain options named <code>"KX"</code> and <code>"KY"</code>.
</p>

<ul>
<li> <p><code>KX</code> is a boolean indicating whether the input Gram matrices have to be saved.
</p>
</li>
<li> <p><code>KY</code> is a boolean indicating whether the output Gram matrix has to be saved.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An object of class <code>"sensiHSIC"</code> storing the state of the sensitivity study (parameters, data, estimates).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A <code class="reqn">n</code>-by-<code class="reqn">q</code> matrix containing all output samples. It comprises <code class="reqn">n</code> observations of the <code class="reqn">q</code> output variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ylim</code></td>
<td>
<p>A vector of two values specifying the <code class="reqn">y</code>-coordinate plotting limits.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Any other arguments for <code>model</code> which are passed unchanged each time <code>model</code> is called.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Let <code class="reqn">(Xi,Y)</code> be an input-output pair. The kernels assigned to <code class="reqn">Xi</code> and <code class="reqn">Y</code> are respectively denoted by <code class="reqn">Ki</code> and <code class="reqn">KY</code>. 
</p>
<p>For many global sensitivity measures, the influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> is measured in the light of the probabilistic dependence that exists within the input-output pair <code class="reqn">(Xi,Y)</code>. For this, a dissimilarity measure is applied between the joint probability distribution (true impact of <code class="reqn">Xi</code> and <code class="reqn">Y</code> in the numerical model) and the product of marginal distributions (no impact of <code class="reqn">Xi</code> on <code class="reqn">Y</code>). For instance, Borgonovo's sensitivity measure is built upon the total variation distance between those two probability distributions. See Borgonovo and Plischke (2016) for further details.
</p>
<p>The <abbr><span class="acronym">HSIC</span></abbr>-based sensitivity measure can be understood in this way since the index <code class="reqn">HSIC(Xi,Y)</code> results from the application of the <b>Hilbert-Schmidt independence criterion (<abbr><span class="acronym">HSIC</span></abbr>)</b> on the pair <code class="reqn">(Xi,Y)</code>. This criterion is nothing but a special kind of dissimilarity measure between the joint probability distribution and the product of marginal distributions. This dissimilarity measure is called the <b>maximum mean discrepancy (MMD)</b> and its definition relies on the selected kernels <code class="reqn">Ki</code> and <code class="reqn">KY</code>. According to the theory of reproducing kernels, every kernel <code class="reqn">K</code> is related to a <b>reproducing kernel Hilbert space (<abbr><span class="acronym">RKHS</span></abbr>)</b>.Then, if <code class="reqn">K</code> is affected to a random variable <code class="reqn">Z</code>, any probability distribution describing the random behavior of <code class="reqn">Z</code> may be represented within the induced <abbr><span class="acronym">RKHS</span></abbr>. In this setup, the dissimilarity between the joint probability distribution and the product of marginal distributions is then measured through the squared norm of their images into the bivariate <abbr><span class="acronym">RKHS</span></abbr>. The user is referred to Gretton et al. (2006) for additional details on the mathematical construction of <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
<p>In practice, it may be difficult to understand how <code class="reqn">HSIC(Xi,Y)</code> measures dependence within <code class="reqn">(Xi,Y)</code>. An alternative definition relies on the concept of <b>feature map</b>. Let us recall that the value taken by a kernel function can always be seen as the scalar product of two <b>feature functions</b> lying in a <b>feature space</b>. Definition 1 in Gretton et al. (2005) introduces <code class="reqn">HSIC(Xi,Y)</code> as the Hilbert-Schmidt norm of a covariance-like operator between random features. For this reason, having access to the input and output feature maps may help identify the dependence patterns captured by <code class="reqn">HSIC(Xi,Y)</code>.
</p>
<p>Kernels must be chosen very carefully. There exists a wide variety of kernels but only a few f them meet the needs of <abbr><span class="acronym">GSA</span></abbr>. As <code class="reqn">HSIC(Xi,Y)</code> is supposed to be a dependence measure, it must be equal to <code class="reqn">0</code> if and only if <code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent. A sufficient condition to enable this equivalence is to take two characteristic kernels. The reader is referred to Fukumizu et al. (2004) for the mathematical definition of a characteristic kernel and to Sriperumbur et al. (2010) for an overview of the major related results. In particular:
</p>

<ul>
<li>
<p> The Gaussian kernel, the Laplace kernel, the Matern <code class="reqn">3/2</code> kernel and the Matern <code class="reqn">5/2</code> kernel (all defined on <code class="reqn">R^2</code>) are <b>characteristic</b>.
</p>
</li>
<li>
<p> The transformed versions of the four abovementioned kernels (all defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li>
<p> All Sobolev kernels (defined on <code class="reqn">[0,1]^2</code>) are <b>characteristic</b>.
</p>
</li>
<li>
<p> The categorical kernel (defined on any discrete probability space) is <b>characteristic</b>.
</p>
</li>
</ul>
<p>Lemma 1 in Gretton et al. (2005) provides a third way of defining <code class="reqn">HSIC(Xi,Y)</code>. Since the associated formula is only based on three expectation terms, the corresponding estimation procedures are very simple and they do not ask for a large amount of input-output samples to be accurate. Two kinds of estimators may be used for <code class="reqn">HSIC(Xi,Y)</code>: the <b>V-statistic estimator</b> (which is non negative, biased and asymptotically unbiased) or the <b>U-statistic estimator</b> (unbiased). For both estimators,  the computational complexity is <code class="reqn">O(n^2)</code> where <code class="reqn">n</code> is the sample size.
</p>
<p>The user must always keep in mind the key steps leading to the estimation of <code class="reqn">HSIC(Xi,Y)</code>:
</p>

<ul>
<li>
<p> Input samples are simulated and the corresponding output samples are computed with the numerical model.
</p>
</li>
<li>
<p> An input kernel <code class="reqn">Ki</code> and an output kernel <code class="reqn">KY</code> are selected.
</p>
</li>
<li> <p><b>In case of target sensitivity analysis:</b> output samples are transformed by means of a weight function <code class="reqn">w</code>.
</p>
</li>
<li>
<p> The input and output Gram matrices are constructed.
</p>
</li>
<li> <p><b>In case of conditional sensitivity analysis:</b> conditioning weights are computed by means of a weight function <code class="reqn">w</code>.
</p>
</li>
<li>
<p> The final estimate is computed. It depends on the selected estimator type (either a U-statistic or a V-statistic).
</p>
</li>
</ul>
<h4>Kernel functions for random variables</h4>

<p>All what follows is written for a scalar output <code class="reqn">Y</code> but the same is true for any scalar input <code class="reqn">Xi</code>. 
</p>
<p>Let <code class="reqn">D</code> denote the support of the output probability distribution. A kernel is a symmetric and positive definite function defined on the domain <code class="reqn">D</code>. Different kernel families are available in <code>sensiHSIC</code>. 
</p>

<ul>
<li>
<p> To deal with continuous probability distributions on <code class="reqn">R</code>, one can use:
</p>

<ul><li>
<p> The covariance kernel of the fractional Browian motion (<code>"dcov"</code>), the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the dot-product kernel (<code>"linear"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the rationale quadratic kernel (<code>"raquad"</code>) and the Gaussian kernel (<code>"rbf"</code>).
</p>
</li></ul>
</li>
<li>
<p> To deal with continuous probability distributions on <code class="reqn">[0,1]</code>, one can use:
</p>

<ul>
<li>
<p> Any of the abovementioned kernel (restricted to <code class="reqn">[0,1]</code>).
</p>
</li>
<li>
<p> The transformed exponential kernel (<code>"laplace_anova"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>), the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>), the transformed Gaussian kernel (<code>"rbf_anova"</code>), the Sobolev kernel with smoothness parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>) and the Sobolev kernel with smoothness parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>).
</p>
</li>
</ul>
</li>
<li>
<p> To deal with any discrete probability distribution, the categorical kernel (<code>"categ"</code>) must be used.
</p>
</li>
</ul>
<p>Two kinds of kernels must be distinguished:
</p>

<ul>
<li> <p><b>Parameter-free kernels:</b> the dot-product kernel (<code>"linear"</code>), the Sobolev kernel with smoothness parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>) and the Sobolev kernel with smoothness parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>).
</p>
</li>
<li> <p><b>One-parameter kernels:</b> the categorical kernel (<code>"categ"</code>), the covariance kernel of the fractional Brownian motion kernel (<code>"dcov"</code>), the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the transformed exponential kernel (<code>"laplace_anova"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>), the rationale quadratic kernel (<code>"raquad"</code>), the Gaussian kernel (<code>"rbf"</code>) and the transformed Gaussian kernel (<code>"rbf_anova"</code>).
</p>
</li>
</ul>
<p>A major issue related to one-parameter kernels is how to set the parameter. It mainly depends on the role played by the parameter in the kernel expression.
</p>

<ul>
<li>
<p> For translation-invariant kernels and their <abbr><span class="acronym">ANOVA</span></abbr> variants (that is all one-parameter kernels except <code>"categ"</code> and <code>"dcov"</code>), the parameter may be interpreted as a correlation length (or a scale parameter). The rule of thumb is to compute the empirical standard deviation of the provided samples.
</p>
</li>
<li>
<p> For the covariance kernel of the fractional Brownian motion (<code>"dcov"</code>), the parameter is an exponent. Default value is <code class="reqn">1</code>.
</p>
</li>
<li>
<p> For the categorical kernel (<code>"categ"</code>), the parameter has no physical sense. It is just a kind of binary encoding.
</p>

<ul>
<li> <p><code class="reqn">0</code> means the user wants to use the basic categorical kernel.
</p>
</li>
<li> <p><code class="reqn">1</code> means the user wants to use the weighted variant of the categorical kernel.
</p>
</li>
</ul>
</li>
</ul>
<h4>How to deal with a low-dimensional vector output?</h4>

<p>Let us assume that the output vector <code class="reqn">Y</code> is composed of <code class="reqn">q</code> random variables <code class="reqn">Y1,...,Yq</code>.
</p>
<p>A kernel <code class="reqn">Kj</code> is affected to each output variable <code class="reqn">Yj</code> and this leads to embed the <code class="reqn">j</code>-th output probability distribution in a <abbr><span class="acronym">RKHS</span></abbr> denoted by <code class="reqn">Hj</code>. Then, the <b>tensorization</b> of <code class="reqn">H1,...,Hq</code> allows to build the final <abbr><span class="acronym">RKHS</span></abbr>, that is the <abbr><span class="acronym">RKHS</span></abbr> where the <code class="reqn">q</code>-variate output probability distribution describing the overall random behavior of <code class="reqn">Y</code> will be embedded. In this situation:
</p>

<ul>
<li>
<p> The final output kernel is the tensor product of all output kernels.
</p>
</li>
<li>
<p> The final output Gram matrix is the Hadamard product of all output Gram matrices.
</p>
</li>
</ul>
<p>Once the final output Gram matrix is built, <abbr><span class="acronym">HSIC</span></abbr> indices can be estimated, just as in the case of a scalar output.
</p>



<h4>How to deal with a high-dimensional vector output or a functional output?</h4>

<p>In <code>sensiHSIC</code>, three different methods are proposed in order to compute <abbr><span class="acronym">HSIC</span></abbr>-based sensitivity indices in presence of functional outputs.
</p>
<p><b>Dimension reduction</b>
</p>
<p>This approach was initially proposed by Da Veiga (2015). The key idea is to approximate the random functional output by the first terms of its <b>Kharunen-Loeve expansion</b>. This can be achived with a <b>principal component analysis (PCA)</b> that is carried out on the empirical covariance matrix.
</p>

<ul>
<li>
<p> The eigenvectors (or <b>principal directions</b>) allow to approximate the (deterministic) functional terms involved in the Kharunen-Loeve decomposition.
</p>
</li>
<li>
<p> The eigenvalues allow to determine how many principal directions are sufficient in order to accurately represent the random function by means of its truncated Kharunen-Loeve expansion. The key idea behind dimension reduction is to keep as few principal directions as possible while preserving a prescribed level of explained variance.
</p>
</li>
</ul>
<p>The <b>principal components</b> are the coordinates of the functional output in the low-dimensional subspace resulting from <abbr><span class="acronym">PCA</span></abbr>. There are computed for all output samples (time series observations). See Le Maitre and Knio (2010) for more detailed explanations.
</p>
<p>The last step consists in constructing a kernel in the reduced subspace. One single kernel family is selected and affected to all principal directions. Moreover, all kernel parameters are computed automatically (with appropriate rules of thumb). Then, several strategies may be considered.
</p>

<ul>
<li>
<p> The initial method described in Da Veiga (2015) is based on a direct tensorization. One can also decide to sum kernels.
</p>
</li>
<li>
<p> This approach was improved by El Amri and Marrel (2021). For each principal direction, a weight coefficient (equal the ratio between the eigenvalue and the sum of all selected eigenvalues) is computed.
</p>

<ul>
<li>
<p> The principal components are multiplied by their respective weight coefficients before summing kernels or tensorizing kernels.
</p>
</li>
<li>
<p> The kernels can also be directly applied on the principal components before being linearly combined according to the weight coefficients.
</p>
</li>
</ul>
</li>
</ul>
<p>In sensiHSIC, all these strategies correspond to the following specifications in <code>kernelY</code>:
</p>

<ul>
<li> <p><b>Direct tensorization:</b>
<code>kernelY=list(method="PCA", combi="prod", position="nowhere")</code>
</p>
</li>
<li> <p><b>Direct sum:</b>
<code>kernelY=list(method="PCA", combi="sum", position="nowhere")</code>
</p>
</li>
<li> <p><b>Rescaled tensorization:</b>
<code>kernelY=list(method="PCA", combi="prod", position="intern")</code>
</p>
</li>
<li> <p><b>Rescaled sum:</b> 
<code>kernelY=list(method="PCA", combi="sum", position="intern")</code>
</p>
</li>
<li> <p><b>Weighted linear combination:</b>
<code>kernelY=list(method="PCA", combi="sum", position="extern")</code>
</p>
</li>
</ul>
<p><b>Dynamic Time Warping (<abbr><span class="acronym">DTW</span></abbr>)</b>
</p>
<p>The <abbr><span class="acronym">DTW</span></abbr> algorithm developed by Sakoe and Chiba (1978) can be combined with a translation-invariant kernel in order to create a kernel function for times series. The resulting <abbr><span class="acronym">DTW</span></abbr>-based output kernel is well-adapted to measure similarity between two given time series.
</p>
<p>Suitable translation-invariant kernels include the inverse multiquadratic kernel (<code>"invmultiquad"</code>), the exponential kernel (<code>"laplace"</code>), the Matern <code class="reqn">3/2</code> kernel (<code>"matern3"</code>), the Matern <code class="reqn">5/2</code> kernel (<code>"matern5"</code>), the rationale quadratic kernel (<code>"raquad"</code>) and the Gaussian kernel (<code>"rbf"</code>).
</p>
<p>The user is warned against the fact that <abbr><span class="acronym">DTW</span></abbr>-based kernels are not positive definite functions. As a consequence, many theoretical properties do not hold anymore for <abbr><span class="acronym">HSIC</span></abbr> indices.
</p>
<p>For faster computations, <code>sensiHSIC</code> is using the function <code>dtw_dismat</code> from the package <code>incDTW</code>.
</p>
<p><b>Global Alignment Kernel (<abbr><span class="acronym">GAK</span></abbr>)</b>
</p>
<p>Unlike <abbr><span class="acronym">DTW</span></abbr>-based kernels, the <abbr><span class="acronym">GAK</span></abbr> is a positive definite function. This time-series kernel was originally introduced in Cuturi et al. (2007) and further investigated in Cuturi (2011). It was used to compute <abbr><span class="acronym">HSIC</span></abbr> indices on a simplified compartmental epidemiological model in Da Veiga (2021).
</p>
<p>For faster computations, <code>sensiHSIC</code> is using the function <code>gak</code> from the package <code>dtwclust</code>. 
</p>
<p>In <code>sensiHSIC</code>, two <abbr><span class="acronym">GAK</span></abbr>-related parameters may be tuned by the user with <code>paramY</code>. They exactly correspond to the arguments <code>sigma</code> and <code>window.size</code> in the function <code>gak</code>.
</p>



<h4>About normalized <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">R2-HSIC</span></abbr>)</h4>

<p>No doubt interpretability is the major drawback of <abbr><span class="acronym">HSIC</span></abbr> indices. This shortcoming led Da Veiga (2021) to introduce a normalized version of <code class="reqn">HSIC(Xi,Y)</code>. The so-called R2-HSIC index is thus defined as the ratio between <code class="reqn">HSIC(Xi,Y)</code> and the square root of a normalizing constant equal to <code class="reqn">HSIC(Xi,Xi)*HSIC(Y,Y)</code>. 
</p>
<p>This normalized sensitivity measure is inspired from the <b>distance correlation measure</b> proposed by Szekely et al. (2007) and the resulting sensitivity indices are easier to interpret since they all fall in the interval <code class="reqn">[0,1]</code>.
</p>



<h4>About target <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">T-HSIC</span></abbr>)</h4>

<p>T-HSIC indices were designed by Marrel and Chabridon (2021) for <abbr><span class="acronym">TSA</span></abbr>. They are only defined for a scalar output. Vector and functional outputs are not supported. The main idea of <abbr><span class="acronym">TSA</span></abbr> is to measure the influence of each input variable <code class="reqn">Xi</code> on a modified version of <code class="reqn">Y</code>. To do so, a preliminary mathematical transform <code class="reqn">w</code> (called the <b>weight function</b>) is applied on <code class="reqn">Y</code>. The collection of <abbr><span class="acronym">HSIC</span></abbr> indices is then estimated with respect to <code class="reqn">w(Y)</code>. Here are two examples of situations where <abbr><span class="acronym">TSA</span></abbr> is particularly relevant:
</p>

<ul>
<li>
<p> How to measure the impact of <code class="reqn">Xi</code> on the upper values taken by <code class="reqn">Y</code> (for example the values above a given threshold <code class="reqn">T</code>)?
</p>

<ul><li>
<p> To answer this question, one may take <code class="reqn">w(Y)=Y*1_{Y&gt;T}</code> (<b>zero-thresholding</b>). <br>
This can be specified in <code>sensiHSIC</code> with <code>target=list(c=T, type="zeroTh", upper=TRUE)</code>.
</p>
</li></ul>
</li>
<li>
<p> How to measure the influence of <code class="reqn">Xi</code> on the occurrence of the event <code class="reqn">{Y&gt;T}</code>?
</p>

<ul><li>
<p> To answer this question, one may take <code class="reqn">w(Y)=1_{Y&lt;T}</code> (<b>indicator-thresholding</b>). <br>
This can be specified in <code>sensiHSIC</code> with <code>target=list(c=T, type="indicTh", upper=FALSE)</code>.
</p>
</li></ul>
</li>
</ul>
<p>In Marrel and Chabridon (2021), the two situations described above are referred to as <b>"hard thresholding"</b>. To avoid using discontinuous weight functions, <b>"smooth thresholding"</b> may be used instead.
</p>

<ul>
<li>
<p> Spagnol et al. (2019): logistic transformation on both sides of the threshold <code class="reqn">T</code>.
</p>
</li>
<li>
<p> Marrel and Chabridon (2021): exponential transformation above or below the threshold <code class="reqn">T</code>.
</p>
</li>
</ul>
<p>These two smooth relaxation functions depend on a tuning parameter that helps control smoothness. For further details, the user is invited to consult the documentation of the function <code>weightTSA</code>.
</p>
<p><b>Remarks:</b>
</p>

<ul>
<li>
<p> When <code>type="indicTh"</code> (<b>indicator-thesholding</b>), <code class="reqn">w(Y)</code> becomes a binary random variable. Accordingly, the output kernel selected in <code>kernelY</code> must be the categorical kernel.
</p>
</li>
<li>
<p> In the spirit of <abbr><span class="acronym">R2-HSIC</span></abbr> indices, <abbr><span class="acronym">T-HSIC</span></abbr> indices can be normalized. The associated normalizing constant is equal to the square root of <code class="reqn">HSIC(Xi,Xi)*HSIC(w(Y),w(Y))</code>.
</p>
</li>
<li> <p><abbr><span class="acronym">T-HSIC</span></abbr> indices can be very naturally combined with the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> decomposition proposed by Da Veiga (2021). As a consequence, the arguments <code>target</code> and <code>anova</code> in <code>sensiHSIC</code> can be enabled simultaneously. Compared with basic <abbr><span class="acronym">HSIC</span></abbr> indices, there are three main differences: the input variables must be mutually independent, <abbr><span class="acronym">ANOVA</span></abbr> kernels must be used for all input variables and the output of interest is <code class="reqn">w(Y)</code>.
</p>
</li>
<li> <p><abbr><span class="acronym">T-HSIC</span></abbr> indices can be very naturally combined with the tests of independence proposed in <code>testHSIC</code>. In this context, the null hypothesis is <code class="reqn">H0</code>: "<code class="reqn">Xi</code> and <code class="reqn">w(Y)</code> are independent".
</p>
</li>
</ul>
<h4>About conditional <abbr><span class="acronym">HSIC</span></abbr> indices (<abbr><span class="acronym">C-HSIC</span></abbr>)</h4>

<p><abbr><span class="acronym">C-HSIC</span></abbr> indices were designed by Marrel and Chabridon (2021) for <abbr><span class="acronym">CSA</span></abbr>. They are only defined for a scalar output. Vector and functional outputs are not supported. The idea is to measure the impact of each input variable <code class="reqn">Xi</code> on <code class="reqn">Y</code> when a specific event occurs. This conditioning event is defined on <code class="reqn">Y</code> thanks to a <b>weight function</b> <code class="reqn">w</code>. In order to compute the conditioning weights, <code class="reqn">w</code> is applied on the output samples and an empirical normalization is carried out (so that the overall sum of conditioning weights is equal to <code class="reqn">1</code>). The conditioning weights are then combined with the simulated Gram matrices in order to estimate <abbr><span class="acronym">C-HSIC</span></abbr> indices. All formulas can be found in Marrel and Chabridon (2021). Here is an exemple of a situation where <abbr><span class="acronym">CSA</span></abbr> is particularly relevant:
</p>

<ul><li>
<p>  Let us imagine that the event <code class="reqn">{Y&gt;T}</code> coincides with a system failure. <br>
How to measure the influence of <code class="reqn">Xi</code> on <code class="reqn">Y</code> when failure occurs?
</p>

<ul><li>
<p> To answer this question, one may take <code class="reqn">w(Y) = 1_{Y&gt;T}</code> (<b>indicator-thresholding</b>). <br>
This can be specified in <code>sensiHSIC</code> with <code>cond=list(c=T, type="indicTh", upper=TRUE)</code>.
</p>
</li></ul>
</li></ul>
<p>The three other weight functions proposed for TSA (namely <code>"zeroTh"</code>, <code>"logistic"</code> and <code>"exp1side"</code>) can also be used but the role they play is less intuitive to understand. See Marrel and Chabridon (2021) for better explanations.
</p>
<p><b>Remarks:</b>
</p>

<ul>
<li>
<p> Unlike what is pointed out for <abbr><span class="acronym">TSA</span></abbr>, when <code>type="thresholding"</code>, the output of interest <code class="reqn">Y</code> remains a continuous random variable. The categorical kernel is thus inappropriate. A continuous kernel must be used instead.
</p>
</li>
<li>
<p> In the spirit of <abbr><span class="acronym">R2-HSIC</span></abbr> indices, <abbr><span class="acronym">C-HSIC</span></abbr> indices can be normalized. The associated normalizing constant is equal to the square root of <code class="reqn">C-HSIC(Xi,Xi)*C-HSIC(Y,Y)</code>.
</p>
</li>
<li>
<p> Only V-statistics are supported to estimate C-HSIC indices. The reason is because the normalized version of C-HSIC indices cannot always be estimated with U-statistics. In particular, the estimates of <code class="reqn">C-HSIC(Xi,Xi)*C-HSIC(Y,Y)</code> may be negative.
</p>
</li>
<li> <p><abbr><span class="acronym">C-HSIC</span></abbr> indices cannot be combined with the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> decomposition proposed in Da Veiga (2021). In fact, the conditioning operation is feared to introduce statistical dependence among input variables, which forbids using <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices. As a consequence, the arguments <code>cond</code> and <code>anova</code> in <code>sensiHSIC</code> cannot be enabled simultaneously.
</p>
</li>
<li> <p><abbr><span class="acronym">C-HSIC</span></abbr> indices can harly be combined with the tests of inpendence proposed in <code>testHSIC</code>. This is only possible if <code>type="indicTh"</code>. In this context, the null hypothesis is <code class="reqn">H0</code>: "<code class="reqn">Xi</code> and <code class="reqn">Y</code> are independent if the event described in <code>cond</code> occurs".
</p>
</li>
</ul>
<h4>About <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices</h4>

<p>In comparison with <abbr><span class="acronym">HSIC</span></abbr> indices, <abbr><span class="acronym">R2-HSIC</span></abbr> indices are easier to interpret. However, in terms of interpretability, Sobol' indices remain much more convenient since they can be understood as shares of the total output variance. Such an interpretation is made possible by the Hoeffding decomposition, also known as <abbr><span class="acronym">ANOVA</span></abbr> decomposition.
</p>
<p>It was proved in Da Veiga (2021) that an <abbr><span class="acronym">ANOVA</span></abbr>-like decomposition can be achived for <abbr><span class="acronym">HSIC</span></abbr> indices under certain conditions:
</p>

<ul>
<li>
<p> The input variables must be mutually independent (which was not required to compute all other kinds of <abbr><span class="acronym">HSIC</span></abbr> indices).
</p>
</li>
<li> <p><b><abbr><span class="acronym">ANOVA</span></abbr> kernels</b> must be assigned to all input variables.
</p>
</li>
</ul>
<p>This <abbr><span class="acronym">ANOVA</span></abbr> setup allows to establish a strict separation between main effects and interaction effects in the <abbr><span class="acronym">HSIC</span></abbr> sense. The first-order and total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices are then defined in the same fashion than first-order and total-order Sobol' indices. It is worth noting that the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> normalizing constant is equal to <code class="reqn">HSIC(X,Y)</code> and is thus different from the one used for <abbr><span class="acronym">R2-HSIC</span></abbr> indices.
</p>
<p>For a given probability measure <code class="reqn">P</code>, an <abbr><span class="acronym">ANOVA</span></abbr> kernel <code class="reqn">K</code> is a kernel that can rewritten <code class="reqn">1+k</code> where <code class="reqn">k</code> is an orthogonal kernel with respect to <code class="reqn">P</code>. Among the well-known parametric families of probability distributions and kernel functions, there are very few examples of orthogonal kernels. One example is given by <b>Sobolev kernels</b> when there are matched with the uniform probability measure on [0,1]. See Wahba et al. (1995) for further details on Sobolev kernels.
</p>
<p>Moreover, several strategies to construct orthogonal kernels from non-orthogonal kernels are recalled in Da Veiga (2021). One of them consists in translating the feature map so that the resulting kernel becomes centered at the prescribed probability measure <code class="reqn">P</code>. This can be done analytically for some basic kernels (Gaussian, exponential, Matern <code class="reqn">3/2</code> and Matern <code class="reqn">5/2</code>) when <code class="reqn">P</code> is the uniform measure on <code class="reqn">[0,1]</code>. See Section 9 in Ginsbourger et al. (2016) for the corresponding formulas.
</p>
<p>In <code>sensiHSIC</code>, <abbr><span class="acronym">ANOVA</span></abbr> kernels are only available for the uniform probability measure on <code class="reqn">[0,1]</code>. This includes the Sobolev kernel with parameter <code class="reqn">r=1</code> (<code>"sobolev1"</code>), the Sobolev kernel with parameter <code class="reqn">r=2</code> (<code>"sobolev2"</code>), the transformed Gaussian kernel (<code>"rbf_anova"</code>), the transformed exponential kernel (<code>"laplace_anova"</code>), the transformed Matern <code class="reqn">3/2</code> kernel (<code>"matern3_anova"</code>) and the transformed Matern <code class="reqn">5/2</code> kernel (<code>"matern5_anova"</code>).
</p>
<p>As explained above, the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can only be computed if all input variables are uniformly distributed on <code class="reqn">[0,1]</code>. Because of this limitation, a preliminary reformulation is needed if the <abbr><span class="acronym">GSA</span></abbr> problem includes other kinds of input probability distributions. The <b>probability integral transform (PIT)</b> must be applied on each input variable <code class="reqn">Xi</code>. In addition, all quantile functions must be encapsulated in the numerical model, which may lead to reconsider the way <code>model</code> is specified. In <code>sensiHSIC</code>, if <code>check=TRUE</code> is selected in <code>anova</code>, it is checked that all input samples lie in <code class="reqn">[0,1]</code>. If this is not the case, a non-parametric rescaling (based on empirical distribution functions) is operated.
</p>
<p><abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices can be used for <abbr><span class="acronym">TSA</span></abbr>. The only difference with <abbr><span class="acronym">GSA</span></abbr> is the use of a weight function <code class="reqn">w</code>. On the contrary, CSA cannot be conducted with <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices. Indeed, the conditioning operation is feared to introduce statistical independence among the input variables, which prevents using the <abbr><span class="acronym">HSIC-ANOVA</span></abbr> approach.
</p>



<h3>Value</h3>

<p><code>sensiHSIC</code> returns a list of class <code>"sensiHSIC"</code>. It contains all the input arguments detailed before, except <code>sensi</code> which is not kept. It must be noted that some of them might have been altered, corrected or completed.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>kernelX</code></td>
<td>
<p>A vector of <code class="reqn">p</code> strings with input kernels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramX</code></td>
<td>
<p>A vector of <code class="reqn">p</code> values with input kernel parameters. For each one-parameter kernel, a real number is returned. It is either the original value (if correct), a corrected value (if not) or the default value (computed from a rule of thumb when <code>NA</code> is specified). For each parameter-free kernel, <code>NA</code> is returned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernelY</code></td>
<td>
<p>A vector of <code class="reqn">q</code> strings or a list of options that specifies how the output kernel was constructed. In the case where <code>kernelY</code> is a list of options with <code>method="PCA"</code>, <code>kernelY</code> contains additional information resulting from <abbr><span class="acronym">PCA</span></abbr>.
</p>

<ul>
<li>
<p> If <code>kernelY</code> initally contained an option named <code>"expl.var"</code>, <code>kernelY</code> now also contains an option named <code>"PC"</code> that provides the associated number of principal components.
</p>
</li>
<li>
<p> If <code>kernelY</code> initially contained an option named <code>"PC"</code>, <code>kernelY</code> now also contains an option named <code>"expl.var"</code> that provides the associated percentage of output variance that is explained by <abbr><span class="acronym">PCA</span></abbr>.
</p>
</li>
<li>
<p> If <code>kernelY</code> initally contained an option named <code>"position"</code> that was set to <code>"intern"</code> or <code>"extern"</code>, <code>kernelY</code> now contains an option named <code>"ratios"</code> that provides the weights used to combine kernels in the reduced subspace given by <abbr><span class="acronym">PCA</span></abbr>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramY</code></td>
<td>
<p>A vector of values with output kernel parameters.
</p>
<p><b>Case 1:</b> <code>kernelY</code> is a list of <code class="reqn">q</code> strings.
</p>
<p><code>paramY</code> is a vector of <code>q</code> values. For each one-parameter kernel, a real number is returned. It is either the original value (if correct), a corrected value or the default value (computed with a rule of thumb if <code>NA</code> was initially specified). For each parameter-free kernel, <code>NA</code> is returned.
</p>
<p><b>Case 2:</b> <code>kernelY</code> is a list of options with <code>method="PCA"</code>.
</p>
<p><code>paramY</code> is a vector of <code>PC</code> values. For this method, let us recall that all kernels belong to the same family which is specified by an option named <code>"fam"</code> within <code>kernelY</code>. For each dimension in the reduced subspace, the kernel parameter is computed (with a rule of thumb) from the corresponding principal component. If the kernel in <code>fam</code> is parameter-free, <code>paramY</code> is a vector where <code>NA</code> is repeated <code>PC</code> times.
</p>
<p><b>Case 3:</b> <code>kernelY</code> is a list of options with <code>method="DTW"</code>.
</p>
<p><code>paramY</code> remains equal to <code>NA</code>.
</p>
<p><b>Case 4:</b> <code>kernelY</code> is a list of options with <code>method="GAK"</code>.
</p>
<p><code>paramY</code> is a vector of <code class="reqn">2</code> values. For each parameter, the returned value is either the original value (if correct), a corrected value or the default value (computed with a rule of thumb if <code>NA</code> was initially specified). 
</p>
</td>
</tr>
</table>
<p>More importantly, the list of class <code>"sensiHSIC"</code> contains all expected results (output samples, sensitivity measures and conditioning weights).
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>The matched call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A <code class="reqn">n</code>-row matrix containing all output samples. The <code class="reqn">i</code>-th row in <code>y</code> is obtained from the <code class="reqn">i</code>-th row in <code>X</code> after computing the model response. If <code>target</code> is passed to <code>sensiHSIC</code>, output samples in <code>y</code> are obtained after applying consecutively <code>model</code> and the specified weight function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>HSICXY</code></td>
<td>
<p>The estimated <abbr><span class="acronym">HSIC</span></abbr> indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>The estimated <abbr><span class="acronym">R2-HSIC</span></abbr> indices (also called normalized <abbr><span class="acronym">HSIC</span></abbr> indices).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Only if <code>cond</code> is passed to <code>sensiHSIC</code>. <br>
A vector of <code class="reqn">n</code> values containing all conditioning weights. In the <abbr><span class="acronym">CSA</span></abbr> context, the conditioning factor is defined by <code class="reqn">w(Y)/E[w(Y)]</code>. See Marrel and Chabridon (2021) for further explanations.</p>
</td>
</tr>
</table>
<p>Depending on what is specified in <code>anova</code>, the list of class <code>"sensiHSIC"</code> may also contain the following objects:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>FO</code></td>
<td>
<p>The estimated first-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TO</code></td>
<td>
<p>The estimated total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TO.num</code></td>
<td>
<p>The estimated numerators of total-order <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>denom</code></td>
<td>
<p>The estimated common denominator of <abbr><span class="acronym">HSIC-ANOVA</span></abbr> indices.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Sebastien Da Veiga, Amandine Marrel, Anouar Meynaoui, Reda El Amri and Gabriel Sarazin.
</p>


<h3>References</h3>

<p>Borgonovo, E. and Plischke, E. (2016), <em>Sensitivity analysis: a review of recent advances</em>, European Journal of Operational Research, 248(3), 869-887.
</p>
<p>Cuturi, M., Vert, J. P., Birkenes, O. and Matsui, T. (2007), <em>A kernel for time series based on global alignments</em>, 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07 (Vol. 2, pp. II-413), IEEE.
</p>
<p>Cuturi, M. (2011), <em>Fast global alignment kernels</em>, Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 929-936).
</p>
<p>Da Veiga, S. (2015), <em>Global sensitivity analysis with dependence measures</em>, Journal of Statistical Computation and Simulation, 85(7), 1283-1305.
</p>
<p>Da Veiga, S. (2021). <em>Kernel-based <abbr><span class="acronym">ANOVA</span></abbr> decomposition and Shapley effects: application to global sensitivity analysis</em>, arXiv preprint arXiv:2101.05487.
</p>
<p>El Amri, M. R. and Marrel, A. (2021), <em>More powerful <abbr><span class="acronym">HSIC</span></abbr>-based independence tests, extension to space-filling designs and functional data</em>.
<a href="https:/cea.hal.science/cea-03406956/">https:/cea.hal.science/cea-03406956/</a>
</p>
<p>Fukumizu, K., Bach, F. R. and Jordan, M. I. (2004), <em>Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</em>, Journal of Machine Learning Research, 5(Jan), 73-99.
</p>
<p>Ginsbourger, D., Roustant, O., Schuhmacher, D., Durrande, N. and Lenz, N. (2016), <em>On <abbr><span class="acronym">ANOVA</span></abbr> decompositions of kernels and Gaussian random field paths</em>, Monte Carlo and Quasi-Monte Carlo Methods (pp. 315-330), Springer, Cham.
</p>
<p>Gretton, A., Bousquet, O., Smola, A., and Scholkopf, B. (2005), <em>Measuring statistical dependence with Hilbert-Schmidt norms</em>, International Conference on Algorithmic Learning Theory (pp. 63-77), Springer, Berlin, Heidelberg.
</p>
<p>Gretton, A., Borgwardt, K., Rasch, M., Scholkopf, B. and Smola, A. (2006), <em>A kernel method for the two-sample-problem</em>, Advances in Neural Information Processing Systems, 19.
</p>
<p>Le Maitre, O. and Knio, O. M. (2010), <em>Spectral methods for uncertainty quantification with applications to computational fluid dynamics</em>, Springer Science &amp; Business Media.
</p>
<p>Marrel, A. and Chabridon, V. (2021), <em>Statistical developments for target and conditional sensitivity analysis: application on safety studies for nuclear reactor</em>, Reliability Engineering &amp; System Safety, 214, 107711.
</p>
<p>Sakoe, H. and Chiba, S. (1978), <em>Dynamic programming algorithm optimization for spoken word recognition</em>, IEEE International Conference on Acoustics, Speech and Signal, 26(1), 43-49.
</p>
<p>Spagnol, A., Riche, R. L. and Veiga, S. D. (2019), <em>Global sensitivity analysis for optimization with variable selection</em>, SIAM/ASA Journal on Uncertainty Quantification, 7(2), 417-443.
</p>
<p>Sriperumbudur, B., Fukumizu, K. and Lanckriet, G. (2010), <em>On the relation between universality, characteristic kernels and <abbr><span class="acronym">RKHS</span></abbr> embedding of measures</em>, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (pp. 773-780). JMLR Workshop and Conference Proceedings.
</p>
<p>Szekely, G. J., Rizzo, M. L. and Bakirov, N. K. (2007), <em>Measuring and testing dependence by correlation of distances</em>, The Anals of Statistics, 35(6), 2769-2794.
</p>
<p>Wahba, G., Wang, Y., Gu, C., Klein, R. and Klein, B. (1995), <em>Smoothing spline <abbr><span class="acronym">ANOVA</span></abbr> for exponential families, with application to the Wisconsin Epidemiological Study of Diabetic Retinopathy: the 1994 Neyman Memorial Lecture</em>, The Annals of Statistics, 23(6), 1865-1895.
</p>


<h3>See Also</h3>

<p><code>testHSIC, weightTSA</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"> 

############################
### HSIC indices for GSA ###
############################

# Test case 1: the Friedman function
# --&gt; 5 input variables

### GSA with a given model ###

n &lt;- 800
p &lt;- 5
X &lt;- matrix(runif(n*p), n, p)

kernelX &lt;- c("rbf", "rbf", "laplace", "laplace", "sobolev1")
paramX &lt;- c(0.2, 0.3, 0.4, NA, NA)

# kernel for X1: Gaussian kernel with given parameter 0.2
# kernel for X2: Gaussian kernel with given parameter 0.3
# kernel for X3: exponential kernel with given parameter 0.4
# kernel for X4: exponential kernel with automatic computation of the parameter
# kernel for X5: Sobolev kernel (r=1) with no parameter

kernelY &lt;- "raquad"
paramY &lt;- NA 

sensi &lt;- sensiHSIC(model=friedman.fun, X,
                   kernelX=kernelX, paramX=paramX, 
                   kernelY=kernelY, paramY=paramY)

print(sensi)
plot(sensi)
title("GSA for the Friedman function")

### GSA with given data ###

Y &lt;- friedman.fun(X)
sensi &lt;- sensiHSIC(model=NULL, X,
                   kernelX=kernelX, paramX=paramX, 
                   kernelY=kernelY, paramY=paramY)
tell(sensi, y=Y)

print(sensi)

### GSA from a prior object of class "sensiHSIC" ###

new.sensi &lt;- sensiHSIC(model=friedman.fun, X,
                       kernelX=kernelX, paramX=paramX, 
                       kernelY=kernelY, paramY=paramY,
                       estimator.type="U-stat", 
                       sensi=sensi,
                       save.GM=list(KX=FALSE, KY=FALSE))

print(new.sensi)

# U-statistics are computed without rebuilding all Gram matrices.
# Those Gram matrices are not saved a second time.

##################################
### HSIC-ANOVA indices for GSA ###
##################################

# Test case 2: the Matyas function with Gaussian input variables
# --&gt; 3 input variables (including 1 dummy variable)

n &lt;- 10^3
p &lt;- 2

X &lt;- matrix(rnorm(n*p), n, p)

# The Sobolev kernel (with r=1) is used to achieve the HSIC-ANOVA decomposition.
# Both first-order and total-order HSIC-ANOVA indices are expected.

### AUTOMATIC RESCALING ###

kernelX &lt;- "sobolev1"
anova &lt;- list(obj="both", is.uniform=FALSE)

sensi.A &lt;- sensiHSIC(model=matyas.fun, X, kernelX=kernelX, anova=anova)

print(sensi.A)
plot(sensi.A)
title("GSA for the Matyas function")

### PROBLEM REFORMULATION ###

U &lt;- matrix(runif(n*p), n, p)
new.matyas.fun &lt;- function(U){ matyas.fun(qnorm(U)) }

kernelX &lt;- "sobolev1"
anova &lt;- list(obj="both", is.uniform=TRUE)

sensi.B &lt;- sensiHSIC(model=new.matyas.fun, U, kernelX=kernelX, anova=anova)

print(sensi.B)

####################################
### T-HSIC indices for target SA ###
####################################

# Test case 3: the Sobol function
# --&gt; 8 input variables

n &lt;- 10^3
p &lt;- 8

X &lt;- matrix(runif(n*p), n, p)

kernelY &lt;- "categ"
target &lt;- list(c=0.4, type="indicTh")

sensi &lt;- sensiHSIC(model=sobol.fun, X, kernelY=kernelY, target=target)

print(sensi)
plot(sensi)
title("TSA for the Sobol function")

#########################################
### C-HSIC indices for conditional SA ###
#########################################

# Test case 3: the Sobol function
# --&gt; 8 input variables

n &lt;- 10^3
p &lt;- 8

X &lt;- matrix(runif(n*p), n, p)

cond &lt;- list(c=0.2, type="exp1side", upper=FALSE)

sensi &lt;- sensiHSIC(model=sobol.fun, X, cond=cond)

print(sensi)
plot(sensi)
title("CSA for the Sobol function")

##########################################
### How to deal with discrete outputs? ###
##########################################

# Test case 4: classification of the Ishigami output
# --&gt; 3 input variables
# --&gt; 3 categories

classif &lt;- function(X){
  
  Ytemp &lt;- ishigami.fun(X) 
  Y &lt;- rep(NA, n)
  Y[Ytemp&lt;0] &lt;- 0
  Y[Ytemp&gt;=0 &amp; Ytemp&lt;10] &lt;- 1                
  Y[Ytemp&gt;=10] &lt;- 2  
  
  return(Y)
  
}

###

n &lt;- 10^3
p &lt;- 3

X &lt;- matrix(runif(n*p, -pi, pi), n, p)

kernelY &lt;- "categ"
paramY &lt;- 0

sensi &lt;- sensiHSIC(model=classif, X, kernelY=kernelY, paramY=paramY)
print(sensi)
plot(sensi)
title("GSA for the classified Ishigami function")

############################################
### How to deal with functional outputs? ###
############################################

# Test case 5: the arctangent temporal function
# --&gt; 3 input variables (including 1 dummy variable)

n &lt;- 500
p &lt;- 3

X &lt;- matrix(runif(n*p,-7,7), n, p)

### with a preliminary dimension reduction by PCA ###

kernelY &lt;- list(method="PCA", 
                data.centering=TRUE, data.scaling=TRUE,
                fam="rbf", expl.var=0.95, combi="sum", position="extern")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("PCA-based GSA for the arctangent temporal function")

### with a kernel based on dynamic time warping ###

kernelY &lt;- list(method="DTW", fam="rbf")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("DTW-based GSA for the arctangent temporal function")



### with the global alignment kernel ###

kernelY &lt;- list(method="GAK")

sensi &lt;- sensiHSIC(model=atantemp.fun, X, kernelY=kernelY)

print(sensi)
plot(sensi)
title("GAK-based GSA for the arctangent temporal function")

  
</code></pre>


</div>