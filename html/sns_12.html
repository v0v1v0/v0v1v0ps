<div class="container">

<table style="width: 100%;"><tr>
<td>sns.run</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Drawing multiple samples using Stochastic Newton Sampler
</h2>

<h3>Description</h3>

<p>This is a wrapper around <code>sns</code>, allowing one to draw multiple samples from a distribution while collecting diagnostic information.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sns.run(init, fghEval, niter = 100, nnr = min(10, round(niter/4))
  , mh.diag = FALSE, part = NULL, print.level = 0
  , report.progress = ceiling(niter/10)
  , numderiv = 0, numderiv.method = c("Richardson", "simple")
  , numderiv.args = list()
  , ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p>Initial value for the MCMC chain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fghEval</code></td>
<td>
<p>Log-density to be sampled from. A valid log-density can have one of 3 forms: 1) return log-density, but no gradient or Hessian, 2) return a list of <code>f</code> and <code>g</code> for log-density and its gradient vector, respectively, 3) return a list of <code>f</code>, <code>g</code>, and <code>h</code> for log-density, gradient vector, and Hessian matrix. Missing derivatives are computed numerically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>niter</code></td>
<td>
<p>Number of iterations to perform (in ‘nr’ and ‘mcmc’ mode combined).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nnr</code></td>
<td>
<p>Number of initial iterations to spend in ‘nr’ mode.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mh.diag</code></td>
<td>
<p>Boolean flag, indicating whether detailed MH diagnostics such as components of acceptance test must be returned or not.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>part</code></td>
<td>
<p>List describing partitioning of state space into subsets. Each element of the list must be an integer vector containing a set of indexes (between <code>1</code> and <code>length(x)</code> or <code>length(init)</code>) indicating which subset of all dimensions to jointly sample. These integer vectors must be mutually exclusive and collectively exhaustive, i.e. cover the entire state space and have no duplicates, in order for the partitioning to represent a valid Gibbs sampling approach. See <code>sns.make.part</code> and <code>sns.check.part</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print.level</code></td>
<td>
<p>If greater than 0, print sampling progress report.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>report.progress</code></td>
<td>
<p>Number of sampling iterations to wait before printing progress reports.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numderiv</code></td>
<td>
<p>Integer with value from the set <code>0,1,2</code>. If <code>0</code>, no numerical differentiation is performed, and thus <code>fghEval</code> is expected to supply <code>f</code>, <code>g</code> and <code>h</code>. If <code>1</code>, we expect <code>fghEval</code> to provide <code>f</code> amd <code>g</code>, and Hessian will be calculated numerically. If <code>2</code>, <code>fghEval</code> only returns log-density, and numerical differentiation is needed to calculate gradient and Hessian.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numderiv.method</code></td>
<td>
<p>Method used for numeric differentiation. This is passed to the <code>grad</code> and <code>hessian</code> functions in <span class="pkg">numDeriv</span> package. See the package documentation for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numderiv.args</code></td>
<td>
<p>Arguments to the numeric differentiation method chosen in <code>numderiv.method</code>, passed to <code>grad</code> and <code>hessian</code> functions in <span class="pkg">numDeriv</span>. See package documentation for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other parameters to be passed to <code>fghEval</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>sns.run</code> returns an object of class <code>sns</code> with elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>samplesMat</code></td>
<td>
<p>A matrix object with <code>nsample</code> rows and <code>K</code> cols.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>acceptance</code></td>
<td>
<p>Metropolis proposal percentage acceptance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burn.iters</code></td>
<td>
<p>Number of burn-in ierations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample.time</code></td>
<td>
<p>Time in seconds spent in sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin.time</code></td>
<td>
<p>Time in seconds spent in burn-in.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>1. <code>sns.run</code> cannot be used if SNS is being run as part of a Gibbs cycle, such that the conditional distribution being sampled by SNS changes from one iteration to next. In such cases, <code>sns</code> must be used instead, inside an explicit Gibbs-cycle <code>for</code> loop.
</p>
<p>2. See package vignette for more details on SNS theory, software, examples, and performance.
</p>


<h3>Author(s)</h3>

<p>Alireza S. Mahani, Asad Hasan, Marshall Jiang, Mansour T.A. Sharabiani
</p>


<h3>References</h3>

<p>Mahani A.S., Hasan A., Jiang M. &amp;  Sharabiani M.T.A. (2016). Stochastic Newton Sampler: The R Package sns. Journal of Statistical Software, Code Snippets, 74(2), 1-33. doi:10.18637/jss.v074.c02
</p>


<h3>See Also</h3>

<p><code>sns</code>, <code>summary.sns</code>, <code>plot.sns</code>, <code>predict.sns</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

# using RegressionFactory for generating log-likelihood and its derivatives
library(RegressionFactory)

loglike.poisson &lt;- function(beta, X, y) {
  regfac.expand.1par(beta, X = X, y = y,
    fbase1 = fbase1.poisson.log)
}

# simulating data
K &lt;- 5
N &lt;- 1000
X &lt;- matrix(runif(N * K, -0.5, +0.5), ncol = K)
beta &lt;- runif(K, -0.5, +0.5)
y &lt;- rpois(N, exp(X 

beta.init &lt;- rep(0.0, K)

# glm estimate (ML), for reference
beta.glm &lt;- glm(y ~ X - 1, family = "poisson",
                start = beta.init)$coefficients

# sampling of likelihood
beta.smp &lt;- sns.run(init = beta.init
  , fghEval = loglike.poisson, niter = 1000
  , nnr = 20, X = X, y = y)
smp.summ &lt;- summary(beta.smp)

# compare mean of samples against ML estimate (from glm)
cbind(beta.glm, smp.summ$smp$mean)

# trying numerical differentiation
loglike.poisson.fonly &lt;- function(beta, X, y) {
  regfac.expand.1par(beta, X = X, y = y, fgh = 0,
                     fbase1 = fbase1.poisson.log)
}
beta.smp &lt;- sns.run(init = beta.init
  , fghEval = loglike.poisson.fonly, niter = 1000, nnr = 20
  , X = X, y = y, numderiv = 2)
smp.summ &lt;- summary(beta.smp)
cbind(beta.glm, smp.summ$smp$mean)


## End(Not run)
</code></pre>


</div>