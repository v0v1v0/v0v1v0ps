<div class="container">

<table style="width: 100%;"><tr>
<td>spark_compilation_spec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Define a Spark Compilation Specification</h2>

<h3>Description</h3>

<p>For use with <code>compile_package_jars</code>. The Spark compilation
specification is used when compiling Spark extension Java Archives, and
defines which versions of Spark, as well as which versions of Scala, should
be used for compilation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_compilation_spec(
  spark_version = NULL,
  spark_home = NULL,
  scalac_path = NULL,
  scala_filter = NULL,
  jar_name = NULL,
  jar_path = NULL,
  jar_dep = NULL,
  embedded_srcs = "embedded_sources.R"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>spark_version</code></td>
<td>
<p>The Spark version to build against. This can
be left unset if the path to a suitable Spark home is supplied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>spark_home</code></td>
<td>
<p>The path to a Spark home installation. This can
be left unset if <code>spark_version</code> is supplied; in such a case,
<code>sparklyr</code> will attempt to discover the associated Spark
installation using <code>spark_home_dir</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scalac_path</code></td>
<td>
<p>The path to the <code>scalac</code> compiler to be used
during compilation of your Spark extension. Note that you should
ensure the version of <code>scalac</code> selected matches the version of
<code>scalac</code> used with the version of Spark you are compiling against.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scala_filter</code></td>
<td>
<p>An optional <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> function that can be used to filter
which <code>scala</code> files are used during compilation. This can be
useful if you have auxiliary files that should only be included with
certain versions of Spark.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jar_name</code></td>
<td>
<p>The name to be assigned to the generated <code>jar</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jar_path</code></td>
<td>
<p>The path to the <code>jar</code> tool to be used
during compilation of your Spark extension.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jar_dep</code></td>
<td>
<p>An optional list of additional <code>jar</code> dependencies.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>embedded_srcs</code></td>
<td>
<p>Embedded source file(s) under <code>&lt;R package root&gt;/java</code> to
be included in the root of the resulting jar file as resources</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Most Spark extensions won't need to define their own compilation specification,
and can instead rely on the default behavior of <code>compile_package_jars</code>.
</p>


</div>