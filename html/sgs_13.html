<div class="container">

<table style="width: 100%;"><tr>
<td>fit_sgs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit an SGS model.</h2>

<h3>Description</h3>

<p>Sparse-group SLOPE (SGS) main fitting function. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fit_sgs(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  path_length = 20,
  min_frac = 0.05,
  alpha = 0.95,
  vFDR = 0.1,
  gFDR = 0.1,
  pen_method = 1,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  standardise = "l2",
  intercept = TRUE,
  screen = TRUE,
  verbose = FALSE,
  w_weights = NULL,
  v_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li>
<p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between SLOPE and gSLOPE. Must be between 0 and 1. Recommended value is 0.95.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vFDR</code></td>
<td>
<p>Defines the desired variable false discovery rate (FDR) level, which determines the shape of the variable penalties. Must be between 0 and 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gFDR</code></td>
<td>
<p>Defines the desired group false discovery rate (FDR) level, which determines the shape of the group penalties. Must be between 0 and 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen_method</code></td>
<td>
<p>The type of penalty sequences to use (see Feser and Evangelou (2023)):
</p>

<ul>
<li> <p><code>"1"</code> uses the vMean SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"2"</code> uses the vMax SGS and gMean gSLOPE sequences.
</p>
</li>
<li> <p><code>"3"</code> uses the BH SLOPE and gMean gSLOPE sequences, also known as SGS Original.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/\sqrt{n}</code>.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/n</code>.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screen</code></td>
<td>
<p>Logical flag for whether to apply screening rules (see Feser and Evangelou (2024)). Screening discards irrelevant groups before fitting, greatly improving speed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the penalties from <code>pen_method</code> if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the penalties from <code>pen_method</code> if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>fit_sgs()</code> fits an SGS model (Feser and Evangelou (2023)) using adaptive three operator splitting (ATOS). SGS is a sparse-group method, so that it selects both variables and groups. Unlike group selection approaches, not every variable within a group is set as active.
It solves the convex optimisation problem given by
</p>
<p style="text-align: center;"><code class="reqn">
  \frac{1}{2n} f(b ; y, \mathbf{X}) + \lambda \alpha \sum_{i=1}^{p}v_i |b|_{(i)} + \lambda (1-\alpha)\sum_{g=1}^{m}w_g \sqrt{p_g} \|b^{(g)}\|_2,
</code>
</p>

<p>where the penalty sequences are sorted and <code class="reqn">f(\cdot)</code> is the loss function. In the case of the linear model, the loss function is given by the mean-squared error loss:
</p>
<p style="text-align: center;"><code class="reqn">
 f(b; y, \mathbf{X}) = \left\|y-\mathbf{X}b \right\|_2^2.
</code>
</p>

<p>In the logistic model, the loss function is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(b;y,\mathbf{X})=-1/n \log(\mathcal{L}(b; y, \mathbf{X})).
</code>
</p>

<p>where the log-likelihood is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{L}(b; y, \mathbf{X}) = \sum_{i=1}^{n}\left\{y_i b^\intercal x_i - \log(1+\exp(b^\intercal x_i)) \right\}.
</code>
</p>

<p>SGS can be seen to be a convex combination of SLOPE and gSLOPE, balanced through <code>alpha</code>, such that it reduces to SLOPE for <code>alpha = 0</code> and to gSLOPE for <code>alpha = 1</code>.
The penalty parameters in SGS are sorted so that the largest coefficients are matched with the largest penalties, to reduce the FDR.
For the group penalties, see <code>fit_gslope()</code>. For the variable penalties, the vMean SGS sequence (<code>pen_method=1</code>) (Feser and Evangelou (2023)) is given by
</p>
<p style="text-align: center;"><code class="reqn">
v_i^{\text{mean}} = \overline{F}_{\mathcal{N}}^{-1} \left( 1 - \frac{q_v i}{2p} \right), \; \text{where} \; \overline{F}_{\mathcal{N}}(x) := \frac{1}{m} \sum_{j=1}^{m} F_{\mathcal{N}} \left( \alpha x + \frac{1}{3} (1-\alpha) a_j w_j \right),\; i = 1,\ldots,p,
</code>
</p>

<p>where <code class="reqn">F_\mathcal{N}</code> is the cumulative distribution functions of a standard Gaussian distribution. The vMax SGS sequence (<code>pen_method=2</code>) (Feser and Evangelou (2023)) is given by
</p>
<p style="text-align: center;"><code class="reqn">
v_i^{\text{max}} = \max_{j=1,\dots,m} \left\{ \frac{1}{\alpha} F_{\mathcal{N}}^{-1} \left(1 - \frac{q_v i}{2p}\right) - \frac{1}{3\alpha}(1-\alpha) a_j w_j \right\},
</code>
</p>

<p>The BH SLOPE sequence (<code>pen_method=3</code>) (Bogdan et. al. (2015)) is given by
</p>
<p style="text-align: center;"><code class="reqn">
v_i = z(1-i q_v/2p),
</code>
</p>

<p>where <code class="reqn">z</code> is the quantile function of a standard normal distribution.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>z</code>, which is usually the former. A filter is applied to remove very small values, where ATOS has not been able to shrink exactly to zero. Check this against <code>x</code> and <code>z</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen_slope</code></td>
<td>
<p>Vector of the variable penalty sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pen_gslope</code></td>
<td>
<p>Vector of the group penalty sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Value(s) of <code class="reqn">\lambda</code> used to fit the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Bogdan, M., van den Berg, E., Sabatti, C., Candes, E. (2015). <em>SLOPE - Adaptive variable selection via convex optimization</em>, <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full">https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/SLOPEAdaptive-variable-selection-via-convex-optimization/10.1214/15-AOAS842.full</a>
</p>
<p>Feser, F., Evangelou, M. (2023). <em>Sparse-group SLOPE: adaptive bi-level selection with FDR-control</em>, <a href="https://arxiv.org/abs/2305.09467">https://arxiv.org/abs/2305.09467</a>
</p>
<p>Feser, F., Evangelou, M. (2024). <em>Strong screening rules for group-based SLOPE models</em>, <a href="https://arxiv.org/abs/2405.15357">https://arxiv.org/abs/2405.15357</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>


<h3>See Also</h3>

<p>Other SGS-methods: 
<code>as_sgs()</code>,
<code>coef.sgs()</code>,
<code>fit_sgo()</code>,
<code>fit_sgo_cv()</code>,
<code>fit_sgs_cv()</code>,
<code>plot.sgs()</code>,
<code>predict.sgs()</code>,
<code>print.sgs()</code>,
<code>scaled_sgs()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data =  gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run SGS 
model = fit_sgs(X = data$X, y = data$y, groups = groups, type="linear", path_length = 5, 
alpha=0.95, vFDR=0.1, gFDR=0.1, standardise = "l2", intercept = TRUE, verbose=FALSE)
</code></pre>


</div>