<div class="container">

<table style="width: 100%;"><tr>
<td>madgrad</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>madgrad</h2>

<h3>Description</h3>

<p>stochastic gradient descent optimizer
</p>


<h3>Usage</h3>

<pre><code class="language-R">madgrad(momentum = 0.9, weight_decay = 0, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>strength of momentum</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>l2 penalty on weights</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>epsilon</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Anonymous function that returns optimizer when called.
</p>


<h3>References</h3>

<p>Defazio, A., &amp; Jelassi, S. (2021). Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. arXiv preprint arXiv:2101.11075.
</p>


</div>