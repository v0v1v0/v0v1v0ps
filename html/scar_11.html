<div class="container">

<table style="width: 100%;"><tr>
<td>scar</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Compute the maximum likelihood estimator of the generalised additive regression 
with shape constraints
</h2>

<h3>Description</h3>

<p>This function uses the active set algorithm to compute the maximum likelihood 
estimator (mle) of the generalised additive regression with shape constraints. 
Each component function of the additive predictors is assumed to belong 
to one of the nine possible shape restrictions. The estimator's value at the 
data points is unique.
</p>
<p>The output is an object of class <code>scar</code> which contains all the information 
needed to plot the estimator using the <code>plot</code> method, or 
to evaluate it using the <code>predict</code> method.
</p>


<h3>Usage</h3>

<pre><code class="language-R">scar(x, y, shape = rep("l", d), family = gaussian(),
  weights = rep(1, length(y)), epsilon = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Observed covariates in <code class="reqn">R^d</code>, in the form of an <code class="reqn">n \times d</code> 
numeric <code>matrix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Observed responses, in the form of a numeric <code>vector</code> of length <code class="reqn">n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shape</code></td>
<td>
<p>A vector that specifies the shape restrictions for each component function,
in the form of a string vector of length <code class="reqn">d</code>. The string allowed and its 
corresponding shape constraint is listed as follows (see Details):
</p>
<p><code>l</code>:	 linear
</p>
<p><code>in</code>:	 monotonically increasing
</p>
<p><code>de</code>:	 monotonically decreasing
</p>
<p><code>cvx</code>:	 convex
</p>
<p><code>cvxin</code>:	 convex and increasing	
</p>
<p><code>cvxde</code>:	 convex and decreasing	
</p>
<p><code>ccv</code>:	 concave
</p>
<p><code>ccvin</code>:	 concave and increasing	
</p>
<p><code>ccvde</code>:	 concave and decreasing</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A description of the error distribution and link function to
be used in the model. This can be a character string naming a
family function, a family function or the result of a call to
a family function.  Currently only the following five common 
exponential families are allowed: Gaussian, Binomial, Poisson,
and Gamma. By default the canonical link function is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>An optional vector of prior weights to be used when maximising the 
likelihood. It is a numeric vector of length <code class="reqn">n</code>. By default 
equal weights are used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Positive convergence tolerance epsilon when performing the 
iteratively reweighted least squares (IRLS) method at each iteration of 
the active set algorithm.  See <code>glm.control</code> 
for more details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For <code class="reqn">i = 1,\ldots,n</code>, let <code class="reqn">X_i</code> be the <code class="reqn">d</code>-dimensional
covariates, <code class="reqn">Y_i</code> be the corresponding one-dimensional response and 
<code class="reqn">w_i</code> be its weight. The generalised additive model can be written as 
</p>
<p style="text-align: center;"><code class="reqn">g(\mu) = f(x),</code>
</p>
<p> where <code class="reqn">x=(x_1,\ldots,x_d)^T</code>,
<code class="reqn">g</code> is a known link function and <code class="reqn">f</code> is an additive function (to be estimated). 
</p>
<p>Assume the canonical link function is used here, then the maximum likelihood estimator 
of the generalised additive model based on observations 
<code class="reqn">(X_1,Y_1), \ldots, (X_n,Y_n)</code> 
is the function that maximises 
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n} \sum_{i=1}^n w_i \{Y_i f(X_i) - B(f(X_i))\}</code>
</p>
 
<p>subject to the restrictions that for every <code class="reqn">j = 1,\ldots,d</code>, 
the <code class="reqn">j</code>-th additive component of <code class="reqn">f</code> satisfies the constraint indicated by the  
<code class="reqn">j</code>-th element of <code>shape</code>. Here <code class="reqn">B(.)</code> is the log-partition function of 
the specified exponential family distribution, and <code class="reqn">w_i</code> are the weights. For i.i.d. data, 
<code class="reqn">w_i</code> should be <code class="reqn">1</code> for each <code class="reqn">i</code>.
</p>
<p>To make each component of <code class="reqn">f</code> identifiable, we write
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \sum_{j=1}^d f_j(x_j) + c</code>
</p>
  
<p>and let <code class="reqn">f_j(0) = 0</code> for every <code class="reqn">j = 1,\ldots,d</code>. 
In case zero is outside the range of the <code class="reqn">j</code>-th observed covariate, 
for the sake of convenience, we set <code class="reqn">f_j</code> to be zero at the sample mean of
the <code class="reqn">j</code>-th predictor.
</p>
<p>This problem can then be re-written as a concave optimisation problem, and 
our function uses the active set algorithm to find out the maximum likelihood estimator. 
A general introduction can be found in <cite>Nocedal and Wright (2006)</cite>. 
A detailed description of our algorithm can be found in <cite>Chen and Samworth (2016)</cite>.
See also <cite>Groeneboom, Jongbloed and Wellner (2008)</cite> for some theoretical supports.
</p>


<h3>Value</h3>

<p>An object of class <code>scar</code>, with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Covariates copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shape</code></td>
<td>
<p>Shape vector copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>The vector of weights copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>The exponential family copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>componentfit</code></td>
<td>
<p>Value of the fitted component function at each observed 
covariate, in the form of an <code class="reqn">n \times d</code> numeric <code>matrix</code>,
where the element at the <code class="reqn">i</code>-th row and the <code class="reqn">j</code>-th column
is the value of <code class="reqn">f_j</code> at the <code class="reqn">j</code>-th coordinate of <code class="reqn">X_i</code>,
with the identifiability condition satisfied (see Details)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constant</code></td>
<td>
<p>The estimated value of the constant <code class="reqn">c</code> in the 
additive function <code class="reqn">f</code> (see Details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deviance</code></td>
<td>
<p>Up to a constant, minus twice the maximised log-likelihood.
Where applicable, the constant is chosen to make the saturated
model to have zero deviance. See also <code>glm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nulldeviance</code></td>
<td>
<p>The deviance for the null model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Total number of iterations of the active set algorithm</p>
</td>
</tr>
</table>
<p>.
</p>


<h3>Note</h3>

<p>We acknowledge that <code>glm.fit</code> from the R package 
<span class="pkg">stats</span> is called to perform the method of iterated reweighted least squares 
(IRLS) in our routine. It is possible to speed up the implementation considerably
by simply suppressing all the run-time checks there. 
</p>
<p>If all the component functions are linear, then it is prefered to call directly 
the function <code>glm</code>.
</p>
<p>For the one-dimensional covariate, see the pool adjacent violators algorithm (PAVA)
of <cite>Robertson, Wright and Dykstra (1998)</cite> and the support reduction method of 
<cite>Groeneboom, Jongbloed and Wellner (2008)</cite>. 
</p>
<p>A different approach to tackle this problem is to use splines. See the R package 
<code>scam</code>. We stress here that our approach is free 
of tuning parameters while <code>scam</code> is not, which 
can be viewed as a major difference.
</p>
<p>To estimate the generalised additive regression function without any shape 
restrictions, see <cite>Wood (2004)</cite> and <cite>Hastie and Tibshirani (1990)</cite>.
Their corresponding R implementations are <code>mgcv</code>
and <code>gam</code>.
</p>


<h3>Author(s)</h3>

<p>Yining Chen and Richard Samworth</p>


<h3>References</h3>

<p>Chen, Y. and Samworth, R. J. (2016).  Generalized additive and index models with shape constraints.
Journal of the Royal Statistical Society: Series B, 78, 729-754.
</p>
<p>Groeneboom, P., Jongbloed, G. and Wellner, J.A. (2008).
The support reduction algorithm for computing non-parametric function 
estimates in mixture models. Scandinavian Journal of Statistics, 35, 385-399.
</p>
<p>Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. 
Chapman and Hall, London.
</p>
<p>Meyer, M. C. (2013).  Semi-parametric additive constrained regression.
Journal of nonparametric statistics, 25, 715-743.
</p>
<p>Nocedal, J., and Wright, S. J. (2006). Numerical Optimization, 2nd edition. 
Springer, New York.
</p>
<p>Robertson, T., Wright, F. T. and Dykstra, R. L. (1988). 
Order Restricted Statistical Inference. Wiley, New York.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002). Modern Applied
Statistics with S. Springer, New York.
</p>
<p>Wood, S.N. (2004). Stable and efficient multiple smoothing parameter estimation 
for generalized additive models. Journal of American Statistical Association, 
99, 673-686.
</p>


<h3>See Also</h3>

<p><code>plot.scar</code>, <code>predict.scar</code>, <code>scair</code>, 
<code>scam</code>, <code>mgcv</code>,
<code>gam</code>, <code>glm</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## An example in the Poission additive regression setting:
## Define the additive function f on the scale of the predictors
f&lt;-function(x){
  return(1*abs(x[,1]) + 2*(x[,2])^2 + 3*(abs(x[,3]))^3) 
}

## Simulate the covariates and the responses
## covariates are drawn uniformly from [-1,1]^3
set.seed(0)
d = 3
n = 500
x = matrix(runif(n*d)*2-1,nrow=n,ncol=d) 
rpoisson &lt;- function(m){rpois(1,exp(m))}
y = sapply(f(x),rpoisson)

## All the components are convex so one can use scar
shape=c("cvx","cvx","cvx")
object = scar(x,y,shape=shape, family=poisson())

## Plot each component of the estimatied additive function
plot(object)

## Evaluate the estimated additive function at 10^4 random points 
## drawing from the interior of the support
testx = matrix((runif(10000*d)*1.96-0.98),ncol=d)
testf = predict(object,testx)

## and calculate the (estimated) absolute prediction error
mean(abs(testf-f(testx))) 
</code></pre>


</div>