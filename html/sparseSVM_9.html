<div class="container">

<table style="width: 100%;"><tr>
<td>sparseSVM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit sparse linear SVM with lasso or elasti-net regularization</h2>

<h3>Description</h3>

<p>Fit solution paths for sparse linear SVM regularized by lasso or elastic-net 
over a grid of values for the regularization parameter lambda.</p>


<h3>Usage</h3>

<pre><code class="language-R">sparseSVM(X, y, alpha = 1, gamma = 0.1, nlambda=100, 
	  lambda.min = ifelse(nrow(X)&gt;ncol(X), 0.01, 0.05), 
          lambda, preprocess = c("standardize", "rescale", "none"),  
          screen = c("ASR", "SR", "none"), max.iter = 1000, eps = 1e-5, 
          dfmax = ncol(X)+1, penalty.factor=rep(1, ncol(X)), message = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Output vector. Currently the function only supports binary output and converts 
the output into +1/-1 coding internally.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The elastic-net mixing parameter that controls the relative contribution 
from the lasso and the ridge penalty. It must be a number between 0 and 1. <code>alpha=1</code> 
is the lasso penalty and <code>alpha=0</code> the ridge penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>The tuning parameter for huberization smoothing of hinge loss. Default is 0.1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of lambda values.  Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min</code></td>
<td>
<p>The smallest value for lambda, as a fraction of lambda.max, the data 
derived entry value. Default is 0.01 if the number of observations is larger than the 
number of variables and 0.05 otherwise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A user-specified sequence of lambda values. Typical usage is to leave 
blank and have the program automatically compute a <code>lambda</code> sequence based on 
<code>nlambda</code> and <code>lambda.min</code>. Specifying <code>lambda</code> overrides this. This 
argument should be used with care and supplied with a decreasing sequence instead of 
a single value. To get coefficients for a single <code>lambda</code>, use <code>coef</code> or 
<code>predict</code> instead after fitting the solution path with <code>sparseSVM</code>.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preprocess</code></td>
<td>
<p>Preprocessing technique to be applied to the input. Either 
"standardize" (default), "rescale" or "none" (see <code>Details</code>). The coefficients 
are always returned on the original scale.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screen</code></td>
<td>
<p>Screening rule to be applied at each <code>lambda</code> that discards variables 
for speed. Either "ASR" (default), "SR" or "none". "SR" stands for the strong rule, 
and "ASR" for the adaptive strong rule. Using "ASR" typically requires fewer iterations 
to converge than "SR", but the computing time are generally close. Note that the option 
"none" is used mainly for debugging, which may lead to much longer computing time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>Maximum number of iterations. Default is 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Convergence threshold. The algorithms continue until the maximum change in the
objective after any coefficient update is less than <code>eps</code> times the null deviance. 
Default is <code>1E-7</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>Upper bound for the number of nonzero coefficients. The algorithm exits and 
returns a partial path if <code>dfmax</code> is reached. Useful for very large dimensions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>A numeric vector of length equal to the number of variables. Each 
component multiplies <code>lambda</code> to allow differential penalization. Can be 0 for 
some variables, in which case the variable is always in the model without penalization. 
Default is 1 for all variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>message</code></td>
<td>
<p>If set to TRUE,  sparseSVM will inform the user of its progress. This argument 
is kept for debugging. Default is FALSE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sequence of models indexed by the regularization parameter <code>lambda</code> is fitted
using a semismooth Newton coordinate descent algorithm. The objective function is defined 
to be </p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n} \sum hingeLoss(y_i (x_i' w + b)) + \lambda\textrm{penalty}(w).</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">hingeLoss(t) = max(0, 1-t)</code>
</p>
<p> and the intercept <code>b</code> is unpenalized. 
</p>
<p>The program supports different types of preprocessing techniques. They are applied to 
each column of the input matrix <code>X</code>. Let x be a column of <code>X</code>. For 
<code>preprocess = "standardize"</code>, the formula is 
</p>
<p style="text-align: center;"><code class="reqn">x' = \frac{x-mean(x)}{sd(x)};</code>
</p>

<p>for <code>preprocess = "rescale"</code>, 
</p>
<p style="text-align: center;"><code class="reqn">x' = \frac{x-min(x)}{max(x)-min(x)}.</code>
</p>

<p>The models are fit with preprocessed input, then the coefficients are transformed back
to the original scale via some algebra.
</p>


<h3>Value</h3>

<p>The function returns an object of S3 class <code>"sparseSVM"</code>, which is a list containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>The call that produced this object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>The fitted matrix of coefficients.  The number of rows is equal to the number 
of coefficients, and the number of columns is equal to <code>nlambda</code>. An intercept is included.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>A vector of length <code>nlambda</code> containing the number of iterations until 
convergence at each value of <code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>saturated</code></td>
<td>
<p>A logical flag for whether the number of nonzero coefficients has reached <code>dfmax</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The sequence of regularization parameter values in the path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Same as above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>Same as above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>Same as above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>levels</code></td>
<td>
<p>Levels of the output class labels.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Congrui Yi and Yaohui Zeng <br>
Maintainer: Congrui Yi &lt;eric.ycr@gmail.com&gt;
</p>


<h3>See Also</h3>

<p><code>plot.sparseSVM</code>, <code>cv.sparseSVM</code></p>


<h3>Examples</h3>

<pre><code class="language-R">X = matrix(rnorm(1000*100), 1000, 100)
b = 3
w = 5*rnorm(10)
eps = rnorm(1000)
y = sign(b + drop(X[,1:10] %*% w + eps))

fit = sparseSVM(X, y)
coef(fit, 0.05)
predict(fit, X[1:5,], lambda = c(0.2, 0.1))
</code></pre>


</div>