<div class="container">

<table style="width: 100%;"><tr>
<td>spark_write_rds</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Write Spark DataFrame to RDS files</h2>

<h3>Description</h3>

<p>Write Spark dataframe to RDS files. Each partition of the dataframe will be
exported to a separate RDS file so that all partitions can be processed in
parallel.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_write_rds(x, dest_uri)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A Spark DataFrame to be exported</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dest_uri</code></td>
<td>
<p>Can be a URI template containing 'partitionId' (e.g.,
<code>"hdfs://my_data_part_{partitionId}.rds"</code>) where 'partitionId' will be
substituted with ID of each partition using 'glue', or a list of URIs
to be assigned to RDS output from all partitions (e.g.,
<code>"hdfs://my_data_part_0.rds"</code>, <code>"hdfs://my_data_part_1.rds"</code>, and so on)
If working with a Spark instance running locally, then all URIs should be
in <code>"file://&lt;local file path&gt;"</code> form. Otherwise the scheme of the URI should
reflect the underlying file system the Spark instance is working with
(e.g., "hdfs://"). If the resulting list of URI(s) does not contain unique
values, then it will be post-processed with 'make.unique()' to ensure
uniqueness.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A tibble containing partition ID and RDS file location for each
partition of the input Spark dataframe.
</p>


</div>