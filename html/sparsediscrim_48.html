<div class="container">

<table style="width: 100%;"><tr>
<td>lda_eigen</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The Minimum Distance Rule using Moore-Penrose Inverse (MDMP) classifier</h2>

<h3>Description</h3>

<p>Given a set of training data, this function builds the MDMP classifier from
Srivistava and Kubokawa (2007). The MDMP classifier is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Srivastava and Kubokawa (2007) have
proposed a modification of the standard maximum likelihood estimator of the
pooled covariance matrix, where only the largest 95% of the eigenvalues and
their corresponding eigenvectors are kept. The value of 95% is the default
and can be changed via the <code>eigen_pct</code> argument.
</p>
<p>The MDMP classifier from Srivistava and Kubokawa (2007) is an adaptation of the
linear discriminant analysis (LDA) classifier that is designed for
small-sample, high-dimensional data. Srivastava and Kubokawa (2007) have
proposed a modification of the standard maximum likelihood estimator of the
pooled covariance matrix, where only the largest 95% of the eigenvalues and
their corresponding eigenvectors are kept.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lda_eigen(x, ...)

## Default S3 method:
lda_eigen(x, y, prior = NULL, eigen_pct = 0.95, ...)

## S3 method for class 'formula'
lda_eigen(formula, data, prior = NULL, ...)

## S3 method for class 'lda_eigen'
predict(object, newdata, type = c("class", "prob", "score"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Matrix or data frame containing the training data. The rows are the
sample observations, and the columns are the features. Only complete data are
retained.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments (not currently used).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of class labels for each training observation. Only complete
data are retained.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>Vector with prior probabilities for each class. If NULL
(default), then equal probabilities are used. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eigen_pct</code></td>
<td>
<p>the percentage of eigenvalues kept</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A formula of the form <code>groups ~ x1 + x2 + ...</code> That is,
the response is the grouping factor and the right hand side specifies the
(non-factor) discriminators.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>data frame from which variables specified in <code>formula</code> are
preferentially to be taken.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Fitted model object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>Matrix or data frame of observations to predict. Each row
corresponds to a new observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Prediction type: either <code>"class"</code>, <code>"prob"</code>, or <code>"score"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The matrix of training observations are given in <code>x</code>. The rows of <code>x</code>
contain the sample observations, and the columns contain the features for each
training observation.
</p>
<p>The vector of class labels given in <code>y</code> are coerced to a <code>factor</code>.
The length of <code>y</code> should match the number of rows in <code>x</code>.
</p>
<p>An error is thrown if a given class has less than 2 observations because the
variance for each feature within a class cannot be estimated with less than 2
observations.
</p>
<p>The vector, <code>prior</code>, contains the <em>a priori</em> class membership for
each class. If <code>prior</code> is NULL (default), the class membership
probabilities are estimated as the sample proportion of observations belonging
to each class. Otherwise, <code>prior</code> should be a vector with the same length
as the number of classes in <code>y</code>. The <code>prior</code> probabilities should be
nonnegative and sum to one.
</p>


<h3>Value</h3>

<p><code>lda_eigen</code> object that contains the trained MDMP classifier
</p>


<h3>References</h3>

<p>Srivastava, M. and Kubokawa, T. (2007). "Comparison of
Discrimination Methods for High Dimensional Data," Journal of the Japanese
Statistical Association, 37, 1, 123-134.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(modeldata)
data(penguins)
pred_rows &lt;- seq(1, 344, by = 20)
penguins &lt;- penguins[, c("species", "body_mass_g", "flipper_length_mm")]
mdmp_out &lt;- lda_eigen(species ~ ., data = penguins[-pred_rows, ])
predicted &lt;- predict(mdmp_out, penguins[pred_rows, -1], type = "class")

mdmp_out2 &lt;- lda_eigen(x = penguins[-pred_rows, -1], y = penguins$species[-pred_rows])
predicted2 &lt;- predict(mdmp_out2, penguins[pred_rows, -1], type = "class")
all.equal(predicted, predicted2)
</code></pre>


</div>