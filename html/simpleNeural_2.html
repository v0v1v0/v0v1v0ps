<div class="container">

<table style="width: 100%;"><tr>
<td>sN.MLPtrain</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Trains a multilayer perceptron with 1 hidden layer</h2>

<h3>Description</h3>

<p>Trains a multilayer perceptron with 1 hidden layer and a sigmoid activation function,
using backpropagation and gradient descent.
Don't forget to normalize the data first - sN.normalizeDF(), provided in the package, can be used to do so.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sN.MLPtrain(X, y, hidden_layer_size = 5, it = 50, lambda = 0.5,
  alpha = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Matrix of predictors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of output (the ANN learns y=ANN(X)).
Classes should be assigned an integer number, starting at 0 for the first class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_layer_size</code></td>
<td>
<p>Number of units in the hidden layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>it</code></td>
<td>
<p>Number of iterations for the gradient descent.
The default value of 50 may be a little low in some cases. 100 to 1000 are generally sensible values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Penalization for model coefficients (regularization parameter)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Speed multiplier (learning rate) for gradient descent</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The coefficients of the MLP, in a list (Theta1 between input and hidden layers, Theta2 between hidden and output layers)
</p>


<h3>References</h3>

<p>M.W Gardner, S.R Dorling, Artificial neural networks (the multilayer perceptron)-
a review of applications in the atmospheric sciences, Atmospheric Environment, Volume 32,
Issues 14-15, 1 August 1998, Pages 2627-2636, ISSN 1352-2310, doi: 10.1016/S1352-2310(97)00447-0
[<a href="http://www.sciencedirect.com/science/article/pii/S1352231097004470">http://www.sciencedirect.com/science/article/pii/S1352231097004470</a>]
</p>
<p>Jain, A.K.; Jianchang Mao; Mohiuddin, K.M., "Artificial neural networks: a tutorial,"
Computer , vol.29, no.3, pp.31,44, Mar 1996. doi: 10.1109/2.485891
[<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=485891&amp;isnumber=10412">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=485891&amp;isnumber=10412</a>]
</p>
<p>Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams.
"Learning Internal Representations by Error Propagation". David E. Rumelhart, James L. McClelland, and
the PDP research group (editors).
Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 1986.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># NB: the provided examples are just here to help use the package's functions.
# In real use cases you should perform a proper validation (cross-validation,
# external validation data...)
data(UCI.BCD.Wisconsin);
X=as.matrix(sN.normalizeDF(as.data.frame(UCI.BCD.Wisconsin[,3:32])));
y=as.matrix(UCI.BCD.Wisconsin[,2]);
myMLP=sN.MLPtrain(X=X,y=y,hidden_layer_size=20,it=50,lambda=0.5,alpha=0.5);
myPrediction=sN.MLPpredict(nnModel=myMLP,X=X,raw=TRUE);
#library('verification');
#roc.area(y,myPrediction[,2]);
</code></pre>


</div>