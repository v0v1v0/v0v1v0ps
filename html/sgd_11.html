<div class="container">

<table style="width: 100%;"><tr>
<td>sgd</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Stochastic gradient descent</h2>

<h3>Description</h3>

<p>Run stochastic gradient descent in order to optimize the induced loss
function given a model and data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sgd(x, ...)

## S3 method for class 'formula'
sgd(formula, data, model, model.control = list(), sgd.control = list(...), ...)

## S3 method for class 'matrix'
sgd(x, y, model, model.control = list(), sgd.control = list(...), ...)

## S3 method for class 'big.matrix'
sgd(x, y, model, model.control = list(), sgd.control = list(...), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x, y</code></td>
<td>
<p>a design matrix and the respective vector of outcomes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments to be used to form the default <code>sgd.control</code>
arguments if it is not supplied directly.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>an object of class <code>"formula"</code> (or one that can be
coerced to that class): a symbolic description of the model to be fitted.
The details can be found in <code>"glm"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>an optional data frame, list or environment (or object coercible
by <code>as.data.frame</code> to a data frame) containing the
variables in the model. If not found in data, the variables are taken from
environment(formula), typically the environment from which glm is called.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>character specifying the model to be used: <code>"lm"</code> (linear
model), <code>"glm"</code> (generalized linear model), <code>"cox"</code> (Cox
proportional hazards model), <code>"gmm"</code> (generalized method of moments),
<code>"m"</code> (M-estimation). See ‘Details’.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.control</code></td>
<td>
<p>a list of parameters for controlling the model.
</p>

<dl>
<dt>
<code>family</code> (<code>"glm"</code>)</dt>
<dd>
<p>a description of the error distribution and
link function to be used in the model. This can be a character string
naming a family function, a family function or the result of a call to
a family function. (See <code>family</code> for details of
family functions.)</p>
</dd>
<dt>
<code>rank</code> (<code>"glm"</code>)</dt>
<dd>
<p>logical. Should the rank of the design matrix
be checked?</p>
</dd>
<dt>
<code>fn</code> (<code>"gmm"</code>)</dt>
<dd>
<p>a function <code class="reqn">g(\theta,x)</code> which returns a
<code class="reqn">k</code>-vector corresponding to the <code class="reqn">k</code> moment conditions. It is a
required argument if <code>gr</code> not specified.</p>
</dd>
<dt>
<code>gr</code> (<code>"gmm"</code>)</dt>
<dd>
<p>a function to return the gradient. If
unspecified, a finite-difference approximation will be used.</p>
</dd>
<dt>
<code>nparams</code> (<code>"gmm"</code>)</dt>
<dd>
<p>number of model parameters. This is
automatically determined for other models.</p>
</dd>
<dt>
<code>type</code> (<code>"gmm"</code>)</dt>
<dd>
<p>character specifying the generalized method of
moments procedure: <code>"twostep"</code> (Hansen, 1982), <code>"iterative"</code>
(Hansen et al., 1996). Defaults to <code>"iterative"</code>.</p>
</dd>
<dt>
<code>wmatrix</code> (<code>"gmm"</code>)</dt>
<dd>
<p>weighting matrix to be used in the loss
function. Defaults to the identity matrix.</p>
</dd>
<dt>
<code>loss</code> (<code>"m"</code>)</dt>
<dd>
<p>character specifying the loss function to be
used in the estimating equation. Default is the Huber loss.</p>
</dd>
<dt><code>lambda1</code></dt>
<dd>
<p>L1 regularization parameter. Default is 0.</p>
</dd>
<dt><code>lambda2</code></dt>
<dd>
<p>L2 regularization parameter. Default is 0.</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sgd.control</code></td>
<td>
<p>an optional list of parameters for controlling the estimation.
</p>

<dl>
<dt><code>method</code></dt>
<dd>
<p>character specifying the method to be used: <code>"sgd"</code>,
<code>"implicit"</code>, <code>"asgd"</code>, <code>"ai-sgd"</code>, <code>"momentum"</code>,
<code>"nesterov"</code>. Default is <code>"ai-sgd"</code>. See ‘Details’.</p>
</dd>
<dt><code>lr</code></dt>
<dd>
<p>character specifying the learning rate to be used:
<code>"one-dim"</code>, <code>"one-dim-eigen"</code>, <code>"d-dim"</code>,
<code>"adagrad"</code>, <code>"rmsprop"</code>. Default is <code>"one-dim"</code>.
See ‘Details’.</p>
</dd>
<dt><code>lr.control</code></dt>
<dd>
<p>vector of scalar hyperparameters one can
set dependent on the learning rate. For hyperparameters aimed
to be left as default, specify <code>NA</code> in the corresponding
entries. See ‘Details’.</p>
</dd>
<dt><code>start</code></dt>
<dd>
<p>starting values for the parameter estimates. Default is
random initialization around zero.</p>
</dd>
<dt><code>size</code></dt>
<dd>
<p>number of SGD estimates to store for diagnostic purposes
(distributed log-uniformly over total number of iterations)</p>
</dd>
<dt><code>reltol</code></dt>
<dd>
<p>relative convergence tolerance. The algorithm stops
if it is unable to change the relative mean squared difference in the
parameters by more than the amount. Default is <code>1e-05</code>.</p>
</dd>
<dt><code>npasses</code></dt>
<dd>
<p>the maximum number of passes over the data. Default
is 3.</p>
</dd>
<dt><code>pass</code></dt>
<dd>
<p>logical. Should <code>tol</code> be ignored and run the
algorithm for all of <code>npasses</code>?</p>
</dd>
<dt><code>shuffle</code></dt>
<dd>
<p>logical. Should the algorithm shuffle the data set
including for each pass?</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>logical. Should the algorithm print progress?</p>
</dd>
</dl>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Models:
The Cox model assumes that the survival data is ordered when passed
in, i.e., such that the risk set of an observation i is all data points after
it.
</p>
<p>Methods:
</p>

<dl>
<dt><code>sgd</code></dt>
<dd>
<p>stochastic gradient descent (Robbins and Monro, 1951)</p>
</dd>
<dt><code>implicit</code></dt>
<dd>
<p>implicit stochastic gradient descent (Toulis et al.,
2014)</p>
</dd>
<dt><code>asgd</code></dt>
<dd>
<p>stochastic gradient with averaging (Polyak and Juditsky,
1992)</p>
</dd>
<dt><code>ai-sgd</code></dt>
<dd>
<p>implicit stochastic gradient with averaging (Toulis et
al., 2015)</p>
</dd>
<dt><code>momentum</code></dt>
<dd>
<p>"classical" momentum (Polyak, 1964)</p>
</dd>
<dt><code>nesterov</code></dt>
<dd>
<p>Nesterov's accelerated gradient (Nesterov, 1983)</p>
</dd>
</dl>
<p>Learning rates and hyperparameters:
</p>

<dl>
<dt><code>one-dim</code></dt>
<dd>
<p>scalar value prescribed in Xu (2011) as
</p>
<p style="text-align: center;"><code class="reqn">a_n = scale * gamma/(1 + alpha*gamma*n)^(-c)</code>
</p>

<p>where the defaults are
<code>lr.control = (scale=1, gamma=1, alpha=1, c)</code>
where <code>c</code> is <code>1</code> if implemented without averaging,
<code>2/3</code> if with averaging</p>
</dd>
<dt><code>one-dim-eigen</code></dt>
<dd>
<p>diagonal matrix
<code>lr.control = NULL</code></p>
</dd>
<dt><code>d-dim</code></dt>
<dd>
<p>diagonal matrix
<code>lr.control = (epsilon=1e-6)</code></p>
</dd>
<dt><code>adagrad</code></dt>
<dd>
<p>diagonal matrix prescribed in Duchi et al. (2011) as
<code>lr.control = (eta=1, epsilon=1e-6)</code></p>
</dd>
<dt><code>rmsprop</code></dt>
<dd>
<p>diagonal matrix prescribed in Tieleman and Hinton
(2012) as
<code>lr.control = (eta=1, gamma=0.9, epsilon=1e-6)</code></p>
</dd>
</dl>
<h3>Value</h3>

<p>An object of class <code>"sgd"</code>, which is a list containing the following
components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>name of the model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>a named vector of coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>logical. Was the algorithm judged to have converged?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimates</code></td>
<td>
<p>estimates from algorithm stored at each iteration
specified in <code>pos</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p>the fitted mean values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pos</code></td>
<td>
<p>vector of indices specifying the iteration number each estimate
was stored for</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals</code></td>
<td>
<p>the residuals, that is response minus fitted values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>times</code></td>
<td>
<p>vector of times in seconds it took to complete the number of
iterations specified in <code>pos</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model.out</code></td>
<td>
<p>a list of model-specific output attributes</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Dustin Tran, Tian Lan, Panos Toulis, Ye Kuang, Edoardo Airoldi
</p>


<h3>References</h3>

<p>John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for
online learning and stochastic optimization. <em>Journal of Machine
Learning Research</em>, 12:2121-2159, 2011.
</p>
<p>Yurii Nesterov. A method for solving a convex programming problem with
convergence rate <code class="reqn">O(1/k^2)</code>. <em>Soviet Mathematics Doklady</em>,
27(2):372-376, 1983.
</p>
<p>Boris T. Polyak. Some methods of speeding up the convergence of iteration
methods. <em>USSR Computational Mathematics and Mathematical Physics</em>,
4(5):1-17, 1964.
</p>
<p>Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic
approximation by averaging. <em>SIAM Journal on Control and Optimization</em>,
30(4):838-855, 1992.
</p>
<p>Herbert Robbins and Sutton Monro. A stochastic approximation method.
<em>The Annals of Mathematical Statistics</em>, pp. 400-407, 1951.
</p>
<p>Panos Toulis, Jason Rennie, and Edoardo M. Airoldi, "Statistical analysis of
stochastic gradient methods for generalized linear models", In
<em>Proceedings of the 31st International Conference on Machine Learning</em>,
2014.
</p>
<p>Panos Toulis, Dustin Tran, and Edoardo M. Airoldi, "Stability and optimality
in stochastic gradient descent", arXiv preprint arXiv:1505.02417, 2015.
</p>
<p>Wei Xu. Towards optimal one pass large scale learning with averaged
stochastic gradient descent. arXiv preprint arXiv:1107.2490, 2011.
</p>
<p># Dimensions
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Linear regression
set.seed(42)
N &lt;- 1e4
d &lt;- 5
X &lt;- matrix(rnorm(N*d), ncol=d)
theta &lt;- rep(5, d+1)
eps &lt;- rnorm(N)
y &lt;- cbind(1, X) %*% theta + eps
dat &lt;- data.frame(y=y, x=X)
sgd.theta &lt;- sgd(y ~ ., data=dat, model="lm")
sprintf("Mean squared error: %0.3f", mean((theta - as.numeric(sgd.theta$coefficients))^2))


</code></pre>


</div>