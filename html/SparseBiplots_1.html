<div class="container">

<table style="width: 100%;"><tr>
<td>ElasticNet_HJBiplot</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Elastic Net HJ Biplot</h2>

<h3>Description</h3>

<p>This function is a generalization of the Ridge regularization method and the LASSO penalty. Realizes the representation of the SPARSE HJ Biplot through a combination of LASSO and Ridge, on the data matrix. This means that with this function you can eliminate weak variables completely as with the LASSO regularization or contract them to zero as in Ridge.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ElasticNet_HJBiplot(X, Lambda = 1e-04, Alpha = 1e-04, Transform.Data = 'scale')
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>array_like; <br>
A data frame with the information to be analyzed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Lambda</code></td>
<td>
<p>float; <br>
Tuning parameter of the LASSO penalty. Higher values lead to sparser components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Alpha</code></td>
<td>
<p>float; <br>
Tuning parameter of the Ridge shrinkage</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Transform.Data</code></td>
<td>
<p>character; <br>
A value indicating whether the columns of X (variables) should be centered or scaled. Options are: "center" that removes the columns means and "scale" that removes the columns means and divide by its standard deviation. Default is "scale".</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Algorithm used to perform automatic selection of variables and continuous contraction simultaneously. With this method, the model obtained is simpler and more interpretable. It is a particularly useful method when the number of variables is much greater than the number of observations.
</p>


<h3>Value</h3>

<p><code>ElasticNet_HJBiplot</code> returns a list containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>loadings</code></td>
<td>
<p>  array_like; <br>
penalized loadings, the loadings of the sparse principal components.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_ceros</code></td>
<td>
<p>  array_like; <br>
number of loadings equal to cero in each component.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coord_ind</code></td>
<td>
<p>  array_like; <br>
matrix with the coordinates of individuals.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>coord_var</code></td>
<td>
<p>  array_like; <br>
matrix with the coordinates of variables.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eigenvalues</code></td>
<td>
<p>  array_like; <br>
vector with the eigenvalues penalized.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>explvar</code></td>
<td>
<p>  array_like; <br>
an vector containing the proportion of variance explained by the first 1, 2,.,k sparse principal components obtained.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Mitzi Cubilla-Montilla, Carlos Torres-Cubilla, Ana Belen Nieto Librero and Purificacion Galindo Villardon
</p>


<h3>References</h3>


<ul>
<li>
<p> Galindo, M. P. (1986). Una alternativa de representacion simultanea: HJ-Biplot. Questiio, 10(1), 13-23.
</p>
</li>
<li>
<p> Erichson, N. B., Zheng, P., Manohar, K., Brunton, S. L., Kutz, J. N., &amp; Aravkin, A. Y. (2018). Sparse principal component analysis via variable projection. arXiv preprint arXiv:1804.00341.
</p>
</li>
<li>
<p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the royal statistical society: series B (statistical methodology), 67(2), 301-320.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>spca</code>, <code>Plot_Biplot</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"> ElasticNet_HJBiplot(mtcars, Lambda = 0.2, Alpha = 0.1)

</code></pre>


</div>