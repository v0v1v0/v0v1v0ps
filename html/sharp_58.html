<div class="container">

<table style="width: 100%;"><tr>
<td>PenalisedRegression</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Penalised regression</h2>

<h3>Description</h3>

<p>Runs penalised regression using implementation from
<code>glmnet</code>. This function is not using stability.
</p>


<h3>Usage</h3>

<pre><code class="language-R">PenalisedRegression(
  xdata,
  ydata,
  Lambda = NULL,
  family,
  penalisation = c("classic", "randomised", "adaptive"),
  gamma = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xdata</code></td>
<td>
<p>matrix of predictors with observations as rows and variables as
columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ydata</code></td>
<td>
<p>optional vector or matrix of outcome(s). If <code>family</code> is set
to <code>"binomial"</code> or <code>"multinomial"</code>, <code>ydata</code> can be a vector
with character/numeric values or a factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Lambda</code></td>
<td>
<p>matrix of parameters controlling the level of sparsity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>type of regression model. This argument is defined as in
<code>glmnet</code>. Possible values include <code>"gaussian"</code>
(linear regression), <code>"binomial"</code> (logistic regression),
<code>"multinomial"</code> (multinomial regression), and <code>"cox"</code> (survival
analysis).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalisation</code></td>
<td>
<p>type of penalisation to use. If
<code>penalisation="classic"</code> (the default), penalised regression is done
with the same regularisation parameter, or using <code>penalty.factor</code>, if
specified. If <code>penalisation="randomised"</code>, the regularisation for each
of the variables is uniformly chosen between <code>lambda</code> and
<code>lambda/gamma</code>. If <code>penalisation="adaptive"</code>, the regularisation
for each of the variables is weighted by <code>1/abs(beta)^gamma</code> where
<code>beta</code> is the regression coefficient obtained from unpenalised
regression.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>parameter for randomised or adaptive regularisation. Default is
<code>gamma=0.5</code> for randomised regularisation and <code>gamma=2</code> for
adaptive regularisation. The parameter <code>gamma</code> should be between
<code>0</code> and <code>1</code> for randomised regularisation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters passed to <code>glmnet</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with: </p>
<table>
<tr style="vertical-align: top;">
<td><code>selected</code></td>
<td>
<p>matrix of binary selection status. Rows
correspond to different model parameters. Columns correspond to
predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_full</code></td>
<td>
<p>array of model coefficients. Rows correspond
to different model parameters. Columns correspond to predictors. Indices
along the third dimension correspond to outcome variable(s).</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Zou H (2006).
“The adaptive lasso and its oracle properties.”
<em>Journal of the American statistical association</em>, <b>101</b>(476), 1418–1429.
</p>
<p>Tibshirani R (1996).
“Regression Shrinkage and Selection via the Lasso.”
<em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <b>58</b>(1), 267–288.
ISSN 00359246, <a href="http://www.jstor.org/stable/2346178">http://www.jstor.org/stable/2346178</a>.
</p>


<h3>See Also</h3>

<p><code>SelectionAlgo</code>, <code>VariableSelection</code>
</p>
<p>Other underlying algorithm functions: 
<code>CART()</code>,
<code>ClusteringAlgo()</code>,
<code>PenalisedGraphical()</code>,
<code>PenalisedOpenMx()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Data simulation
set.seed(1)
simul &lt;- SimulateRegression(pk = 50)

# Running the LASSO
mylasso &lt;- PenalisedRegression(
  xdata = simul$xdata, ydata = simul$ydata,
  Lambda = c(0.1, 0.2), family = "gaussian"
)

# Using glmnet arguments
mylasso &lt;- PenalisedRegression(
  xdata = simul$xdata, ydata = simul$ydata,
  Lambda = c(0.1), family = "gaussian",
  penalty.factor = c(rep(0, 10), rep(1, 40))
)
mylasso$beta_full
</code></pre>


</div>