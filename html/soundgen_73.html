<div class="container">

<table style="width: 100%;"><tr>
<td>findVoicedSegments</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find voiced segments</h2>

<h3>Description</h3>

<p>Internal soundgen function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">findVoicedSegments(
  pitchCands,
  shortestSyl,
  shortestPause,
  step,
  samplingRate,
  minVoicedCands,
  pitchMethods,
  manualV = NULL,
  manualTryToV = NULL,
  manualUnv = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pitchCands</code></td>
<td>
<p>matrix of possible pitch values per column. One column is
one fft frame, one row is one pitch candidate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shortestSyl</code></td>
<td>
<p>the smallest length of a voiced segment (ms) that
constitutes a voiced syllable (shorter segments will be replaced by NA, as
if unvoiced)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shortestPause</code></td>
<td>
<p>the smallest gap between voiced syllables (ms): large
value = interpolate and merge, small value = treat as separate syllables
separated by an unvoiced gap</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step</code></td>
<td>
<p>you can override <code>overlap</code> by specifying FFT step, ms (NB:
because digital audio is sampled at discrete time intervals of
1/samplingRate, the actual step and thus the time stamps of STFT frames
may be slightly different, eg 24.98866 instead of 25.0 ms)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>samplingRate</code></td>
<td>
<p>sampling rate (Hz)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minVoicedCands</code></td>
<td>
<p>a frame is considered to be voiced if at least this
many pitch candidates are not NA. Defaults to 2: since dom is usually
defined, in practice this means that we also want at least one other pitch
candidate (autocor, cep or BaNa)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pitchMethods</code></td>
<td>
<p>methods of pitch tracking in analyze()</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>manualV</code></td>
<td>
<p>index of frames that should definitely be voiced (manual
candidates)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>manualTryToV</code></td>
<td>
<p>index of frames that should be treated as voiced as long
as they have any candidates at all (even &lt;minVoicedCands)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>manualUnv</code></td>
<td>
<p>index of frames forced to be unvoiced</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Internal helper function for postprocessing of pitch contours. Merges voiced
segments at least <code>shortestSyl</code> ms long and separated by less than
<code>shortestPause</code> ms. Called by <code>analyze</code>
</p>


<h3>Value</h3>

<p>Returns a dataframe specifying where each voiced segment starts and
ends (in fft frames, not ms!)
</p>


</div>