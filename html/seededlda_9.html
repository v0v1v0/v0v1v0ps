<div class="container">

<table style="width: 100%;"><tr>
<td>textmodel_seededlda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Semisupervised Latent Dirichlet allocation</h2>

<h3>Description</h3>

<p>Implements semisupervised Latent Dirichlet allocation
(Seeded LDA). <code>textmodel_seededlda()</code> allows users to specify
topics using a seed word dictionary. Users can run Seeded Sequential LDA by
setting <code>gamma &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textmodel_seededlda(
  x,
  dictionary,
  levels = 1,
  valuetype = c("glob", "regex", "fixed"),
  case_insensitive = TRUE,
  residual = 0,
  weight = 0.01,
  max_iter = 2000,
  auto_iter = FALSE,
  alpha = 0.5,
  beta = 0.1,
  gamma = 0,
  adjust_alpha = 0,
  batch_size = 1,
  ...,
  verbose = quanteda_options("verbose")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the dfm on which the model will be fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dictionary</code></td>
<td>
<p>a <code>quanteda::dictionary()</code> with seed words that define
topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>levels</code></td>
<td>
<p>levels of entities in a hierarchical dictionary to be used as
seed words. See also quanteda::flatten_dictionary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>valuetype</code></td>
<td>
<p>see quanteda::valuetype</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>case_insensitive</code></td>
<td>
<p>see quanteda::valuetype</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residual</code></td>
<td>
<p>the number of undefined topics. They are named "other" by
default, but it can be changed via <code>base::options(seededlda_residual_name)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>determines the size of pseudo counts given to matched seed words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>the maximum number of iteration in Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auto_iter</code></td>
<td>
<p>if <code>TRUE</code>, stops Gibbs sampling on convergence before
reaching <code>max_iter</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the values to smooth topic-document distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the values to smooth topic-word distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>a parameter to determine change of topics between sentences or
paragraphs. When <code>gamma &gt; 0</code>, Gibbs sampling of topics for the current
document is affected by the previous document's topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjust_alpha</code></td>
<td>
<p>[experimental] if <code>adjust_alpha &gt; 0</code>, automatically adjust
<code>alpha</code> by the size of the topics. The smallest value of adjusted <code>alpha</code>
will be <code>alpha * (1 - adjust_alpha)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>split the corpus into the smaller batches (specified in
proportion) for distributed computing; it is disabled when a batch include
all the documents <code>batch_size = 1.0</code>. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>passed to quanteda::dfm_trim to restrict seed words based on
their term or document frequency. This is useful when glob patterns in the
dictionary match too many words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; if <code>TRUE</code> print diagnostic information during
fitting.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The same as <code>textmodel_lda()</code> with extra elements for <code>dictionary</code>.
</p>


<h3>References</h3>

<p>Lu, Bin et al. (2011). "Multi-aspect Sentiment Analysis with
Topic Models". doi:10.5555/2117693.2119585. <em>Proceedings of the 2011 IEEE
11th International Conference on Data Mining Workshops</em>.
</p>
<p>Watanabe, Kohei &amp; Zhou, Yuan. (2020). "Theory-Driven Analysis of Large
Corpora: Semisupervised Topic Classification of the UN Speeches".
doi:10.1177/0894439320907027. <em>Social Science Computer Review</em>.
</p>
<p>Watanabe, Kohei &amp; Baturo, Alexander. (2023). "Seeded Sequential LDA:
A Semi-supervised Algorithm for Topic-specific Analysis of Sentences".
doi:10.1177/08944393231178605. <em>Social Science Computer Review</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
require(seededlda)
require(quanteda)

corp &lt;- head(data_corpus_moviereviews, 500)
toks &lt;- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE, remove_number = TRUE)
dfmt &lt;- dfm(toks) %&gt;%
    dfm_remove(stopwords("en"), min_nchar = 2) %&gt;%
    dfm_trim(max_docfreq = 0.1, docfreq_type = "prop")

dict &lt;- dictionary(list(people = c("family", "couple", "kids"),
                        space = c("alien", "planet", "space"),
                        moster = c("monster*", "ghost*", "zombie*"),
                        war = c("war", "soldier*", "tanks"),
                        crime = c("crime*", "murder", "killer")))
lda_seed &lt;- textmodel_seededlda(dfmt, dict, residual = TRUE, min_termfreq = 10,
                                max_iter = 500)
terms(lda_seed)
topics(lda_seed)

</code></pre>


</div>