<div class="container">

<table style="width: 100%;"><tr>
<td>get_pycox_activation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Get Pytorch Activation Function</h2>

<h3>Description</h3>

<p>Helper function to return a class or constructed object for
pytorch activation function from <code>torch.nn.modules.activation</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">get_pycox_activation(
  activation = "relu",
  construct = TRUE,
  alpha = 1,
  dim = NULL,
  lambd = 0.5,
  min_val = -1,
  max_val = 1,
  negative_slope = 0.01,
  num_parameters = 1L,
  init = 0.25,
  lower = 1/8,
  upper = 1/3,
  beta = 1,
  threshold = 20,
  value = 20
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p><code>(character(1))</code><br> Activation function method, see
details for list of implemented methods.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>construct</code></td>
<td>
<p><code>(logical(1))</code><br> If <code>TRUE</code> (default) returns constructed
object, otherwise a class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>celu</code> and <code>elu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p><code>(integer(1))</code><br> Passed to <code>glu</code>, <code>logsoftmax</code>, <code>softmax</code>, and</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambd</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>hardshrink</code> and <code>softshrink</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_val, max_val</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>hardtanh</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative_slope</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>leakyrelu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_parameters</code></td>
<td>
<p><code>(integer(1))</code><br> Passed to <code>prelu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>prelu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower, upper</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>rrelu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>softplus</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>softplus</code> and <code>threshold</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>value</code></td>
<td>
<p><code>(numeric(1))</code><br> Passed to <code>threshold</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Implemented methods (with help pages) are
</p>

<ul>
<li> <p><code>"celu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$CELU)</code>
</p>
</li>
<li> <p><code>"elu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$ELU)</code>
</p>
</li>
<li> <p><code>"gelu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$GELU)</code>
</p>
</li>
<li> <p><code>"glu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$GLU)</code>
</p>
</li>
<li> <p><code>"hardshrink"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Hardshrink)</code>
</p>
</li>
<li> <p><code>"hardsigmoid"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Hardsigmoid)</code>
</p>
</li>
<li> <p><code>"hardswish"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Hardswish)</code>
</p>
</li>
<li> <p><code>"hardtanh"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Hardtanh)</code>
</p>
</li>
<li> <p><code>"relu6"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$ReLU6)</code>
</p>
</li>
<li> <p><code>"leakyrelu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$LeakyReLU)</code>
</p>
</li>
<li> <p><code>"logsigmoid"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$LogSigmoid)</code>
</p>
</li>
<li> <p><code>"logsoftmax"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$LogSoftmax)</code>
</p>
</li>
<li> <p><code>"prelu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$PReLU)</code>
</p>
</li>
<li> <p><code>"rrelu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$RReLU)</code>
</p>
</li>
<li> <p><code>"relu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$ReLU)</code>
</p>
</li>
<li> <p><code>"selu"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$SELU)</code>
</p>
</li>
<li> <p><code>"sigmoid"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Sigmoid)</code>
</p>
</li>
<li> <p><code>"softmax"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softmax)</code>
</p>
</li>
<li> <p><code>"softmax2d"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softmax2d)</code>
</p>
</li>
<li> <p><code>"softmin"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softmin)</code>
</p>
</li>
<li> <p><code>"softplus"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softplus)</code>
</p>
</li>
<li> <p><code>"softshrink"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softshrink)</code>
</p>
</li>
<li> <p><code>"softsign"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Softsign)</code>
</p>
</li>
<li> <p><code>"tanh"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Tanh)</code>
</p>
</li>
<li> <p><code>"tanhshrink"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Tanhshrink)</code>
</p>
</li>
<li> <p><code>"threshold"</code> <br><code>reticulate::py_help(torch$nn$modules$activation$Threshold)</code>
</p>
</li>
</ul>
<h3>Value</h3>

<p>No return value.
</p>


</div>