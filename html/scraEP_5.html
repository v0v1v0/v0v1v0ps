<div class="container">

<table style="width: 100%;"><tr>
<td>xscrape</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Extract information from webpages to a data.frame, using XPath or CSS queries.
</h2>

<h3>Description</h3>

<p>This function transforms an html/xml page (or list of pages) into a data.frame, extracting nodes specified by their XPath.
</p>


<h3>Usage</h3>

<pre><code class="language-R">xscrape(pages, 
        col.xpath = ".", row.xpath = "/html", 
        col.css = NULL, row.css = NULL, 
        collapse = " | ", encoding = NULL, 
        page.name = TRUE, nice.text = TRUE, 
        parallel = 0, 
        engine = c("auto", "XML", "xml2"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pages</code></td>
<td>
<p>an object of class <code>XMLInternalDocument</code> or <code>xml_document</code> (as returned by functions <code>XML::htmlParse</code> or <code>xml2::read_html</code> or <code>rvest::read_html</code>), or list of such objects. Alternatively, a character vector containing the URLs or local paths of webpages to be parsed. These are the webpages that information is to be extracted from. If the provided list or vector is named, its names will be used to indicate data provenance when <code>page.name</code> is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col.xpath</code></td>
<td>
<p>a character vector of XPath queries used for creating the result columns. If the vector is named, these names are given to the columns. The default "." takes the text from the whole of each page or intermediary node (specified by <code>row.xpath</code> or <code>row.css</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.xpath</code></td>
<td>
<p>a character string, containing an XPath query for creating the result rows. The result of this query (on each page) becomes a row in the resulting data.frame. If not specified (default), the intermediary nodes are whole html pages, so that each page becomes a row in the result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col.css</code></td>
<td>
<p>same as <code>col.xpath</code>, but with CSS selectors instead of XPath queries. If <code>col.xpath</code> was also given, the XPath columns will be placed before the CSS columns in the result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.css</code></td>
<td>
<p>same as <code>row.xpath</code>, but with a CSS selector instead of an XPath query. If given, this will be used instead of <code>row.xpath</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>collapse</code></td>
<td>
<p>a character string, containing the separator that will be used in case a <code>col.xpath</code> query yields multiple results within a given intermediary node. The default is " | ".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encoding</code></td>
<td>
<p> a character string (eg. "UTF-8" or "ISO-8859-1"), containing the encoding parameter that will be used by <code>htmlParse</code> or <code>read_html</code> if <code>pages</code> is a vector of URLs or local file names.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>page.name</code></td>
<td>
<p>a logical. If TRUE, the result will contain a column indicating the name of the page each row was extracted from. If <code>pages</code> has no names, they will be numbered from 1 to <code>length(pages)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nice.text</code></td>
<td>
<p>a logical. If TRUE (only possible with engine xml2), the rvest::html_text2 function is used to extract text into the result, often making the text much cleaner. If FALSE, the function runs faster, but the text might be less clean.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>a numeric, indicating the number of cores to use for parallel computation. The default 0 takes all available cores. The parallelization is done on the pages if their number is greater than the number of provided cores, otherwise it is done on the intermediary nodes. Note that parallelization relies on parallel::mclapply, and is thus not supported on Windows systems.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>engine</code></td>
<td>
<p>a character string, indicating the engine to use for data extraction: either "XML", "xml2", or "auto" (default). The default will adapt the engine to the type of <code>pages</code>, or will use "xml2" if <code>pages</code> are URLs or file names. Note: CSS selectors and <code>nice.text</code> are only available for "xml2", but XPath queries and "XML" engine tend to be much faster.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If a <code>col.xpath</code> or <code>col.css</code> query designs a full node, only its text is extracted. If it designs an attribute (eg. ends with '/@href' for weblinks), only the attribute's value is extracted.
</p>
<p>If a <code>col.xpath</code> or <code>col.css</code> query matches no elements in a page, returned value is <code>NA</code>. If it matches multiple elements, they are concatenated into a single character string, separated by <code>collapse</code>.
</p>


<h3>Value</h3>

<p>A data.frame, where each row corresponds to an intermediary node (either a full page or an XML node within a page, specified by <code>row.xpath</code> or <code>row.css</code>), and each column corresponds to the text of a <code>col.xpath</code> or <code>col.css</code> query.
</p>


<h3>Author(s)</h3>

<p>Julien Boelaert <a href="mailto:jubo.stats@gmail.com">jubo.stats@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Extract all external links and their titles from a wikipedia page
data(wiki)
wiki.parse &lt;- XML::htmlParse(wiki)
links &lt;- xscrape(wiki.parse, 
                 row.xpath= "//a[starts-with(./@href, 'http')]", 
                 col.xpath= c(title= ".", link= "./@href"), 
                 parallel = 1)

## Not run: 
## Convert results from a search for 'R' on duckduckgo.com
## First download the search page
duck &lt;- XML::htmlParse("http://duckduckgo.com/html/?q=R")
## Then run xscrape on the dowloaded and parsed page
results &lt;- xscrape(duck, 
                   row.xpath= "//div[contains(@class, 'result__body')]",
                   col.xpath= c(title= "./h2", 
                                snippet= ".//*[@class='result__snippet']", 
                                url= ".//a[@class='result__url']/@href"))

## End(Not run)

## Not run: 
## Convert results from a search for 'R' and 'Julia' on duckduckgo.com
## Directly provide the URLs to xscrape
results &lt;- xscrape(c("http://duckduckgo.com/html/?q=R", 
                     "http://duckduckgo.com/html/?q=julia"), 
                   row.xpath= "//div[contains(@class, 'result__body')]",
                   col.xpath= c(title= "./h2", 
                                snippet= ".//*[@class='result__snippet']", 
                                url= ".//a[@class='result__url']/@href"))

## End(Not run)

</code></pre>


</div>