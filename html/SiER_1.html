<div class="container">

<table style="width: 100%;"><tr>
<td>cv.SiER</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross-validation for high-dimensional multivariate regression

</h2>

<h3>Description</h3>

<p>Conduct the cross-validation and build the final
model for the following high dimensional multivariate regression model:
</p>
<p style="text-align: center;"><code class="reqn">Y= \mu+X\beta+\epsilon,</code>
</p>

<p>where <code class="reqn">Y</code> is the <code class="reqn">n\times q</code> response matrix with <code class="reqn">q\ge  1</code>,
<code class="reqn">X</code> is the <code class="reqn">n\times p</code> predictor matrix, and
<code class="reqn">\epsilon</code> is the noise matrix.  The coefficient matrix
<code class="reqn">\beta</code> is <code class="reqn">p\times q</code> and  <code class="reqn">\mu</code> is the
intercept.  The number of predictors <code class="reqn">p</code> can be much larger than the
sample size <code class="reqn">n</code>. The response is univariate if
<code class="reqn">q=1</code> and  multivariate if <code class="reqn">q&gt;1</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cv.SiER(X, Y, K.cv = 5, upper.comp = 10, thresh = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>the <code class="reqn">n\times p</code> predictor matrix.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>the <code class="reqn">n\times q</code> response matrix, where <code class="reqn">q\ge 1</code> is
the number of response variables.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K.cv</code></td>
<td>
<p> the number of CV sets. Default is 5.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper.comp</code></td>
<td>
<p>the upper bound for the maximum number of components to be calculated. Default is 10.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thresh</code></td>
<td>
<p>a number between 0 and 1 specifying the minimum proportion of variation to be explained by each selected component relative to all the selected components. It is used to determine the maximum number of components to be calculated in the CV procedure. The optimal number of components will be selected from the integers from 1 to the minimum of upper.comp and this maximum number. A smaller thresh leads to a larger maximum number of components and a longer running time. A larger thresh value leads to a smaller running time, but may miss some important components and lead to a larger prediction error. Default is 0.01.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Based on the best rank <code class="reqn">K</code> approximation to <code class="reqn">X\beta</code>, the  coefficient matrix has decomposition  <code class="reqn">\beta=\sum \alpha_k w_k    ^T</code>, where <code class="reqn">\alpha_k</code> is  the vector so that <code class="reqn">X\alpha_k</code> has the maximum correlation with  <code class="reqn">Y</code> under the restriction that  <code class="reqn">X\alpha_k</code> has unit variance and is uncorrelated  with   <code class="reqn">X\alpha_1</code>,..., <code class="reqn">X\alpha_{k-1}</code>. We estimate  <code class="reqn">\alpha_k</code> by solving a penalized generalized eigenvalue problem  with penalty <code class="reqn">\tau||\alpha_k||_{\lambda}^2</code> where  <code class="reqn">||\alpha_k||_{\lambda}^2=(1-\lambda)||\alpha_k||_2^2+\lambda||\alpha_k||_1^2</code>  is a mixture of the squared <code class="reqn">l_2</code> and squared <code class="reqn">l_1</code> norms. The <code class="reqn">w_k</code> is  estimated by regressing <code class="reqn">Y</code> on <code class="reqn">X\alpha_k</code>.
</p>


<h3>Value</h3>

<p>A fitted CV-object, which is used in the function <code>pred.SiER</code>() for prediction and <code>getcoef.SiER</code>() for extracting the estimated coefficient matrix.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>the estimated intercept vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the estimated slope coefficient matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min.error</code></td>
<td>
<p>minimum CV error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale.x</code></td>
<td>
<p>the maximum absolute value of X used to scale X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>the input X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>the input Y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params.set</code></td>
<td>
<p>a 9*2 matrix specifying the set of values of <code class="reqn">tau</code> and <code class="reqn">lambda</code> used in CV.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>error</code></td>
<td>
<p>a list for CV errors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt.K</code></td>
<td>
<p>optimal number of components to be selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt.tau</code></td>
<td>
<p>optimal value for <code class="reqn">tau</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt.lambda</code></td>
<td>
<p>optimal value for <code class="reqn">lambda</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Ruiyan Luo and Xin Qi

</p>


<h3>References</h3>

<p>Ruiyan Luo and Xin Qi (2017) Signal extraction approach for
sparse multivariate response regression, Journal of Multivariate
Statistics. 153: 83-97.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># q=1
library(MASS)
nvar=100
nvarq &lt;- 1
sigmaY &lt;- 0.1
sigmaX=0.1
nvar.eff=15
rho &lt;- 0.3
Sigma=matrix(0,nvar.eff,nvar.eff)
for(i in 1:nvar.eff){
    for(j in 1:nvar.eff){
        Sigma[i,j]=rho^(abs(i-j))
    }
}

betas.true &lt;- matrix(0, nvar, 1)
betas.true[1:15,1]=rep(1,15)/sqrt(15)

ntest &lt;- 100
ntrain &lt;- 90
ntot &lt;- ntest+ntrain
X &lt;- matrix(0,ntot,nvar)
X[,1:nvar.eff] &lt;- mvrnorm(n=ntot, rep(0, nvar.eff), Sigma)
X[,-(1:nvar.eff)] &lt;- matrix(sigmaX*rnorm((nvar-nvar.eff)*dim(X)[1]),
                            dim(X)[1],(nvar-nvar.eff))
Y &lt;- X%*%betas.true
Y &lt;- Y+rnorm(ntot, 0, sigmaY)

X.train &lt;- X[1:ntrain,]
Y.train &lt;- Y[1:ntrain,]
X.test &lt;- X[-(1:ntrain),]
Y.test &lt;- Y[-(1:ntrain),]

cv.fit &lt;- cv.SiER(X.train,Y.train, K.cv=5)

Y.pred=pred.SiER(cv.fit, X.test)
error=sum((Y.pred-Y.test)^2)/ntest
print(c("predict error=", error))
coefs=getcoef.SiER(cv.fit)


#q&gt;1
library(MASS)
total.noise &lt;- 0.1
rho &lt;- 0.3
rho.e &lt;- 0.2
nvar=500
nvarq &lt;- 3
sigma2 &lt;- total.noise/nvarq
sigmaX=0.1
nvar.eff=150

Sigma=matrix(0,nvar.eff,nvar.eff)
for(i in 1:nvar.eff){
    for(j in 1:nvar.eff){
        Sigma[i,j]=rho^(abs(i-j))
    }
}
Sigma2.y &lt;- matrix(sigma2*rho.e,nvarq, nvarq)
diag(Sigma2.y) &lt;- sigma2

betas.true &lt;- matrix(0, nvar, 3)
betas.true[1:15,1]=rep(1,15)/sqrt(15)
betas.true[16:45,2]=rep(0.5,30)/sqrt(30)
betas.true[46:105,3]=rep(0.25,60)/sqrt(60)

ntest &lt;- 500
ntrain &lt;- 90
ntot &lt;- ntest+ntrain
X &lt;- matrix(0,ntot,nvar)
X[,1:nvar.eff] &lt;- mvrnorm(n=ntot, rep(0, nvar.eff), Sigma)
X[,-(1:nvar.eff)] &lt;- matrix(sigmaX*rnorm((nvar-nvar.eff)*dim(X)[1]),
                           dim(X)[1],(nvar-nvar.eff))
Y &lt;- X%*%betas.true
Y &lt;- Y+mvrnorm(n=ntot, rep(0,nvarq), Sigma2.y)

X.train &lt;- X[1:ntrain,]
Y.train &lt;- Y[1:ntrain,]
X.test &lt;- X[-(1:ntrain),]
Y.test &lt;- Y[-(1:ntrain),]

cv.fit &lt;- cv.SiER(X.train,Y.train, K.cv=5)

Y.pred=pred.SiER(cv.fit, X.test)
error=sum((Y.pred-Y.test)^2)/ntest
print(c("predict error=", error))

</code></pre>


</div>