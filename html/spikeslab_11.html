<div class="container">

<table style="width: 100%;"><tr>
<td>spikeslab</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Spike and Slab Regression</h2>

<h3>Description</h3>

  
<p>Fits a rescaled spike and slab model using a continuous bimodal prior.
A generalized elastic net estimator is used for variable selection and
estimation.  Can be used for prediction and variable selection in low-
and high-dimensional linear regression models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spikeslab(formula, data = NULL, x = NULL, y = NULL,
    n.iter1 = 500, n.iter2 = 500, mse = TRUE,
    bigp.smalln = FALSE, bigp.smalln.factor = 1, screen = (bigp.smalln),
    r.effects = NULL, max.var = 500, center = TRUE, intercept = TRUE,
    fast = TRUE, beta.blocks = 5, verbose = FALSE, ntree = 300,
    seed = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>A symbolic description of the model to be fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Data frame containing the data used in the formula.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>x predictor matrix (can be used in place of formula and
data frame call).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>y response (can be used in place of formula and data frame call).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.iter1</code></td>
<td>
<p>Number of burn-in Gibbs sampled values (i.e., discarded values).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.iter2</code></td>
<td>
<p>Number of Gibbs sampled values, following burn-in.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mse</code></td>
<td>
<p>If TRUE, an external estimate for the
overall variance is calculated using ridge regression or random
forests (the latter is used when the degrees of freedom are low).
Otherwise, the variance is included in the prior and estimated
using Gibbs sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bigp.smalln</code></td>
<td>
<p>Use if <code>p</code> &gt;&gt; <code>n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bigp.smalln.factor</code></td>
<td>
<p>Removes all variables except the top
<code>n</code>times <code>bigp.smalln.factor</code> ones (used in filtering when 
<code>p</code> &gt;&gt; <code>n</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screen</code></td>
<td>
<p>If TRUE, variables are pre-filtered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r.effects</code></td>
<td>
<p>List used for grouping variables (see details below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.var</code></td>
<td>
<p>Maximum number of variables allowed in the final model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>If TRUE, variables are centered by their
means. Default is TRUE and should only be adjusted in extreme examples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>If TRUE, an intercept is included in the model,
otherwise no intercept is included.  Default is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fast</code></td>
<td>
<p>If TRUE, use blocked Gibbs sampling to accelerate the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta.blocks</code></td>
<td>
<p>Update beta using this number of blocks (<code>fast</code>
must be TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If TRUE, verbose output is sent to the terminal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p>Number of trees used by random forests (applies only when <code>mse</code> is TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Seed for random number generator.  Must be a negative
integer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>—&gt; General:
</p>
<p>The spike and slab method is described in detail in Ishwaran and Rao
(2003, 2005a, 2005b and 2009).  For high-dimensional problems in which
<code>p</code> &gt;&gt; <code>n</code>, where <code>p</code> is the number of variables and
<code>n</code> is the sample size, use the option <code>bigp.smalln</code>=TRUE.
Doing so implements a three-stage procedure:
</p>
<p>(1) Filtering step.  This removes all variables except the top
<code>n</code> times <code>bigp.smalln.factor</code> ones.  Uses spike and slab
regression with grouped regularization (complexity) parameters.
</p>
<p>(2) Model averaging step.  Refit the model using only those
predictors from step 1.  Returns the posterior mean values from
fitting a spike and slab model; referred to as the Bayesian model
averaged (bma) estimate.
</p>
<p>(3) Variable selection step.  Select variables using the generalized
elastic net (gnet).
</p>
<p>The filtering step is omitted when <code>bigp.smalln</code>=FALSE.
Filtering can however be requested by setting <code>screen</code>=TRUE
although users should be aware that this may degrade performance and
should only be used when <code>p</code> is on the same order of <code>n</code>.
</p>
<p>Variables can be grouped using <code>r.effects</code>.  Grouping has the
effect of forcing variables within a given group to share a common
complexity (regularization) parameter.  To do so, define a list with
each entry in the list made up of the variable names to be grouped.
There is no limit to the number of groups.  Any variable that does
not appear in the list will be assigned to a default group (the
default group also has its own group-specific regularization
parameter).  See Examples 1 and 3 below.
</p>
<p>—&gt; Miscellanea:
</p>
<p>By default, <code>fast</code>=TRUE when <code>bigp.smalln</code>=TRUE.  This
invokes an ultra-fast filtering step.  Setting <code>fast</code>=FALSE
invokes a more thorough filtering method that may slightly improve
inferential results, but computational times will become very slow.
The trade-off is unlikely to be justified.
</p>
<p>The formula and data-frame call should be avoided in high-dimensional
problems and instead the x-predictor matrix and y response vector
should be passed directly (see Example 3).  This avoids the huge
overhead in parsing formula in R.
</p>
<p>By default, predictors are normalized to have mean 0 and variance 1.
Pre-processing also involves centering y unless the user specifically
requests that the intercept be excluded from the model.  Users can
also over-ride centering predictors by setting <code>center</code>=FALSE.
Use with extreme care.
</p>
<p>The <code>verbose</code> option sends output to the terminal showing the
number of Gibbs iterations and the current complexity (regularization)
parameter(s).
</p>
<p>Depends on the <code>randomForest</code> package for estimating the variance
when <code>mse</code>=TRUE.  Note that <code>mse</code> is over-ridden and set to
FALSE when <code>bigp.smalln</code>=TRUE.
</p>
<p>Depends on the <code>lars</code> package for the variable slection step.
</p>


<h3>Value</h3>

<p>An object of class <code>spikeslab</code> with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>summary</code></td>
<td>
<p>Summary object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Verbose details (used for printing).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>terms</code></td>
<td>
<p>Terms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma.hat</code></td>
<td>
<p>Estimated variance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Original y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xnew</code></td>
<td>
<p>Centered, rescaled x-matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Original x-matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.center</code></td>
<td>
<p>Centering for original y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.center</code></td>
<td>
<p>Centering for original x-matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.scale</code></td>
<td>
<p>Scaling for original x-matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>names</code></td>
<td>
<p>Variable names.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bma</code></td>
<td>
<p>bma coefficients in terms of xnew.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bma.scale</code></td>
<td>
<p>bma coefficients rescaled in terms of original x.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet</code></td>
<td>
<p>gnet coefficients in terms of xnew.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet.scale</code></td>
<td>
<p>gnet coefficients rescaled in terms of original x.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet.path</code></td>
<td>
<p>gnet path scaled in terms of the original x.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet.obj</code></td>
<td>
<p>gnet object (a lars-type object).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet.obj.vars</code></td>
<td>
<p>Variables (in order) used to calculate the gnet object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gnet.parms</code></td>
<td>
<p>Generalized ridge regression parameters used to define the gnet.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phat</code></td>
<td>
<p>Estimated model dimension.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>complexity</code></td>
<td>
<p>Complexity (regularization) parameter estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ridge</code></td>
<td>
<p>List containing ridge values used to determine the bma.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>models</code></td>
<td>
<p>List containing the models sampled.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Hemant Ishwaran (<a href="mailto:hemant.ishwaran@gmail.com">hemant.ishwaran@gmail.com</a>)
</p>
<p>J. Sunil Rao (<a href="mailto:rao.jsunil@gmail.com">rao.jsunil@gmail.com</a>)
</p>
<p>Udaya B. Kogalur (<a href="mailto:ubk@kogalur.com">ubk@kogalur.com</a>)
</p>


<h3>References</h3>

<p>Breiman L. (2001). Random forests, <em>Machine Learning</em>, 45:5-32.
</p>
<p>Efron B., Hastie T., Johnstone I. and Tibshirani R. (2004).
Least angle regression (with discussion). <em>Ann. Statist.</em>,
32:407-499.
</p>
<p>Ishwaran H. and Rao J.S. (2003).  Detecting differentially expressed
genes in microarrays using Bayesian model selection.
<em>J. Amer. Stat. Assoc.</em>, 98:438-455.
</p>
<p>Ishwaran H. and Rao J.S. (2005a).  Spike and slab variable selection:
frequentist and Bayesian strategies.  <em>Ann. Statist.</em>,
33:730-773.
</p>
<p>Ishwaran H. and Rao J.S. (2005b).  Spike and slab gene selection for
multigroup microarray data. <em>J. Amer. Stat. Assoc.</em>, 100:764-780.
</p>
<p>Ishwaran H. and Rao J.S. (2010).  Generalized ridge regression:
geometry and computational solutions when p is larger than n.
</p>
<p>Ishwaran H., Kogalur U.B. and Rao J.S. (2010). spikeslab: prediction
and variable selection using spike and slab regression. <em>R Journal</em>,
2(2), 68-73.
</p>
<p>Ishwaran H. and Rao J.S. (2011).  Mixing generalized ridge
regressions.
</p>
<p>Zou H. and Hastie T. (2005).  Regularization and variable selection
via the elastic net.  <em>J. Royal Statist. Society B</em>,
67(2):301-320.
</p>


<h3>See Also</h3>

<p><code>cv.spikeslab</code>,
<code>plot.spikeslab</code>,
<code>predict.spikeslab</code>,
<code>print.spikeslab</code>,
<code>sparsePC.spikeslab</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#------------------------------------------------------------
# Example 1:  diabetes data
#------------------------------------------------------------

# basic call
data(diabetesI, package = "spikeslab")
obj &lt;- spikeslab(Y ~ . , diabetesI, verbose=TRUE)
print(obj)
plot(obj)

# grouping effect
# separate main effects and interactions into two groups
# use a group-specific regularization parameter for each group
xnames &lt;- names(diabetesI[, -1])
r.eff &lt;- vector("list", 2)
r.eff[[1]] &lt;- xnames[c(1:10)]
r.eff[[2]] &lt;- xnames[-c(1:10)]
obj2 &lt;- spikeslab(Y ~ . , diabetesI, verbose=TRUE, r.effects=r.eff)
obj2
# extract the regularization parameters
print(apply(obj2$complexity, 2, summary))

## Not run: 
#------------------------------------------------------------
# Example 2: high-dimensional noise (diabetes data)
#------------------------------------------------------------

# add 2000 noise variables
data(diabetesI, package = "spikeslab")
diabetes.noise &lt;- cbind(diabetesI,
      noise = matrix(rnorm(nrow(diabetesI) * 2000), nrow(diabetesI)))

# example of a big p, small n call
# don't use formula call; make call with x and y arguments
x &lt;- diabetes.noise[, -1]
y &lt;- diabetes.noise[, 1]
obj &lt;- spikeslab(x=x, y=y, verbose=TRUE, bigp.smalln=TRUE, max.var=100)
obj

# same example ... but now group variables 
r.eff &lt;- vector("list", 2)
r.eff[[1]] &lt;- names(x)[c(1:100)]
r.eff[[2]] &lt;- names(x)[-c(1:100)]
obj2 &lt;- spikeslab(x=x, y=y, verbose=TRUE, bigp.smalln=TRUE,
                 r.effects=r.eff, max.var=100)
obj2

#------------------------------------------------------------
# Example 3: housing data with interactions
#------------------------------------------------------------

# another example of a big p, small n call
data(housingI, package = "spikeslab")
obj &lt;- spikeslab(medv ~ ., housingI, verbose = TRUE,
           bigp.smalln = TRUE, max.var = 200)
print(obj)



## End(Not run)
</code></pre>


</div>