<div class="container">

<table style="width: 100%;"><tr>
<td>comp.prop</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compares two distributions of the same categorical variable</h2>

<h3>Description</h3>

<p>This function compares two (estimated) distributions of the same categorical variable(s).
</p>


<h3>Usage</h3>

<pre><code class="language-R">comp.prop(p1, p2, n1, n2=NULL, ref=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>p1</code></td>
<td>

<p>A vector or an array containing relative or absolute frequencies for one or more categorical variables.  Usually it is the output of the function <code>xtabs</code> or <code>table</code>. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p2</code></td>
<td>
 
<p>A vector or an array containing relative or absolute frequencies for one or more categorical variables.  Usually it is the output of the function <code>xtabs</code> or <code>table</code>.  If <code>ref = FALSE</code> then <code>p2</code> is a further estimate of the distribution of the categorical variable(s) being considered.  On the contrary (<code>ref = TRUE</code>) it is the 'reference' distribution (the distribution considered true or a reliable estimate).  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n1</code></td>
<td>

<p>The size of the sample on which <code>p1</code> has been estimated.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n2</code></td>
<td>

<p>The size of the sample on which <code>p2</code> has been estimated, required just when <code>ref = FALSE</code> (<code>p2</code> is estimated on another sample and is not the reference distribution).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref</code></td>
<td>

<p>Logical.  When <code>ref = TRUE</code>, <code>p2</code> is the reference distribution (true or reliable estimate of distribution), on the contrary when <code>ref = FALSE</code> it an estimate of the distribution derived from another sample with sample size <code>n2</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function computes some similarity or dissimilarity measures between marginal (joint) distribution of categorical variables(s).
The following measures are considered:
</p>
<p><em>Dissimilarity index</em> or <em>total variation distance</em>: 
</p>
<p style="text-align: center;"><code class="reqn">\Delta_{12} = \frac{1}{2} \sum_{j=1}^J \left| p_{1,j} - p_{2,j}  \right|</code>
</p>

<p>where <code class="reqn">p_{s,j}</code> are the relative frequencies (<code class="reqn">0 \leq p_{s,j} \leq 1</code>). The dissimilarity index ranges from 0 (minimum dissimilarity) to 1.  It can be interpreted as the smallest fraction of units that need to be reclassified in order to make the distributions equal.  When <code>p2</code> is the reference distribution (true or expected distribution under a given hypothesis) than, following the Agresti's rule of thumb (Agresti 2002, pp. 329–330) , values of <code class="reqn">\Delta_{12} \leq 0.03</code> denotes that the estimated distribution <code>p1</code> follows the true or expected pattern quite closely.
</p>
<p><em>Overlap</em> between two distributions:
</p>
<p style="text-align: center;"><code class="reqn">O_{12} = \sum_{j=1}^J min(p_{1,j},p_{2,j})  </code>
</p>

<p>It is a measure of similarity which ranges from 0 to 1 (the distributions are equal).  It is worth noting that <code class="reqn">O_{12}=1-\Delta_{12}</code>.
</p>
<p><em>Bhattacharyya coefficient</em>:
</p>
<p style="text-align: center;"><code class="reqn">B_{12} = \sum_{j=1}^J \sqrt{p_{1,j} \times p_{2,j}}  </code>
</p>

<p>It is a measure of similarity and ranges from 0 to 1 (the distributions are equal).
</p>
<p><em>Hellinger's distance</em>:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,12} = \sqrt{1-B_{12}} </code>
</p>

<p>It is a dissimilarity measure ranging from 0 (distributions are equal) to 1 (max dissimilarity).  It satisfies all the properties of a distance measure (<code class="reqn">0 \leq d_{H,12} \leq 1</code>;  symmetry and triangle inequality).  
Hellinger's distance is related to the dissimilarity index, and it is possible to show that:
</p>
<p style="text-align: center;"><code class="reqn">d_{H,12}^2 \leq  \Delta_{12} \leq d_{H,12}\sqrt{2} </code>
</p>

<p>Alongside with those similarity/dissimilarity measures the Pearson's Chi-squared is computed. Two formulas are considered.  When <code>p2</code> is the reference distribution (true or expected under some hypothesis, <code>ref=TRUE</code>):
</p>
<p style="text-align: center;"><code class="reqn"> \chi^2_P = n_1 \sum_{j=1}^J \frac{\left( p_1,j - p_{2,j}\right)^2}{p_{2,j}}  </code>
</p>

<p>When <code>p2</code> is a distribution estimated on a second sample then:
</p>
<p style="text-align: center;"><code class="reqn"> \chi^2_P = \sum_{i=1}^2 \sum_{j=1}^J n_i \frac{\left( p_{i,j} - p_{+,j}\right)^2}{p_{+,j}}  </code>
</p>

<p>where <code class="reqn">p_{+,j}</code> is the expected frequency for category <em>j</em>, obtained as follows:
</p>
<p style="text-align: center;"><code class="reqn"> p_{+,j} = \frac{n_1 p_{1,j} + n_2 p_{2,j}}{n_1+n_2}  </code>
</p>

<p>being <code class="reqn">n_1</code> and <code class="reqn">n_2</code> the sizes of the samples.
</p>
<p>The Chi-Square value can be used to test the hypothesis that two distributions are equal (<code class="reqn">df=J-1</code>).  Unfortunately such a test would not be useful when the distribution are estimated from samples selected from a finite population using complex selection schemes (stratification, clustering, etc.).  In such a case different alternative corrected Chi-square tests are available (cf. Sarndal et al., 1992, Sec. 13.5). One possibility consist in dividing the Pearson's Chi-square test by the <em>generalised design effect</em> of both the surveys.  Its estimation is not straightforward (sampling design variables need to be available). Generally speacking, the generalised design effect is smaller than 1 in the presence of stratified random sampling designs, while it exceeds 1 the presence of a two stage cluster sampling design.  For the purposes of analysis it is reported the value of the generalised design effect <em>g</em> that would determine the acceptance of the null hypothesis (equality of distributions) in the case of <code class="reqn">\alpha = 0.05</code> (<code class="reqn">df = J-1</code>), i.e. values of <em>g</em> such that 
</p>
<p style="text-align: center;"><code class="reqn">   \frac{\chi^2_P}{g} \leq \chi^2_{J-1,0.05} </code>
</p>



<h3>Value</h3>

<p>A <code>list</code> object with two or three components depending on the argument <code>ref</code>. 
</p>
<table>
<tr style="vertical-align: top;">
<td><code>meas</code></td>
<td>
<p>A vector with the measures of similarity/dissimilarity between the distributions: dissimilarity index (<code>"tvd"</code>), overlap (<code>"overlap"</code>), Bhattacharyya coefficient <br> 
(<code>"Bhatt"</code>) and Hellinger's distance (<code>"Hell"</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chi.sq</code></td>
<td>
<p> A vector with the following values: Pearson's Chi-square (<code>"Pearson"</code>), the degrees of freedom (<code>"df"</code>), the percentile of a Chi-squared distribution (<code>"q0.05"</code>) and the largest admissible value of the generalised design effect that would determine the acceptance of H0 (equality of distributions).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p.exp</code></td>
<td>
<p> When <code>ref=FALSE</code> it is reported the value of the reference distribution <code class="reqn">p_{+,j}</code> estimated used in deriving the Chi-square statistic and also the dissimilarity index.  On the contrary (<code>ref=FALSE</code>)  it is set equal to the argument <code>p2</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Marcello D'Orazio <a href="mailto:mdo.statmatch@gmail.com">mdo.statmatch@gmail.com</a> 
</p>


<h3>References</h3>

<p>Agresti A (2002) <em>Categorical Data Analysis. Second Edition</em>. Wiley, new York.
</p>
<p>Sarndal CE, Swensson B, Wretman JH (1992) <em>Model Assisted Survey Sampling</em>. Springer–Verlag, New York.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(quine, package="MASS") #loads quine from MASS
str(quine)

# split quine in two subsets
suppressWarnings(RNGversion("3.5.0"))
set.seed(124)
lab.A &lt;- sample(nrow(quine), 70, replace=TRUE)
quine.A &lt;- quine[lab.A, c("Eth","Sex","Age")]
quine.B &lt;- quine[-lab.A, c("Eth","Sex","Age")]

# compare est. distributions from 2 samples
# 1 variable
tt.A &lt;- xtabs(~Age, data=quine.A)
tt.B &lt;- xtabs(~Age, data=quine.B)
comp.prop(p1=tt.A, p2=tt.B, n1=nrow(quine.A), n2=nrow(quine.B), ref=FALSE)

# joint distr. of more variables
tt.A &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
tt.B &lt;- xtabs(~Eth+Sex+Age, data=quine.B)
comp.prop(p1=tt.A, p2=tt.B, n1=nrow(quine.A), n2=nrow(quine.B), ref=FALSE)

# compare est. distr. with a one considered as reference
tt.A &lt;- xtabs(~Eth+Sex+Age, data=quine.A)
tt.all &lt;- xtabs(~Eth+Sex+Age, data=quine)
comp.prop(p1=tt.A, p2=tt.all, n1=nrow(quine.A), n2=NULL, ref=TRUE)


</code></pre>


</div>