<div class="container">

<table style="width: 100%;"><tr>
<td>similarity_measures_classification</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Similarity Measure Infrastructure for Stability Assessment with Ordinal Responses</h2>

<h3>Description</h3>

<p>Functions that provide objects with functionality used by 
<code>stability</code> to measure the similarity between the predictions
of two results in classification problems.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  clagree()
  ckappa()

  bdist()
  tvdist()
  hdist()
  jsdiv(base = 2)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>A positive or complex number: the base with respect to which 
logarithms are computed. Defaults to 2.</p>
</td>
</tr></table>
<h3>Details</h3>

<p>The similarity measure functions provide objects that include functionality 
used by <code>stability</code> to measure the similarity between the 
probability predictions of two results in classification problems.
</p>
<p>The <code>clagree</code> and <code>ckappa</code> functions provide an object that can be 
used to assess the similarity based on the predicted classes of two results.
The predicted classes are selected by the class with the highest probability.
</p>
<p>The <code>bdist</code> (Bhattacharayya distance), <code>tvdist</code> (Total variation 
distance), <code>hdist</code> (Hellinger distance) and <code>jsdist</code> 
(Jenson-Shannon divergence) functions provide an object that can be 
used to assess the similarity based on the predicted class probabilities of 
two results.
</p>


<h3>See Also</h3>

<p><code>stability</code></p>


<h3>Examples</h3>

<pre><code class="language-R">


set.seed(0)

## build trees
library("partykit")
m1 &lt;- ctree(Species ~ ., data = iris[sample(1:nrow(iris), replace = TRUE),])
m2 &lt;- ctree(Species ~ ., data = iris[sample(1:nrow(iris), replace = TRUE),])

p1 &lt;- predict(m1, type = "prob")
p2 &lt;- predict(m2, type = "prob")

## class agreement
m &lt;- clagree()
m$measure(p1, p2)

## cohen's kappa
m &lt;- ckappa()
m$measure(p1, p2)

## bhattacharayya distance
m &lt;- bdist()
m$measure(p1, p2)

## total variation distance
m &lt;- tvdist()
m$measure(p1, p2)

## hellinger distance
m &lt;- hdist()
m$measure(p1, p2)

## jenson-shannon divergence
m &lt;- jsdiv()
m$measure(p1, p2)

## jenson-shannon divergence (base = exp(1))
m &lt;- jsdiv(base = exp(1))
m$measure(p1, p2)



</code></pre>


</div>