<div class="container">

<table style="width: 100%;"><tr>
<td>sdwd</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>fit the sparse DWD</h2>

<h3>Description</h3>

<p>Fits the sparse distance weighted discrimination (SDWD) model with imposing L1, elastic-net, or adaptive elastic-net penalties. The solution path is computed at a grid of values of tuning parameter <code>lambda</code>. This function is modified based on the <code>glmnet</code> and the <code>gcdnet</code> packages.</p>


<h3>Usage</h3>

<pre><code class="language-R">sdwd(x, y, nlambda=100, 
     lambda.factor=ifelse(nobs &lt; nvars, 0.01, 1e-04), 
     lambda=NULL, lambda2=0, pf=rep(1, nvars), 
     pf2=rep(1, nvars), exclude, dfmax=nvars + 1, 
     pmax=min(dfmax * 1.2, nvars), standardize=TRUE, 
     eps=1e-8, maxit=1e6, strong=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A matrix with <code class="reqn">N</code> rows and <code class="reqn">p</code> columns for predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector of length <code class="reqn">p</code> for binary responses. The element of <code>y</code> is either -1 or 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of <code>lambda</code> values, i.e., length of the <code>lambda</code> sequence. Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.factor</code></td>
<td>
<p>The ratio of the smallest to the largest <code>lambda</code> in the sequence: <code>lambda.factor</code> = <code>min(lambda)</code> / <code>max(lambda)</code>.  <code>max(lambda)</code> is the least <code>lambda</code> to make all coefficients to be zero. The default value of <code>lambda.factor</code> is 0.0001 if <code class="reqn">N &gt;= p</code> or 0.01 if <code class="reqn">N &lt; p</code>. Takes no effect when user specifies a <code>lambda</code> sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>An optional user-supplied <code>lambda</code> sequence. If <code>lambda = NULL</code> (default), the program computes its own <code>lambda</code> sequence based on <code>nlambda</code> and <code>lambda.factor</code>; otherwise, the program uses the user-specified one. Since the program will automatically sort user-defined <code>lambda</code> sequence in decreasing order, it is better to supply a decreasing sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>
<p>The L2 tuning parameter <code class="reqn">\lambda_2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf</code></td>
<td>
<p>A vector of length <code class="reqn">p</code> representing the L1 penalty weights to each coefficient of <code class="reqn">\beta</code> for adaptive L1 or adaptive elastic net. <code>pf</code> can be 0 for some predictor(s), leading to including the predictor(s) all the time. One suggested choice of <code>pf</code> is <code class="reqn">{(\beta + 1/n)}^{-1}</code>, where <code class="reqn">n</code> is the sample size and <code class="reqn">\beta</code> is the coefficents obtained by L1 DWD or enet DWD. Default is 1 for all predictors (and infinity if some predictors are listed in <code>exclude</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf2</code></td>
<td>
<p>A vector of length <code class="reqn">p</code> for L2 penalty factor for adaptive L1 or adaptive elastic net. To allow different L2 shrinkage, user can set <code>pf2</code> to be different L2 penalty weights for each coefficient of <code class="reqn">\beta</code>. <code>pf2</code> can be 0 for some variables, indicating no L2 shrinkage. Default is 1 for all predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>Whether to exclude some predictors from the model. This is equivalent to adopting an infinite penalty factor when excluding some predictor. Default is none. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>Restricts at most how many predictors can be incorporated in the model. Default is <code class="reqn">p+1</code>. This restriction is helpful when <code class="reqn">p</code> is large, provided that a partial path is acceptable. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>Restricts the maximum number of variables ever to be nonzero; e.g, once some <code class="reqn">\beta</code> enters the model, it counts once. The count will not change when the <code class="reqn">\beta</code> exits or re-enters the model. Default is <code>min(dfmax*1.2,p)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Whether to standardize the data. If <code>TRUE</code>, <code>sdwd</code> normalizes the predictors such that each column has sum squares<code class="reqn">\sum^N_{i=1}x_{ij}^2/N=1</code> of one. Note that x is always centered (i.e. <code class="reqn">\sum^N_{i=1}x_{ij}=0</code>) no matter <code>standardize</code> is <code>TRUE</code> or <code>FALSE</code>. <code>sdwd</code> always returns coefficient <code>beta</code> on the original scale.  Default value is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>The algorithm stops when (i.e. <code class="reqn">4\max_j(\beta_j^{new}-\beta_j^{old})^2</code> is less than <code>eps</code>, where <code class="reqn">j=0,\ldots, p</code>. Defaults value is <code>1e-8</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Restricts how many outer-loop iterations are allowed. Default is 1e6. Consider increasing <code>maxit</code> when the algorithm does not converge.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>strong</code></td>
<td>
<p>If <code>TRUE</code>, adopts the strong rule to accelerate the algorithm.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>sdwd</code> minimizes the sparse penalized DWD loss function, 
</p>
<p style="text-align: center;"><code class="reqn">L(y, X, \beta)/N + \lambda_1||\beta||_1 + 0.5\lambda_2||\beta||_2^2,</code>
</p>

<p>where <code class="reqn">L(u)=1-u</code> if <code class="reqn">u \le 1/2</code>, <code class="reqn">1/(4u)</code> if <code class="reqn">u &gt; 1/2</code> is the DWD loss. The value of <code>lambda2</code> is user-specified.
</p>
<p>To use the L1 penalty (lasso), set <code>lambda2=0</code>. To use the elastic net, set <code>lambda2</code> as nonzero. To use the adaptive L1, set <code>lambda2=0</code> and specify <code>pf</code> and <code>pf2</code>. To use the adaptive elastic net, set <code>lambda2</code> as nonzero and specify <code>pf</code> and <code>pf2</code> as well.  
</p>
<p>When the algorithm do not converge or run slow, consider increasing <code>eps</code>, decreasing
<code>nlambda</code>, or increasing <code>lambda.factor</code> before increasing
<code>maxit</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>sdwd</code>.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>A vector of length <code>length(lambda)</code> representing the intercept at each <code>lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>A matrix of dimension <code>p*length(lambda)</code> representing the coefficients at each <code>lambda</code> value. The matrix is stored as a sparse matrix  (<code>Matrix</code> package). To convert it into normal type matrix use <code>as.matrix()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>The number of nonzero coefficients at each <code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>The dimension of coefficient matrix, i.e., <code>p*length(lambda)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The <code>lambda</code> sequence that was actually used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>npasses</code></td>
<td>
<p>Total number of iterations for all lambda values. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jerr</code></td>
<td>
<p>Warnings and errors; 0 if no error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>The call that produced this object.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Boxiang Wang and Hui Zou<br>
Maintainer: Boxiang Wang  <a href="mailto:boxiang-wang@uiowa.edu">boxiang-wang@uiowa.edu</a></p>


<h3>References</h3>

<p>Wang, B. and Zou, H. (2016)
“Sparse Distance Weighted Discrimination", <em>Journal of Computational and Graphical Statistics</em>, <b>25</b>(3), 826–838.<br><a href="https://www.tandfonline.com/doi/full/10.1080/10618600.2015.1049700">https://www.tandfonline.com/doi/full/10.1080/10618600.2015.1049700</a><br><br>
Friedman, J., Hastie, T., and Tibshirani, R. (2010), "Regularization paths for generalized
linear models via coordinate descent", <em>Journal of Statistical Software</em>, <b>33</b>(1), 1–22.<br><a href="https://www.jstatsoft.org/v33/i01/paper">https://www.jstatsoft.org/v33/i01/paper</a><br><br>
Marron, J.S., Todd, M.J., and Ahn, J. (2007)
“Distance-Weighted Discrimination", 
<em>Journal of the American Statistical Association</em>, <b>102</b>(408), 1267–1271.<br><a href="https://www.tandfonline.com/doi/abs/10.1198/016214507000001120">https://www.tandfonline.com/doi/abs/10.1198/016214507000001120</a><br><br>
Tibshirani, Robert., Bien, J., Friedman, J.,Hastie, T.,Simon,
N.,Taylor, J., and Tibshirani, Ryan. (2012)
Strong Rules for Discarding Predictors in Lasso-type Problems,
<em>Journal of the Royal Statistical Society, Series B</em>, <b>74</b>(2), 245–266.<br><a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.01004.x">https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.01004.x</a><br><br>
Yang, Y. and Zou, H. (2013)
“An Efficient Algorithm for Computing the HHSVM and Its Generalizations", 
<em>Journal of Computational and Graphical Statistics</em>, <b>22</b>(2), 396–415.<br><a href="https://www.tandfonline.com/doi/full/10.1080/10618600.2012.680324">https://www.tandfonline.com/doi/full/10.1080/10618600.2012.680324</a><br></p>


<h3>See Also</h3>

<p><code>print.sdwd</code>, <code>predict.sdwd</code>, <code>coef.sdwd</code>, <code>plot.sdwd</code>, and <code>cv.sdwd</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># load the data
data(colon)
# fit the elastic-net penalized DWD with lambda2=1
fit = sdwd(colon$x, colon$y, lambda2=1)
print(fit)
# coefficients at some lambda value
c1 = coef(fit, s=0.005)
# make predictions
predict(fit, newx=colon$x[1:10, ], s=c(0.01, 0.005))

</code></pre>


</div>