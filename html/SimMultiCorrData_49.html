<div class="container">

<table style="width: 100%;"><tr>
<td>rcorrvar</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Correlation Method 1</h2>

<h3>Description</h3>

<p>This function simulates <code>k_cat</code> ordinal, <code>k_cont</code> continuous, <code>k_pois</code> Poisson, and/or <code>k_nb</code>
Negative Binomial variables with
a specified correlation matrix <code>rho</code>.  The variables are generated from multivariate normal variables with intermediate correlation
matrix <code>Sigma</code>, calculated by <code>findintercorr</code>, and then transformed.  The <em>ordering</em> of the
variables in <code>rho</code> must be <em>ordinal</em> (r &gt;= 2 categories), <em>continuous</em>, <em>Poisson</em>, and <em>Negative Binomial</em>
(note that it is possible for <code>k_cat</code>, <code>k_cont</code>, <code>k_pois</code>, and/or <code>k_nb</code> to be 0).  The vignette
<b>Overall Workflow for Data Simulation</b> provides a detailed example discussing the step-by-step simulation process and comparing
correlation methods 1 and 2.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rcorrvar(n = 10000, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means = NULL, vars = NULL,
  skews = NULL, skurts = NULL, fifths = NULL, sixths = NULL,
  Six = list(), marginal = list(), support = list(), nrand = 100000,
  lam = NULL, size = NULL, prob = NULL, mu = NULL, Sigma = NULL,
  rho = NULL, cstart = NULL, seed = 1234, errorloop = FALSE,
  epsilon = 0.001, maxit = 1000, extra_correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>the sample size (i.e. the length of each simulated variable; default = 10000)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_cont</code></td>
<td>
<p>the number of continuous variables (default = 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_cat</code></td>
<td>
<p>the number of ordinal (r &gt;= 2 categories) variables (default = 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_pois</code></td>
<td>
<p>the number of Poisson variables (default = 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_nb</code></td>
<td>
<p>the number of Negative Binomial variables (default = 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the method used to generate the k_cont continuous variables.  "Fleishman" uses Fleishman's third-order polynomial transformation
and "Polynomial" uses Headrick's fifth-order transformation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>means</code></td>
<td>
<p>a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vars</code></td>
<td>
<p>a vector of variances (i.e. = rep(1, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skews</code></td>
<td>
<p>a vector of skewness values (i.e. = rep(0, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skurts</code></td>
<td>
<p>a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fifths</code></td>
<td>
<p>a vector of standardized fifth cumulants (not necessary for <code>method</code> = "Fleishman"; i.e. = rep(0, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sixths</code></td>
<td>
<p>a vector of standardized sixth cumulants (not necessary for <code>method</code> = "Fleishman"; i.e. = rep(0, k_cont))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Six</code></td>
<td>
<p>a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: <code>Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))</code>; if no correction is desired for variable Y_i, set set the i-th list
component equal to NULL</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>marginal</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list());
for binary variables, these should be input the same as for ordinal variables with more than 2 categories (i.e. the user-specified
probability is the probability of the 1st category, which has the smaller support value)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>support</code></td>
<td>
<p>a list of length equal to <code>k_cat</code>; the i-th element is a vector containing the r ordered support values;
if not provided (i.e. <code>support = list()</code>), the default is for the i-th element to be the
vector 1, ..., r</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nrand</code></td>
<td>
<p>the number of random numbers to generate in calculating intermediate correlations (default = 10000)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>a vector of lambda (&gt; 0) constants for the Poisson variables (see <code>Poisson</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>a vector of size parameters for the Negative Binomial variables (see <code>NegBinomial</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob</code></td>
<td>
<p>a vector of success probability parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>a vector of mean parameters (*Note: either <code>prob</code> or <code>mu</code> should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Sigma</code></td>
<td>
<p>an intermediate correlation matrix to use if the user wants to provide one (default = NULL)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>the target correlation matrix (<em>must be ordered ordinal, continuous, Poisson, Negative Binomial</em>; default = NULL)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cstart</code></td>
<td>
<p>a list containing initial values for root-solving algorithm used in <code>find_constants</code>
(see <code>multiStart</code> for <code>method</code> = "Fleishman" or <code>nleqslv</code> for <code>method</code> = "Polynomial").
If user specified, each list element must be input as a matrix. If no starting values are specified for a given continuous
variable, that list element should be NULL.  If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see <code>Headrick.dist</code>
data), uses his constants as starting values; else, generates n sets of random starting values from
uniform distributions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>the seed value for random number generation (default = 1234)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>errorloop</code></td>
<td>
<p>if TRUE, uses <code>error_loop</code> to attempt to correct the final correlation
(default = FALSE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with <code>ordnorm</code> or in the error loop</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with <code>ordnorm</code> or in the error loop</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>extra_correct</code></td>
<td>
<p>if TRUE, within each variable pair, if the maximum correlation error is still greater than 0.1, the intermediate
correlation is set equal to the target correlation (with the assumption that the calculated final correlation will be
less than 0.1 away from the target)</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list whose components vary based on the type of simulated variables.  Simulated variables are returned as data.frames:
</p>
<p>If <b>ordinal variables</b> are produced:
</p>
<p><code>ordinal_variables</code> the generated ordinal variables,
</p>
<p><code>summary_ordinal</code> a list, where the i-th element contains a data.frame with column 1 = target cumulative probabilities and
column 2 = simulated cumulative probabilities for ordinal variable Y_i
</p>
<p>If <b>continuous variables</b> are produced:
</p>
<p><code>constants</code> a data.frame of the constants,
</p>
<p><code>continuous_variables</code> the generated continuous variables,
</p>
<p><code>summary_continuous</code> a data.frame containing a summary of each variable,
</p>
<p><code>summary_targetcont</code> a data.frame containing a summary of the target variables,
</p>
<p><code>sixth_correction</code> a vector of sixth cumulant correction values,
</p>
<p><code>valid.pdf</code> a vector where the i-th element is "TRUE" if the constants for the i-th continuous variable generate a valid pdf,
else "FALSE"
</p>
<p>If <b>Poisson variables</b> are produced:
</p>
<p><code>Poisson_variables</code> the generated Poisson variables,
</p>
<p><code>summary_Poisson</code> a data.frame containing a summary of each variable
</p>
<p>If <b>Negative Binomial variables</b> are produced:
</p>
<p><code>Neg_Bin_variables</code> the generated Negative Binomial variables,
</p>
<p><code>summary_Neg_Bin</code> a data.frame containing a summary of each variable
</p>
<p>Additionally, the following elements:
</p>
<p><code>correlations</code> the final correlation matrix,
</p>
<p><code>Sigma1</code> the intermediate correlation before the error loop,
</p>
<p><code>Sigma2</code> the intermediate correlation matrix after the error loop,
</p>
<p><code>Constants_Time</code> the time in minutes required to calculate the constants,
</p>
<p><code>Intercorrelation_Time</code> the time in minutes required to calculate the intermediate correlation matrix,
</p>
<p><code>Error_Loop_Time</code> the time in minutes required to use the error loop,
</p>
<p><code>Simulation_Time</code> the total simulation time in minutes,
</p>
<p><code>niter</code> a matrix of the number of iterations used for each variable in the error loop,
</p>
<p><code>maxerr</code> the maximum final correlation error (from the target rho).
</p>
<p>If a particular element is not required, the result is NULL for that element.
</p>


<h3>Variable Types and Required Inputs</h3>

<p>1) <b>Continuous Variables:</b> Continuous variables are simulated using either Fleishman's third-order (<code>method</code> = "Fleishman",
doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>) or Headrick's fifth-order (<code>method</code> = "Polynomial", doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>) power method
transformation.  This is a computationally efficient algorithm that simulates continuous distributions through the method of moments.
It works by matching standardized cumulants â€“ the first four (mean, variance, skew, and standardized kurtosis) for Fleishman's method, or
the first six (mean, variance, skew, standardized kurtosis, and standardized fifth and sixth cumulants) for Headrick's method.  The
transformation is expressed as follows:
</p>
<p><code class="reqn">Y = c_{0} + c_{1} * Z + c_{2} * Z^2 + c_{3} * Z^3 + c_{4} * Z^4 + c_{5} * Z^5</code>,
</p>
<p>where <code class="reqn">Z ~ N(0,1)</code>, and <code class="reqn">c_{4}</code> and <code class="reqn">c_{5}</code> both equal <code class="reqn">0</code> for Fleishman's method.  The real constants are calculated by
<code>find_constants</code>.  All variables are simulated with mean <code class="reqn">0</code> and variance <code class="reqn">1</code>, and then transformed
to the specified mean and variance at the end.
</p>
<p>The required parameters for simulating continuous variables include: mean, variance, skewness, standardized kurtosis (kurtosis - 3), and
standardized fifth and sixth cumulants (for <code>method</code> = "Polynomial").  If the goal is to simulate a theoretical distribution
(i.e. Gamma, Beta, Logistic, etc.), these values can be obtained using <code>calc_theory</code>.  If the goal is to
mimic an empirical data set, these values can be found using <code>calc_moments</code> (using the method of moments) or
<code>calc_fisherk</code> (using Fisher's k-statistics).  If the standardized cumulants
are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e. <code>skews = round(skews, 8)</code>).  Due to the nature
of the integration involved in <code>calc_theory</code>, the results are approximations.  Greater accuracy can be achieved by increasing the number of
subdivisions (<code>sub</code>) used in the integration process.  For example, in order to ensure that skew is exactly 0 for symmetric distributions.
</p>
<p>For some sets of cumulants, it is either not possible to find
power method constants or the calculated constants do not generate valid power method pdfs.  In these situations, adding a value to the
sixth cumulant may provide solutions (see <code>find_constants</code>).  When using Headrick's fifth-order approximation,
if simulation results indicate that a
continuous variable does not generate a valid pdf, the user can try <code>find_constants</code> with various sixth
cumulant correction vectors to determine if a valid pdf can be found.
</p>
<p>2) <b>Binary and Ordinal Variables:</b> Ordinal variables (<code class="reqn">r \ge 2</code> categories) are generated by discretizing the standard normal variables
at quantiles.  These quantiles are determined by evaluating the inverse standard normal cdf at the cumulative probabilities defined by each
variable's marginal distribution.  The required inputs for ordinal variables are the cumulative marginal probabilities and support values
(if desired).  The probabilities should be combined into a list of length equal to the number of ordinal variables.  The <code class="reqn">i^{th}</code> element
is a vector of the cumulative probabilities defining the marginal distribution of the <code class="reqn">i^{th}</code> variable.  If the variable can take
<code class="reqn">r</code> values, the vector will contain <code class="reqn">r - 1</code> probabilities (the <code class="reqn">r^{th}</code> is assumed to be <code class="reqn">1</code>).
</p>
<p><em>Note for binary variables:</em> the user-suppled probability should be the probability of the <code class="reqn">1^{st}</code> (lower) support value.  This would
ordinarily be considered the probability of <em>failure</em> (<code class="reqn">q</code>), while the probability of the <code class="reqn">2^{nd}</code> (upper) support value would be
considered the probability of <em>success</em> (<code class="reqn">p = 1 - q</code>).  The support values should be combined into a separate list.  The <code class="reqn">i^{th}</code>
element is a vector containing the <code class="reqn">r</code> ordered support values.
</p>
<p>3) <b>Count Variables:</b> Count variables are generated using the inverse cdf method.  The cumulative distribution function of a standard
normal variable has a uniform distribution.  The appropriate quantile function <code class="reqn">F_{Y}^{-1}</code> is applied to this uniform variable with the
designated parameters to generate the count variable: <code class="reqn">Y = F_{y}^{-1}(\Phi(Z))</code>.  For Poisson variables, the lambda (mean) value should be
given.  For Negative Binomial variables, the size (target number of successes) and either the success probability or the mean should be given.
The Negative Binomial variable represents the number of failures which occur in a sequence of Bernoulli trials before the target number of
successes is achieved.
</p>
<p>More details regarding the variable types can be found in the <b>Variable Types</b> vignette.
</p>


<h3>Overview of Correlation Method 1</h3>

<p>The intermediate correlations used in correlation method 1 are more simulation based than those in method 2, which means that accuracy
increases with sample size and the number of repetitions.  In addition, specifying the seed allows for reproducibility.  In
addition, method 1 differs from method 2 in the following ways:
</p>
<p>1) The intermediate correlation for <b>count variables</b> is based on the method of Yahav &amp; Shmueli (2012, doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>), which
uses a simulation based, logarithmic transformation of the target correlation.  This method becomes less accurate as the variable mean
gets closer to zero.
</p>
<p>2) The <b>ordinal - count variable</b> correlations are based on an extension of the method of Amatya &amp; Demirtas (2015,
doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>), in which
the correlation correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between the count
variable and the normal variable used to generate it and a simulated upper bound on the correlation between an ordinal variable
and the normal variable used to generate it (see Demirtas &amp; Hedeker, 2011, doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>).
</p>
<p>3) The <b>continuous - count variable</b> correlations are based on an extension of the methods of Amatya &amp; Demirtas (2015) and
Demirtas et al. (2012, doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>), in which the correlation correction factor is the product of the upper Frechet-Hoeffding bound
on the correlation between the count variable and the normal variable used to generate it and the power method correlation
between the continuous variable and the normal variable used to generate it (see Headrick &amp; Kowalchuk, 2007, doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>).
The intermediate correlations are the ratio of the target correlations to the correction factor.
</p>
<p>Please see the <b>Comparison of Method 1 and Method 2</b> vignette for more information and an step-by-step overview of the
simulation process.
</p>


<h3>Choice of Fleishman's third-order or Headrick's fifth-order method</h3>

<p>Using the fifth-order approximation allows additional control over the fifth and sixth moments of the generated distribution, improving
accuracy.  In addition, the range of feasible standardized kurtosis values, given skew and standardized fifth (<code class="reqn">\gamma_{3}</code>) and sixth
(<code class="reqn">\gamma_{4}</code>) cumulants, is larger than with Fleishman's method (see <code>calc_lower_skurt</code>).
For example, the Fleishman method can not be used to generate a
non-normal distribution with a ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} &gt; 9/14</code> (see Headrick &amp; Kowalchuk, 2007).
This eliminates the
Chi-squared family of distributions, which has a constant ratio of <code class="reqn">\gamma_{3}^2/\gamma_{4} = 2/3</code>.  However, if the fifth and
sixth cumulants do not exist, the Fleishman approximation should be used.
</p>


<h3>Reasons for Function Errors</h3>

<p>1) The most likely cause for function errors is that no solutions to <code>fleish</code> or
<code>poly</code> converged when using <code>find_constants</code>.  If this happens,
the simulation will stop.  It may help to first use <code>find_constants</code> for each continuous variable to
determine if a vector of sixth cumulant correction values is needed.  The solutions can be used as starting values (see <code>cstart</code> below).
If the standardized cumulants are obtained from <code>calc_theory</code>, the user may need to use rounded values as inputs (i.e.
<code>skews = round(skews, 8)</code>).
</p>
<p>2) In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
<code>calc_lower_skurt</code> to determine the boundary for a given set of cumulants.
</p>
<p>3) As mentioned above, the feasibility of the final correlation matrix rho, given the
distribution parameters, should be checked first using <code>valid_corr</code>.  This function either checks
if a given <code>rho</code> is plausible or returns the lower and upper final correlation limits.  It should be noted that even if a target
correlation matrix is within the "plausible range," it still may not be possible to achieve the desired matrix.  This happens most
frequently when generating ordinal variables (r &gt;= 2 categories).  The error loop frequently fixes these problems.
</p>


<h3>References</h3>

<p>Amatya A &amp; Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
Journal of Statistical Computation and Simulation, 85(15): 3129-39. doi: <a href="http://doi.org/10.1080/00949655.2014.953534">10.1080/00949655.2014.953534</a>.
</p>
<p>Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. <a href="https://CRAN.R-project.org/package=GenOrd">https://CRAN.R-project.org/package=GenOrd</a>
</p>
<p>Demirtas H &amp; Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
American Statistician, 65(2): 104-109. doi: <a href="http://doi.org/10.1198/tast.2011.10090">10.1198/tast.2011.10090</a>.
</p>
<p>Demirtas H, Hedeker D, &amp; Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
Statistics in Medicine, 31(27): 3337-3346. doi: <a href="http://doi.org/10.1002/sim.5362">10.1002/sim.5362</a>.
</p>
<p>Ferrari PA, Barbiero A (2012). Simulating ordinal data. Multivariate Behavioral Research, 47(4): 566-589.
doi: <a href="http://doi.org/10.1080/00273171.2012.692630">10.1080/00273171.2012.692630</a>.
</p>
<p>Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532. doi: <a href="http://doi.org/10.1007/BF02293811">10.1007/BF02293811</a>.
</p>
<p>Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.
</p>
<p>Hasselman B (2018). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.3.2.
<a href="https://CRAN.R-project.org/package=nleqslv">https://CRAN.R-project.org/package=nleqslv</a>
</p>
<p>Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
Non-normal Distributions. Computational Statistics &amp; Data Analysis, 40(4):685-711. doi: <a href="http://doi.org/10.1016/S0167-9473(02)00072-5">10.1016/S0167-9473(02)00072-5</a>.
(<a href="http://www.sciencedirect.com/science/article/pii/S0167947302000725">ScienceDirect</a>)
</p>
<p>Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
Journal of Modern Applied Statistical Methods, 3(1), 65-71. doi: <a href="http://doi.org/10.22237/jmasm/1083370080">10.22237/jmasm/1083370080</a>.
</p>
<p>Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249. doi: <a href="http://doi.org/10.1080/10629360600605065">10.1080/10629360600605065</a>.
</p>
<p>Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
Method. Psychometrika, 64, 25-35. doi: <a href="http://doi.org/10.1007/BF02294317">10.1007/BF02294317</a>.
</p>
<p>Headrick TC, Sheng Y, &amp; Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
Mathematica. Journal of Statistical Software, 19(3), 1 - 17. doi: <a href="http://doi.org/10.18637/jss.v019.i03">10.18637/jss.v019.i03</a>.
</p>
<p>Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.
</p>
<p>Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
New York: Springer-Verlag; 1994. p. 57-107.
</p>
<p>Olsson U, Drasgow F, &amp; Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
doi: <a href="http://doi.org/10.1007/BF02294164">10.1007/BF02294164</a>.
</p>
<p>Vale CD &amp; Maurelli VA (1983). Simulating Multivariate Nonnormal Distributions. Psychometrika, 48, 465-471. doi: <a href="http://doi.org/10.1007/BF02293687">10.1007/BF02293687</a>.
</p>
<p>Varadhan R, Gilbert P (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32(4). doi: <a href="http://doi.org/10.18637/jss.v032.i04">10.18637/jss.v032.i04</a>.
<a href="http://www.jstatsoft.org/v32/i04/">http://www.jstatsoft.org/v32/i04/</a>
</p>
<p>Yahav I &amp; Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
Models in Business and Industry, 28(1): 91-102. doi: <a href="http://doi.org/10.1002/asmb.901">10.1002/asmb.901</a>.
</p>


<h3>See Also</h3>

<p><code>find_constants</code>, <code>findintercorr</code>,
<code>multiStart</code>, <code>nleqslv</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">Sim1 &lt;- rcorrvar(n = 1000, k_cat = 1, k_cont = 1, method = "Polynomial",
  means = 0, vars = 1, skews = 0, skurts = 0, fifths = 0, sixths = 0,
  marginal = list(c(1/3, 2/3)), support = list(0:2),
  rho = matrix(c(1, 0.4, 0.4, 1), 2, 2))

## Not run: 

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed &lt;- 1234
n &lt;- 10000

Dist &lt;- c("Logistic", "Weibull")
Params &lt;- list(c(0, 1), c(3, 5))
Stcum1 &lt;- calc_theory(Dist[1], Params[[1]])
Stcum2 &lt;- calc_theory(Dist[2], Params[[2]])
Stcum &lt;- rbind(Stcum1, Stcum2)
rownames(Stcum) &lt;- Dist
colnames(Stcum) &lt;- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six &lt;- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal &lt;- list(0.3)
lam &lt;- 0.5
size &lt;- 2
prob &lt;- 0.75

Rey &lt;- matrix(0.4, 5, 5)
diag(Rey) &lt;- 1

# Make sure Rey is within upper and lower correlation limits
valid &lt;- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                    method = "Polynomial", means = Stcum[, 1],
                    vars = Stcum[, 2]^2, skews = Stcum[, 3],
                    skurts = Stcum[, 4], fifths = Stcum[, 5],
                    sixths = Stcum[, 6], Six = Six, marginal = marginal,
                    lam = lam, size = size, prob = prob, rho = Rey,
                    seed = seed)

# Simulate variables without error loop
Sim1 &lt;- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
                 method = "Polynomial", means = Stcum[, 1],
                 vars = Stcum[, 2]^2, skews = Stcum[, 3],
                 skurts = Stcum[, 4], fifths = Stcum[, 5],
                 sixths = Stcum[, 6], Six = Six, marginal = marginal,
                 lam = lam, size = size, prob = prob, rho = Rey,
                 seed = seed)
names(Sim1)

# Look at the maximum correlation error
Sim1$maxerr

Sim1_error = round(Sim1$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(Sim1_error), 0.25)
quantile(as.numeric(Sim1_error), 0.75)

# Simulate variables with error loop
Sim1_EL &lt;- rcorrvar(n = n, k_cat = 1, k_cont = 2,
                    k_pois = 1, k_nb = 1, method = "Polynomial",
                    means = Stcum[, 1], vars = Stcum[, 2]^2,
                    skews = Stcum[, 3], skurts = Stcum[, 4],
                    fifths = Stcum[, 5], sixths = Stcum[, 6],
                    Six = Six, marginal = marginal, lam = lam,
                    size = size, prob = prob, rho = Rey,
                    seed = seed, errorloop = TRUE)
# Look at the maximum correlation error
Sim1_EL$maxerr

EL_error = round(Sim1_EL$correlations - Rey, 6)

# interquartile-range of correlation errors
quantile(as.numeric(EL_error), 0.25)
quantile(as.numeric(EL_error), 0.75)

# Look at results
# Ordinal variables
Sim1_EL$summary_ordinal

# Continuous variables
round(Sim1_EL$constants, 6)
round(Sim1_EL$summary_continuous, 6)
round(Sim1_EL$summary_targetcont, 6)
Sim1_EL$valid.pdf

# Count variables
Sim1_EL$summary_Poisson
Sim1_EL$summary_Neg_Bin

# Generate Plots

# Logistic (1st continuous variable)
# 1) Simulated Data CDF (find cumulative probability up to y = 0.5)
plot_sim_cdf(Sim1_EL$continuous_variables[, 1], calc_cprob = TRUE,
             delta = 0.5)

# 2) Simulated Data and Target Distribution PDFs
plot_sim_pdf_theory(Sim1_EL$continuous_variables[, 1], Dist = "Logistic",
                    params = c(0, 1))

# 3) Simulated Data and Target Distribution
plot_sim_theory(Sim1_EL$continuous_variables[, 1], Dist = "Logistic",
                params = c(0, 1))


## End(Not run)
</code></pre>


</div>