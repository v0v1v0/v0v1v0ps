<div class="container">

<table style="width: 100%;"><tr>
<td>sits_tempcnn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train temporal convolutional neural network models</h2>

<h3>Description</h3>

<p>Use a TempCNN algorithm to classify data, which has
two stages: a 1D CNN and a  multi-layer perceptron.
Users can define the depth of the 1D network, as well as
the number of perceptron layers.
</p>
<p>This function is based on the paper by Charlotte Pelletier referenced below.
If you use this method, please cite the original tempCNN paper.
</p>
<p>The torch version is based on the code made available by the BreizhCrops
team: Marc Russwurm, Charlotte Pelletier, Marco Korner, Maximilian Zollner.
The original python code is available at the website
https://github.com/dl4sits/BreizhCrops. This code is licensed as GPL-3.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sits_tempcnn(
  samples = NULL,
  samples_validation = NULL,
  cnn_layers = c(64, 64, 64),
  cnn_kernels = c(5, 5, 5),
  cnn_dropout_rates = c(0.2, 0.2, 0.2),
  dense_layer_nodes = 256,
  dense_layer_dropout_rate = 0.5,
  epochs = 150,
  batch_size = 64,
  validation_split = 0.2,
  optimizer = torch::optim_adamw,
  opt_hparams = list(lr = 5e-04, eps = 1e-08, weight_decay = 1e-06),
  lr_decay_epochs = 1,
  lr_decay_rate = 0.95,
  patience = 20,
  min_delta = 0.01,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>samples</code></td>
<td>
<p>Time series with the training samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>samples_validation</code></td>
<td>
<p>Time series with the validation samples. if the
<code>samples_validation</code> parameter is provided,
the <code>validation_split</code> parameter is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cnn_layers</code></td>
<td>
<p>Number of 1D convolutional filters per layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cnn_kernels</code></td>
<td>
<p>Size of the 1D convolutional kernels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cnn_dropout_rates</code></td>
<td>
<p>Dropout rates for 1D convolutional filters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dense_layer_nodes</code></td>
<td>
<p>Number of nodes in the dense layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dense_layer_dropout_rate</code></td>
<td>
<p>Dropout rate (0,1) for the dense layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>Number of iterations to train the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>Number of samples per gradient update.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation_split</code></td>
<td>
<p>Fraction of training data to be used for
validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>Optimizer function to be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>opt_hparams</code></td>
<td>
<p>Hyperparameters for optimizer:
lr : Learning rate of the optimizer
eps: Term added to the denominator
to improve numerical stability.
weight_decay:       L2 regularization</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr_decay_epochs</code></td>
<td>
<p>Number of epochs to reduce learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr_decay_rate</code></td>
<td>
<p>Decay factor for reducing learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>patience</code></td>
<td>
<p>Number of epochs without improvements until
training stops.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_delta</code></td>
<td>
<p>Minimum improvement in loss function
to reset the patience counter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Verbosity mode (TRUE/FALSE). Default is FALSE.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A fitted model to be used for classification.
</p>


<h3>Note</h3>

<p>Please refer to the sits documentation available in
&lt;https://e-sensing.github.io/sitsbook/&gt; for detailed examples.
</p>


<h3>Author(s)</h3>

<p>Charlotte Pelletier, <a href="mailto:charlotte.pelletier@univ-ubs.fr">charlotte.pelletier@univ-ubs.fr</a>
</p>
<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>


<h3>References</h3>

<p>Charlotte Pelletier, Geoffrey Webb and Fran√ßois Petitjean,
"Temporal Convolutional Neural Network for the Classification
of Satellite Image Time Series",
Remote Sensing, 11,523, 2019. DOI: 10.3390/rs11050523.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (sits_run_examples()) {
    # create a TempCNN model
    torch_model &lt;- sits_train(samples_modis_ndvi,
               sits_tempcnn(epochs = 20, verbose = TRUE))
    # plot the model
    plot(torch_model)
    # create a data cube from local files
    data_dir &lt;- system.file("extdata/raster/mod13q1", package = "sits")
    cube &lt;- sits_cube(
        source = "BDC",
        collection = "MOD13Q1-6.1",
        data_dir = data_dir
    )
    # classify a data cube
    probs_cube &lt;- sits_classify(
        data = cube, ml_model = torch_model, output_dir = tempdir()
    )
    # plot the probability cube
    plot(probs_cube)
    # smooth the probability cube using Bayesian statistics
    bayes_cube &lt;- sits_smooth(probs_cube, output_dir = tempdir())
    # plot the smoothed cube
    plot(bayes_cube)
    # label the probability cube
    label_cube &lt;- sits_label_classification(
        bayes_cube,
        output_dir = tempdir()
    )
    # plot the labelled cube
    plot(label_cube)
}
</code></pre>


</div>