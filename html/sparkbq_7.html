<div class="container">

<table style="width: 100%;"><tr>
<td>spark_read_bigquery</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Reading data from Google BigQuery</h2>

<h3>Description</h3>

<p>This function reads data stored in a Google BigQuery table.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_read_bigquery(sc, name,
  billingProjectId = default_billing_project_id(),
  projectId = billingProjectId, datasetId = NULL, tableId = NULL,
  sqlQuery = NULL, type = default_bigquery_type(),
  gcsBucket = default_gcs_bucket(),
  serviceAccountKeyFile = default_service_account_key_file(),
  additionalParameters = NULL, memory = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p><code>spark_connection</code> provided by sparklyr.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>The name to assign to the newly generated table (see also
<code>spark_read_source</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>billingProjectId</code></td>
<td>
<p>Google Cloud Platform project ID for billing purposes.
This is the project on whose behalf to perform BigQuery operations.
Defaults to <code>default_billing_project_id()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>projectId</code></td>
<td>
<p>Google Cloud Platform project ID of BigQuery dataset.
Defaults to <code>billingProjectId</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>datasetId</code></td>
<td>
<p>Google BigQuery dataset ID (may contain letters, numbers and underscores).
Either both of <code>datasetId</code> and <code>tableId</code> or <code>sqlQuery</code> must be specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tableId</code></td>
<td>
<p>Google BigQuery table ID (may contain letters, numbers and underscores).
Either both of <code>datasetId</code> and <code>tableId</code> or <code>sqlQuery</code> must be specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sqlQuery</code></td>
<td>
<p>Google BigQuery SQL query. Either both of <code>datasetId</code> and <code>tableId</code>
or <code>sqlQuery</code> must be specified. The query must be specified in standard SQL
(SQL-2011). Legacy SQL is not supported. Tables are specified as
'&lt;project_id&gt;.&lt;dataset_id&gt;.&lt;table_id&gt;'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>BigQuery import type to use. Options include "direct", "avro",
"json" and "csv". Defaults to <code>default_bigquery_type()</code>.
See bigquery_defaults for more details about the supported types.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gcsBucket</code></td>
<td>
<p>Google Cloud Storage (GCS) bucket to use for storing temporary files.
Temporary files are used when importing through BigQuery load jobs and exporting through
BigQuery extraction jobs (i.e. when using data extracts such as Parquet, Avro, ORC, ...).
The service account specified in <code>serviceAccountKeyFile</code> needs to be given appropriate
rights. This should be the name of an existing storage bucket.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>serviceAccountKeyFile</code></td>
<td>
<p>Google Cloud service account key file to use for authentication
with Google Cloud services. The use of service accounts is highly recommended. Specifically,
the service account will be used to interact with BigQuery and Google Cloud Storage (GCS).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>additionalParameters</code></td>
<td>
<p>Additional spark-bigquery options. See
<a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a> for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p><code>logical</code> specifying whether data should be loaded eagerly into
memory, i.e. whether the table should be cached. Note that eagerly caching prevents
predicate pushdown (e.g. in conjunction with <code>filter</code>) and therefore
the default is <code>FALSE</code>. See also <code>spark_read_source</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>spark_read_source</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A <code>tbl_spark</code> which provides a <code>dplyr</code>-compatible reference to a
Spark DataFrame.
</p>


<h3>References</h3>

<p><a href="https://github.com/miraisolutions/spark-bigquery">https://github.com/miraisolutions/spark-bigquery</a>
<a href="https://cloud.google.com/bigquery/docs/datasets">https://cloud.google.com/bigquery/docs/datasets</a>
<a href="https://cloud.google.com/bigquery/docs/tables">https://cloud.google.com/bigquery/docs/tables</a>
<a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/">https://cloud.google.com/bigquery/docs/reference/standard-sql/</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json</a>
<a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv</a>
<a href="https://cloud.google.com/bigquery/pricing">https://cloud.google.com/bigquery/pricing</a>
<a href="https://cloud.google.com/bigquery/docs/dataset-locations">https://cloud.google.com/bigquery/docs/dataset-locations</a>
<a href="https://cloud.google.com/docs/authentication/">https://cloud.google.com/docs/authentication/</a>
<a href="https://cloud.google.com/bigquery/docs/authentication/">https://cloud.google.com/bigquery/docs/authentication/</a>
</p>


<h3>See Also</h3>

<p><code>spark_read_source</code>, <code>spark_write_bigquery</code>,
<code>bigquery_defaults</code>
</p>
<p>Other Spark serialization routines: <code>spark_write_bigquery</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
config &lt;- spark_config()

sc &lt;- spark_connect(master = "local", config = config)

bigquery_defaults(
  billingProjectId = "&lt;your_billing_project_id&gt;",
  gcsBucket = "&lt;your_gcs_bucket&gt;",
  datasetLocation = "US",
  serviceAccountKeyFile = "&lt;your_service_account_key_file&gt;",
  type = "direct")

# Reading the public shakespeare data table
# https://cloud.google.com/bigquery/public-data/
# https://cloud.google.com/bigquery/sample-tables
shakespeare &lt;-
  spark_read_bigquery(
    sc,
    name = "shakespeare",
    projectId = "bigquery-public-data",
    datasetId = "samples",
    tableId = "shakespeare")

## End(Not run)
</code></pre>


</div>