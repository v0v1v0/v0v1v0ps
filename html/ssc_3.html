<div class="container">

<table style="width: 100%;"><tr>
<td>coBCG</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>CoBC generic method</h2>

<h3>Description</h3>

<p>CoBC is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains <code>N</code> classifiers with the learning scheme defined in 
<code>gen.learner</code> using a reduced set of labeled examples. For each iteration, an unlabeled
example is labeled for a classifier if the most confident classifications assigned by the 
other <code>N-1</code> classifiers agree on the labeling proposed. The unlabeled examples 
candidates are selected randomly from a pool of size <code>u</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">coBCG(y, gen.learner, gen.pred, N = 3, perc.full = 0.7, u = 100,
  max.iter = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gen.learner</code></td>
<td>
<p>A function for training <code>N</code> supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gen.pred</code></td>
<td>
<p>A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>The number of classifiers used as committee members. All these classifiers 
are trained using the <code>gen.learner</code> function. Default is 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>Number of unlabeled instances in the pool. Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>Maximum number of iterations to execute in the self-labeling process. 
Default is 50.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>coBCG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general coBC method,
please see <code>coBC</code> function. Essentially, <code>coBC</code>
function is a wrapper of <code>coBCG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class "coBCG" containing:
</p>

<dl>
<dt>model</dt>
<dd>
<p>The final <code>N</code> base classifiers trained using the enlarged labeled set.</p>
</dd>
<dt>model.index</dt>
<dd>
<p>List of <code>N</code> vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt>
<dd>
<p>The indexes of all training instances used to
train the <code>N</code> models. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt>
<dd>
<p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt>
<dd>
<p>The levels of <code>y</code> factor.</p>
</dd>
</dl>
<h3>Examples</h3>

<pre><code class="language-R">library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
gen.learner1 &lt;- function(indexes, cls)
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
gen.pred1 &lt;- function(model, indexes)
  predict(model, xtrain[indexes, ]) 

set.seed(1)
md1 &lt;- coBCG(y = ytrain, gen.learner1, gen.pred1)

# Predict probabilities per instances using each model
h.prob &lt;- lapply(
  X = md1$model, 
  FUN = function(m) predict(m, xitest)
)
# Combine the predictions
cls1 &lt;- coBCCombine(h.prob, md1$classes)
table(cls1, yitest)

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
gen.learner2 &lt;- function(indexes, cls) {
  m &lt;- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") &lt;- indexes
  m
}

gen.pred2 &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  d &lt;- dtrain[indexes, tra.idxs]
  prob &lt;- predict(model, d, distance.weighting = "none")
  prob
}

set.seed(1)
md2 &lt;- coBCG(y = ytrain, gen.learner2, gen.pred2)

# Predict probabilities per instances using each model
ditest &lt;- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

h.prob &lt;- list()
ninstances &lt;- nrow(dtrain)
for(i in 1:length(md2$model)){
  m &lt;- md2$model[[i]]
  D &lt;- ditest[, md2$model.index.map[[i]]]
  h.prob[[i]] &lt;- predict(m, D)
}
# Combine the predictions
cls2 &lt;- coBCCombine(h.prob, md2$classes)
table(cls2, yitest)

</code></pre>


</div>