<div class="container">

<table style="width: 100%;"><tr>
<td>cpernet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Regularization paths for the coupled sparse asymmetric least squares
(COSALES) regression (or the coupled sparse expectile regression)</h2>

<h3>Description</h3>

<p>Fits regularization paths for coupled sparse asymmetric least squares
regression at a sequence of regularization parameters.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cpernet(
  x,
  y,
  w = 1,
  nlambda = 100L,
  method = "cper",
  lambda.factor = ifelse(2 * nobs &lt; nvars, 0.01, 1e-04),
  lambda = NULL,
  lambda2 = 0,
  pf.mean = rep(1, nvars),
  pf2.mean = rep(1, nvars),
  pf.scale = rep(1, nvars),
  pf2.scale = rep(1, nvars),
  exclude,
  dfmax = nvars + 1,
  pmax = min(dfmax * 1.2, nvars),
  standardize = TRUE,
  intercept = TRUE,
  eps = 1e-08,
  maxit = 1000000L,
  tau = 0.8
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>matrix of predictors, of dimension (nobs * nvars); each row is an
observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>weight applied to the asymmetric squared error loss of the mean
part. See details. Default is 1.0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>the number of <code>lambda</code> values (default is 100).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string specifying the loss function to use. Only
<code>cper</code> is available now.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.factor</code></td>
<td>
<p>The factor for getting the minimal lambda in the
<code>lambda</code> sequence, where we set <code>min(lambda)</code> =
<code>lambda.factor</code> * <code>max(lambda)</code> with <code>max(lambda)</code> being the
smallest value of <code>lambda</code> that penalizes all coefficients to zero.
The default value depends on the relationship between <code class="reqn">N</code> (the number
of observations) and <code class="reqn">p</code> (the number of predictors). If <code class="reqn">N &lt; p</code>,
the default is <code>0.01</code>. If <code class="reqn">N &gt; p</code>, the default is <code>0.0001</code>,
closer to zero. A very small value of <code>lambda.factor</code> will lead to a
saturated fit. The argument takes no effect if there is a user-supplied
<code>lambda</code> sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a user-supplied <code>lambda</code> sequence. Typically, by leaving
this option unspecified users can have the program compute its own
<code>lambda</code> sequence based on <code>nlambda</code> and <code>lambda.factor</code>. It
is better to supply, if necessary, a decreasing sequence of <code>lambda</code>
values than a single (small) value. The program will ensure that the
user-supplied <code>lambda</code> sequence is sorted in decreasing order.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>
<p>regularization parameter <code>lambda2</code> for the quadratic
penalty of the coefficients. Default is 0, meaning no L2 penalization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf.mean, pf.scale</code></td>
<td>
<p>L1 penalty factor of length <code class="reqn">p</code> used for adaptive
LASSO or adaptive elastic net. Separate L1 penalty weights can be applied
to each mean or scale coefficient to allow different L1 shrinkage. Can be 0
for some variables, which imposes no shrinkage and results in that variable
being always included in the model. Default is 1 for all variables (and
implicitly infinity for variables listed in <code>exclude</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf2.mean, pf2.scale</code></td>
<td>
<p>L2 penalty factor of length <code class="reqn">p</code> used for
adaptive elastic net. Separate L2 penalty weights can be applied to each
mean or scale coefficient to allow different L2 shrinkage. Can be 0 for
some variables, which imposes no shrinkage. Default is 1 for all variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>indices of variables to be excluded from the model. Default is
none. Equivalent to an infinite penalty factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>limit the maximum number of variables in the model. Useful for
very large <code class="reqn">p</code>, if a partial path is desired. Default is <code class="reqn">p+1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>limit the maximum number of variables ever to be nonzero. For
example once <code class="reqn">\beta</code> enters the model, no matter how many times it
exits or re-enters the model through the path, it will be counted only
once. Default is <code>min(dfmax*1.2, p)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>logical flag for variable standardization, prior to
fitting the model sequence. The coefficients are always returned to the
original scale. Default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Should intercept(s) be fitted (default=TRUE) or set to zero
(FALSE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>convergence threshold for coordinate descent. Each inner
coordinate descent loop continues until the maximum change in any
coefficient is less than <code>eps</code>. Defaults value is <code>1e-8</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of outer-loop iterations allowed at fixed lambda
values. Default is 1e7. If the algorithm does not converge, consider
increasing <code>maxit</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>the parameter <code>tau</code> in the coupled ALS regression model. The
value must be in (0,1) and cannot be 0.5. Default is 0.8.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that the objective function in <code>cpernet</code> is
</p>
<p style="text-align: center;"><code class="reqn">w*1'\Psi(y-X\beta,0.5)/N + 1'\Psi(y-X\beta-X\theta,\tau)/N +
  \lambda_1*\Vert\beta\Vert_1 + 0.5\lambda_2\Vert\beta\Vert_2^2 +
  \mu_1*\Vert\theta\Vert +
  0.5\mu_2\Vert\theta\Vert_2^2,</code>
</p>
<p> where
<code class="reqn">\Psi(u,\tau)=|\tau-I(u&lt;0)|*u^2</code> denotes the asymmetric squared error
loss and the penalty is a combination of L1 and L2 terms for both the mean
and scale coefficients.
</p>
<p>For faster computation, if the algorithm is not converging or running slow,
consider increasing <code>eps</code>, decreasing <code>nlambda</code>, or increasing
<code>lambda.factor</code> before increasing <code>maxit</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>cpernet</code>. </p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call
that produced this object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0, t0</code></td>
<td>
<p>intercept sequences both of
length <code>length(lambda)</code> for the mean and scale respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta, theta</code></td>
<td>
<p><code>p*length(lambda)</code> matrices of coefficients for the
mean and scale respectively, stored as sparse matrices (<code>dgCMatrix</code>
class, the standard class for sparse numeric matrices in the <code>Matrix</code>
package). To convert them into normal R matrices, use <code>as.matrix()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df.beta, df.theta</code></td>
<td>
<p>the number of nonzero mean and scale coefficients
respectively for each value of <code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimensions of
coefficient matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>npasses</code></td>
<td>
<p>total number of iterations summed
over all lambda values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jerr</code></td>
<td>
<p>error flag, for warnings and errors, 0
if no error.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yuwen Gu and Hui Zou<br></p>
<p>Maintainer: Yuwen Gu &lt;yuwen.gu@uconn.edu&gt;
</p>


<h3>References</h3>

<p>Gu, Y., and Zou, H. (2016).
"High-dimensional generalizations of asymmetric least squares regression and their applications."
<em>The Annals of Statistics</em>, 44(6), 2661â€“2694.<br></p>


<h3>See Also</h3>

<p><code>plot.cpernet</code>, <code>coef.cpernet</code>,
<code>predict.cpernet</code>, <code>print.cpernet</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
set.seed(1)
n &lt;- 100
p &lt;- 400
x &lt;- matrix(rnorm(n * p), n, p)
y &lt;- rnorm(n)
tau &lt;- 0.30
pf &lt;- abs(rnorm(p))
pf2 &lt;- abs(rnorm(p))
w &lt;- 2.0
lambda2 &lt;- 1
m2 &lt;- cpernet(y = y, x = x, w = w, tau = tau, eps = 1e-8,
              pf.mean = pf, pf.scale = pf2,
              standardize = FALSE, lambda2 = lambda2)

</code></pre>


</div>