<div class="container">

<table style="width: 100%;"><tr>
<td>sclr_fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fitter function for the scaled logit model</h2>

<h3>Description</h3>

<p>Computing engine behind <code>sclr</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sclr_fit(
  y,
  x,
  tol = 10^(-7),
  algorithm = c("newton-raphson", "gradient-ascent"),
  nr_iter = 2000,
  ga_iter = 2000,
  n_conv = 3,
  conventional_names = FALSE,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A design matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Tolerance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>algorithm</code></td>
<td>
<p>Algorithms to run. "newton-raphson" or "gradient-ascent".
If a character vector, the algorithms will be applied in the order they
are present in the vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nr_iter</code></td>
<td>
<p>Maximum allowed iterations for Newton-Raphson.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ga_iter</code></td>
<td>
<p>Maximum allowed iterations for gradient ascent.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_conv</code></td>
<td>
<p>Number of times the algorithm has to converge (to work around
local maxima).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conventional_names</code></td>
<td>
<p>If <code>TRUE</code>, estimated parameter names will be
(Baseline), (Intercept) and the column names in the model matrix. Otherwise
- lambda, beta_0 and beta_ prefix in front of column names in the model
matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Seed for the algorithms.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The likelihood maximisation can use the Newton-Raphson or the gradient
ascent algorithms.
</p>


</div>