<div class="container">

<table style="width: 100%;"><tr>
<td>rspca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Randomized Sparse Principal Component Analysis (rspca).</h2>

<h3>Description</h3>

<p>Randomized accelerated implementation of SPCA, using variable projection as an optimization strategy.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rspca(X, k = NULL, alpha = 1e-04, beta = 1e-04, center = TRUE,
  scale = FALSE, max_iter = 1000, tol = 1e-05, o = 20, q = 2,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>array_like; <br>
a real <code class="reqn">(n, p)</code> input matrix (or data frame) to be decomposed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>integer; <br>
specifies the target rank, i.e., the number of components to be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>float; <br>
Sparsity controlling parameter. Higher values lead to sparser components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>float; <br>
Amount of ridge shrinkage to apply in order to improve conditioning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>bool; <br>
logical value which indicates whether the variables should be
shifted to be zero centered (TRUE by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>bool; <br>
logical value which indicates whether the variables should
be scaled to have unit variance (FALSE by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>integer; <br>
maximum number of iterations to perform before exiting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>float; <br>
stopping tolerance for the convergence criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>o</code></td>
<td>
<p>integer; <br>
oversampling parameter (default <code class="reqn">o=20</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>integer; <br>
number of additional power iterations (default <code class="reqn">q=2</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>bool; <br>
logical value which indicates whether progress is printed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Sparse principal component analysis is a modern variant of PCA. Specifically, SPCA attempts to find sparse
weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach
leads to an improved interpretability of the model, because the principal components are formed as a
linear combination of only a few of the original variables. Further, SPCA avoids overfitting in a
high-dimensional data setting where the number of variables <code class="reqn">p</code> is greater than the number of
observations <code class="reqn">n</code>.
</p>
<p>Such a parsimonious model is obtained by introducing prior information like sparsity promoting regularizers.
More concreatly, given an <code class="reqn">(n,p)</code> data matrix <code class="reqn">X</code>, SPCA attemps to minimize the following
objective function:
</p>
<p style="text-align: center;"><code class="reqn"> f(A,B) = \frac{1}{2} \| X - X B A^\top \|^2_F + \psi(B) </code>
</p>

<p>where <code class="reqn">B</code> is the sparse weight (loadings) matrix and <code class="reqn">A</code> is an orthonormal matrix.
<code class="reqn">\psi</code> denotes a sparsity inducing regularizer such as the LASSO (<code class="reqn">\ell_1</code> norm) or the elastic net
(a combination of the <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> norm). The principal components <code class="reqn">Z</code> are formed as
</p>
<p style="text-align: center;"><code class="reqn"> Z = X B </code>
</p>

<p>and the data can be approximately rotated back as
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{X} = Z A^\top </code>
</p>

<p>The print and summary method can be used to present the results in a nice format.
</p>


<h3>Value</h3>

<p><code>spca</code> returns a list containing the following three components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>loadings</code></td>
<td>
<p>  array_like; <br>
sparse loadings (weight) vector;  <code class="reqn">(p, k)</code> dimensional array.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transform</code></td>
<td>
<p>  array_like; <br>
the approximated inverse transform; <code class="reqn">(p, k)</code> dimensional array.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scores</code></td>
<td>
<p>  array_like; <br>
the principal component scores; <code class="reqn">(n, k)</code> dimensional array.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eigenvalues</code></td>
<td>
<p>  array_like; <br>
the approximated eigenvalues; <code class="reqn">(k)</code> dimensional array.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center, scale</code></td>
<td>
<p>  array_like; <br>
the centering and scaling used.
</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>This implementation uses randomized methods for linear algebra to speedup the computations.
<code class="reqn">o</code> is an oversampling parameter to improve the approximation.
A value of at least 10 is recommended, and <code class="reqn">o=20</code> is set by default.
</p>
<p>The parameter <code class="reqn">q</code> specifies the number of power (subspace) iterations
to reduce the approximation error. The power scheme is recommended,
if the singular values decay slowly. In practice, 2 or 3 iterations
achieve good results, however, computing power iterations increases the
computational costs. The power scheme is set to <code class="reqn">q=2</code> by default.
</p>
<p>If <code class="reqn">k &gt; (min(n,p)/4)</code>, a the deterministic <code>spca</code>
algorithm might be faster.
</p>


<h3>Author(s)</h3>

<p>N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin
</p>


<h3>References</h3>


<ul>
<li>
<p> [1] N. B. Erichson, P. Zheng, K. Manohar, S. Brunton, J. N. Kutz, A. Y. Aravkin.
"Sparse Principal Component Analysis via Variable Projection."
Submitted to IEEE Journal of Selected Topics on Signal Processing (2018).
(available at 'arXiv <a href="https://arxiv.org/abs/1804.00341">https://arxiv.org/abs/1804.00341</a>).
</p>
</li>
<li>
<p>  [1] N. B. Erichson, S. Voronin, S. Brunton, J. N. Kutz.
"Randomized matrix decompositions using R."
Submitted to Journal of Statistical Software (2016).
(available at 'arXiv <a href="http://arxiv.org/abs/1608.02148">http://arxiv.org/abs/1608.02148</a>).
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>spca</code>, <code>robspca</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Create artifical data
m &lt;- 10000
V1 &lt;- rnorm(m, 0, 290)
V2 &lt;- rnorm(m, 0, 300)
V3 &lt;- -0.1*V1 + 0.1*V2 + rnorm(m,0,100)

X &lt;- cbind(V1,V1,V1,V1, V2,V2,V2,V2, V3,V3)
X &lt;- X + matrix(rnorm(length(X),0,1), ncol = ncol(X), nrow = nrow(X))

# Compute SPCA
out &lt;- rspca(X, k=3, alpha=1e-3, beta=1e-3, center = TRUE, scale = FALSE, verbose=0)
print(out)
summary(out)

</code></pre>


</div>