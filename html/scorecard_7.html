<div class="container">

<table style="width: 100%;"><tr>
<td>perf_eva</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Binomial Metrics</h2>

<h3>Description</h3>

<p><code>perf_eva</code> calculates metrics to evaluate the performance of binomial classification model. It can also creates confusion matrix and model performance graphics.
</p>


<h3>Usage</h3>

<pre><code class="language-R">perf_eva(pred, label, title = NULL, binomial_metric = c("mse", "rmse",
  "logloss", "r2", "ks", "auc", "gini"), confusion_matrix = FALSE,
  threshold = NULL, show_plot = c("ks", "lift"), pred_desc = TRUE,
  positive = "bad|1", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>A list or vector of predicted probability or score.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label</code></td>
<td>
<p>A list or vector of label values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>title</code></td>
<td>
<p>The title of plot. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>binomial_metric</code></td>
<td>
<p>Defaults to c('mse', 'rmse', 'logloss', 'r2', 'ks', 'auc', 'gini'). If it is NULL, then no metric will calculated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>confusion_matrix</code></td>
<td>
<p>Logical, whether to create a confusion matrix. Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>Confusion matrix threshold. Defaults to the pred on maximum F1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_plot</code></td>
<td>
<p>Defaults to c('ks', 'roc'). Accepted values including c('ks', 'lift', 'gain', 'roc', 'lz', 'pr', 'f1', 'density').</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred_desc</code></td>
<td>
<p>whether to sort the argument of pred in descending order. Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>Value of positive class. Defaults to "bad|1".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional parameters.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Accuracy = true positive and true negative/total cases
</p>
<p>Error rate = false positive and false negative/total cases
</p>
<p>TPR, True Positive Rate(Recall or Sensitivity) = true positive/total actual positive
</p>
<p>PPV, Positive Predicted Value(Precision) = true positive/total predicted positive
</p>
<p>TNR, True Negative Rate(Specificity) = true negative/total actual negative = 1-FPR
</p>
<p>NPV, Negative Predicted Value = true negative/total predicted negative
</p>


<h3>Value</h3>

<p>A list of binomial metric, confusion matrix and graphics
</p>


<h3>See Also</h3>

<p><code>perf_psi</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# load germancredit data
data("germancredit")
# filter variable via missing rate, iv, identical value rate
dtvf = var_filter(germancredit, "creditability")

# breaking dt into train and test
dt_list = split_df(dtvf, "creditability")
label_list = lapply(dt_list, function(x) x$creditability)

# woe binning
bins = woebin(dt_list$train, "creditability")
# scorecard, prob
cardprob = scorecard2(bins, dt = dt_list, y = 'creditability', return_prob = TRUE)

# credit score
score_list = lapply(dt_list, function(x) scorecard_ply(x, cardprob$card))

###### perf_eva examples ######
# Example I, one datset
## predicted p1
perf_eva(pred = cardprob$prob$train, label=label_list$train,
         title = 'train')
## predicted score
# perf_eva(pred = score_list$train, label=label_list$train,
#   title = 'train')

# Example II, multiple datsets
## predicted p1
perf_eva(pred = cardprob$prob, label = label_list,
         show_plot = c('ks', 'lift', 'gain', 'roc', 'lz', 'pr', 'f1', 'density'))
## predicted score
# perf_eva(score_list, label_list)



</code></pre>


</div>