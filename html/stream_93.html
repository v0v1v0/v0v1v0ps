<div class="container">

<table style="width: 100%;"><tr>
<td>evaluate.DSC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate a Stream Clustering Task</h2>

<h3>Description</h3>

<p>Methods for the generic functions <code>evaluate_static()</code> and <code>evaluate_stream()</code>  to calculate evaluation measures for micro or macro-clusters created by a DSC on the
a DSD object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'DSC'
evaluate_static(
  object,
  dsd,
  measure,
  n = 100,
  type = c("auto", "micro", "macro"),
  assign = "micro",
  assignmentMethod = c("auto", "model", "nn"),
  excludeNoise = FALSE,
  callbacks = list(),
  ...
)

## S3 method for class 'DSC'
evaluate_stream(
  object,
  dsd,
  measure,
  n = 1000,
  horizon = 100,
  type = c("auto", "micro", "macro"),
  assign = "micro",
  assignmentMethod = c("auto", "model", "nn"),
  excludeNoise = FALSE,
  callbacks = NULL,
  ...,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>The DSC object that the evaluation measure is being requested
from.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dsd</code></td>
<td>
<p>The DSD object that holds the initial training data for the DSC.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>Evaluation measure(s) to use. If missing then all available
measures are returned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>The number of data points being requested.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Use micro- or macro-clusters for evaluation. Auto used the class
of DSC to decide.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>assign</code></td>
<td>
<p>Assign points to micro or macro-clusters?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>assignmentMethod</code></td>
<td>
<p>How are points assigned to clusters for evaluation
(see <code>predict()</code>)?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>excludeNoise</code></td>
<td>
<p>logical; Should noise points in the data stream be excluded from
the calculation?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>callbacks</code></td>
<td>
<p>A named list of functions to calculate custom evaluation measures.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Unused arguments are ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>horizon</code></td>
<td>
<p>Evaluation is done using horizon many previous points (see
detail section).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>logical; Report progress?</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For evaluation, each data point is assigned to its nearest cluster using
Euclidean distance to the cluster centers. Then for each cluster the
majority class is determined. Based on the majority class several evaluation
measures can be computed.
</p>
<p>We provide two evaluation methods:
</p>

<ul>
<li> <p><code>evaluate_static()</code> evaluates the current static clustering using new data without updating the model.
</p>
</li>
<li> <p><code>evaluate_stream()</code> evaluates the clustering process using
<em>prequential error estimation</em> (see Gama, Sebastiao and Rodrigues; 2013).  The current model is
first applied to the data points in the horizon to calculate the evaluation measures. Then, the
cluster model is updated with the points.
</p>
</li>
</ul>
<p><strong>Evaluation Measures</strong>
</p>
<p>Many evaluation measures are available using
code from other packages including <code>cluster::silhouette()</code>,
<code>clue:: cl_agreement()</code>, and <code>fpc::cluster.stats()</code>.
</p>
<p>The following information items are available:
</p>

<ul>
<li> <p><code>"numPoints"</code> number of points used for evaluation.
</p>
</li>
<li> <p><code>"numMicroClusters"</code> number of micro-clusters
</p>
</li>
<li> <p><code>"numMacroClusters"</code> number of macro-clusters
</p>
</li>
<li> <p><code>"numClasses"</code> number of classes
</p>
</li>
</ul>
<p>The following noise-related/outlier items are available:
</p>

<ul>
<li> <p><code>"noisePredicted"</code> Number data points predicted as noise
</p>
</li>
<li> <p><code>"noiseActual"</code> Number of data points which are actually noise
</p>
</li>
<li> <p><code>"noisePrecision"</code> Precision of the predicting noise (i.e., number of
correctly predicted noise points over the total number of points predicted
as noise)
</p>
</li>
<li> <p><code>"outlierJaccard"</code> - A variant of the Jaccard index used to assess
outlier detection accuracy (see Krleza et al (2020)).  Outlier Jaccard index
is calculated as <code>TP / (TP + FP + UNDETECTED)</code>.
</p>
</li>
</ul>
<p>The following internal evaluation measures are available:
</p>

<ul>
<li> <p><code>"SSQ"</code> within cluster sum of squares. Assigns each point to
its nearest center from the clustering and calculates the sum of squares.
Noise points in the data stream are always ignored.
</p>
</li>
<li> <p><code>"silhouette"</code> average silhouette width. Actual noise points
which stay unassigned by the clustering algorithm are ignored; regular
points that are unassigned by the clustering algorithm form their own
noise cluster) (<span class="pkg">cluster</span>)
</p>
</li>
<li> <p><code>"average.between"</code> average distance between clusters (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"average.within"</code> average distance within clusters (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"max.diameter"</code> maximum cluster diameter (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"min.separation"</code> minimum cluster separation (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"ave.within.cluster.ss"</code> a generalization
of the within clusters sum of squares (half the sum of the within cluster
squared dissimilarities divided by the cluster size) (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"g2"</code> Goodman and Kruskal's Gamma coefficient (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"pearsongamma"</code> correlation between distances and a 0-1-vector where 0
means same cluster, 1 means different clusters (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"dunn"</code> Dunn index (minimum separation / maximum diameter) (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"dunn2"</code> minimum average dissimilarity between two cluster /
maximum average within cluster dissimilarity (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"entropy"</code> entropy of the distribution of cluster memberships (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"wb.ratio"</code> average.within/average.between (<span class="pkg">fpc</span>)
</p>
</li>
</ul>
<p>The following external evaluation measures are available:
</p>

<ul>
<li> <p><code>"precision"</code>, <code>"recall"</code>, <code>"F1"</code> F1.  A true positive (TP)
decision assigns two points in the same true cluster also to the same
cluster, a true negative (TN) decision assigns two points from two different
true clusters to two different clusters.  A false positive (FP) decision
assigns two points from the same true cluster to two different clusters.  A
false negative (FN) decision assigns two points from the same true cluster
to different clusters.
</p>
<p><code>precision = TP / (TP + FP)</code>
</p>
<p><code>recall = TP / (TP + FN)</code>
</p>
<p>The F1 measure is the harmonic mean of precision and recall.
</p>
</li>
<li> <p><code>"purity"</code> Average purity of clusters. The purity of each cluster
is the proportion of the points of the majority true group assigned to it
(see Cao et al. (2006)).
</p>
</li>
<li> <p><code>"classPurity"</code> (of real clusters; see Wan et al (2009)).
</p>
</li>
<li> <p><code>"fpr"</code> false positive rate.
</p>
</li>
<li> <p><code>"Euclidean"</code> Euclidean dissimilarity of the memberships (see
Dimitriadou, Weingessel and Hornik (2002)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"Manhattan"</code> Manhattan dissimilarity of the memberships (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"Rand"</code> Rand index (see Rand (1971)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"cRand"</code> Adjusted Rand index (see Hubert and Arabie (1985)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"NMI"</code> Normalized Mutual Information (see Strehl and Ghosh (2002)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"KP"</code> Katz-Powell index (see Katz and Powell (1953)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"angle"</code> maximal cosine of the angle between the agreements (<span class="pkg">clue</span>)
-<code> "diag"</code> maximal co-classification rate (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"FM"</code> Fowlkes and Mallows's index (see Fowlkes and Mallows (1983)) (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"Jaccard"</code> Jaccard index (<span class="pkg">clue</span>)
</p>
</li>
<li> <p><code>"PS"</code> Prediction Strength (see Tibshirani and Walter (2005)) (<span class="pkg">clue</span>) %
</p>
</li>
<li> <p><code>"corrected.rand"</code>  corrected Rand index (<span class="pkg">fpc</span>)
</p>
</li>
<li> <p><code>"vi"</code> variation of information (VI) index (<span class="pkg">fpc</span>)
</p>
</li>
</ul>
<p>Many measures are the average over all clusters. For example, purity is the
average purity over all clusters.
</p>
<p>For DSC_Micro objects, data points are assigned to micro-clusters and
then each micro-cluster is evaluated. For DSC_Macro objects, data
points by default (<code>assign = "micro"</code>) also assigned to micro-clusters,
but these assignments are translated to macro-clusters. The evaluation is
here done for macro-clusters. This is important when macro-clustering is
done with algorithms which do not create spherical clusters (e.g,
hierarchical clustering with single-linkage or DBSCAN) and this assignment
to the macro-clusters directly (i.e., their center) does not make sense.
</p>
<p>Using <code>type</code> and <code>assign</code>, the user can select how to assign data
points and ad what level (micro or macro) to evaluate.
</p>
<p><code>evaluate_cluster()</code> is used to evaluate an evolving data stream using
the method described by Wan et al. (2009). Of the <code>n</code> data points
<code>horizon</code> many points are clustered and then the evaluation measure is
calculated on the same data points. The idea is to find out if the
clustering algorithm was able to adapt to the changing stream.
</p>
<p><strong>Custom Evaluation Measures</strong>
</p>
<p>The parameter <code>callbacks</code> can be supplied with a named list with
functions with the signature <code style="white-space: pre;">⁠function(actual, predict, points, centers, dsc)⁠</code>
as elements. See the Examples sections for details.
</p>


<h3>Value</h3>

<p><code>evaluate</code> returns an object of class <code>stream_eval</code> which
is a numeric vector of the values of the requested measures and two
attributes, <code>"type"</code> and <code>"assign"</code>, to see at what level the
evaluation was done.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler, Matthew Bolanos, John Forrest, and Dalibor Krleža
</p>


<h3>References</h3>

<p>Joao Gama, Raquel Sebastiao, Pedro Pereira Rodrigues (2013). On
evaluating stream learning algorithms. <em>Machine Learning,</em> March 2013,
Volume 90, Issue 3, pp 317-346.
</p>
<p>F. Cao, M. Ester, W. Qian, A. Zhou (2006). Density-Based Clustering over an
Evolving Data Stream with Noise.
<em>Proceeding of the 2006 SIAM Conference on Data Mining,</em> 326-337.
</p>
<p>E. Dimitriadou, A. Weingessel and K. Hornik (2002).  A combination scheme
for fuzzy clustering.
<em>International Journal of Pattern Recognition and Artificial Intelligence,</em>
16, 901-912.
</p>
<p>E. B. Fowlkes and C. L. Mallows (1983).  A method for comparing two
hierarchical clusterings.
<em>Journal of the American Statistical Association,</em> 78, 553-569.
</p>
<p>L. Hubert and P. Arabie (1985).  Comparing partitions.
<em>Journal of Classification,</em> 2, 193-218.
</p>
<p>W. M. Rand (1971).  Objective criteria for the evaluation of clustering
methods.  <em>Journal of the American Statistical Association,</em> 66,
846-850.
</p>
<p>L. Katz and J. H. Powell (1953).  A proposed index of the conformity of one
sociometric measurement to another. <em>Psychometrika,</em> 18, 249-256.
</p>
<p>A. Strehl and J. Ghosh (2002).  Cluster ensembles - A knowledge reuse
framework for combining multiple partitions.
<em>Journal of Machine Learning Research,</em> 3, 583-617.
</p>
<p>R. Tibshirani and G. Walter (2005).  Cluster validation by Prediction
Strength. <em>Journal of Computational and Graphical Statistics,</em> 14/3,
511-528.
</p>
<p>L Wan, W.K. Ng, X.H. Dang, P.S. Yu and K. Zhang (2009). Density-Based
Clustering of Data Streams at Multiple Resolutions, <em>ACM Transactions
on Knowledge Discovery from Data,</em> 3(3).
</p>
<p>D. Krleža, B. Vrdoljak, and M. Brčić (2020). Statistical Hierarchical
Clustering Algorithm for Outlier Detection in Evolving Data Streams,
<em>Springer Machine Learning</em>.
</p>


<h3>See Also</h3>

<p><code>cluster::silhouette()</code>, <code>clue:: cl_agreement()</code>, and <code>fpc::cluster.stats()</code>.
</p>
<p>Other DSC: 
<code>DSC()</code>,
<code>DSC_Macro()</code>,
<code>DSC_Micro()</code>,
<code>DSC_R()</code>,
<code>DSC_SlidingWindow()</code>,
<code>DSC_Static()</code>,
<code>DSC_TwoStage()</code>,
<code>animate_cluster()</code>,
<code>get_assignment()</code>,
<code>plot.DSC()</code>,
<code>predict()</code>,
<code>prune_clusters()</code>,
<code>read_saveDSC</code>,
<code>recluster()</code>
</p>
<p>Other evaluation: 
<code>animate_cluster()</code>,
<code>evaluate</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Example 1: Static Evaluation
set.seed(0)
stream &lt;- DSD_Gaussians(k = 3, d = 2)

dstream &lt;- DSC_DStream(gridsize = 0.05, Cm = 1.5)
update(dstream, stream, 500)
plot(dstream, stream)

# Evaluate the micro-clusters in the clustering
# Note: we use here only n = 100 points for evaluation to speed up execution
evaluate_static(dstream, stream, n = 100)

evaluate_static(dstream, stream,
  measure = c("numMicro", "numMacro", "purity", "crand", "SSQ"),
  n = 100)

# DStream also provides macro clusters. Evaluate macro clusters with type = "macro"
# Note that SSQ and cRand increase.
plot(dstream, stream, type = "macro")
evaluate_static(dstream, stream, type = "macro",
  measure = c("numMicro", "numMacro", "purity", "crand", "SSQ"),
  n = 100)

# Points are by default assigned to micro clusters using the method
# specified for the clustering algorithm.
# However, points can also be assigned to the closest macro-cluster using
# assign = "macro".
evaluate_static(dstream, stream, type = "macro", assign = "macro",
  measure = c("numMicro", "numMacro", "purity", "crand", "SSQ"),
  n = 100)

# Example 2: Evaluate with Noise/Outliers
stream &lt;- DSD_Gaussians(k = 3, d = 2, noise = .05)
dstream &lt;- DSC_DStream(gridsize = 0.05, Cm = 1.5)
update(dstream, stream, 500)

# For cRand, noise is its own group, for SSQ, actual noise is always
# excluded.
plot(dstream, stream, 500)
evaluate_static(dstream, stream, n = 100,
  measure = c("numPoints", "noisePredicted", "noiseActual",
    "noisePrecision", "outlierJaccard", "cRand", "SSQ"))

# Note that if noise is excluded, the number of used points is reduced.
evaluate_static(dstream, stream, n = 100,
  measure = c("numPoints", "noisePredicted", "noiseActual",
    "noisePrecision", "outlierJaccard", "cRand", "SSQ"), excludeNoise = TRUE)


# Example 3: Evaluate an evolving data stream
stream &lt;- DSD_Benchmark(1)
dstream &lt;- DSC_DStream(gridsize = 0.05, lambda = 0.1)

evaluate_stream(dstream, stream, type = "macro", assign = "micro",
  measure = c("numMicro", "numMacro", "purity", "cRand"),
  n = 600, horizon = 100)

if (interactive()){
# animate the clustering process
reset_stream(stream)
dstream &lt;- DSC_DStream(gridsize = 0.05, lambda = 0.1)
animate_cluster(dstream, stream, horizon = 100, n = 5000,
  measure = "cRand", type = "macro", assign = "micro",
  plot.args = list(type = "both", xlim = c(0,1), ylim = c(0,1)))
}

# Example 4: Add a custom measure as a callback
callbacks &lt;- list(
   noisePercentage = function(actual, predict, points, centers, dsc) {
     sum(actual == 0L) / length(actual)
   },
   noiseFN = function(actual, predict, points, centers, dsc) {
     sum(actual == 0L &amp; predict != 0L)
   },
   noiseFP = function(actual, predict, points, centers, dsc) {
     sum(actual != 0L &amp; predict == 0L)
   }
 )

stream &lt;- DSD_Gaussians(k = 3, d = 2, noise = .2)
dstream &lt;- DSC_DStream(gridsize = 0.05, Cm = 1.5)
update(dstream, stream, 500)

evaluate_static(dstream, stream,
  measure = c("numPoints", "noiseActual", "noisePredicted",
    "noisePercentage", "noiseFN", "noiseFP"),
  callbacks = callbacks, n = 100)

evaluate_static(dstream, stream, callbacks = callbacks)
</code></pre>


</div>