<div class="container">

<table style="width: 100%;"><tr>
<td>learn_k_component_graph</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Learn the Laplacian matrix of a k-component graph
Learns a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.</h2>

<h3>Description</h3>

<p>Learn the Laplacian matrix of a k-component graph
</p>
<p>Learns a k-component graph on the basis of an observed data matrix.
Check out https://mirca.github.io/spectralGraphTopology for code examples.
</p>


<h3>Usage</h3>

<pre><code class="language-R">learn_k_component_graph(
  S,
  is_data_matrix = FALSE,
  k = 1,
  w0 = "naive",
  lb = 0,
  ub = 10000,
  alpha = 0,
  beta = 10000,
  beta_max = 1e+06,
  fix_beta = TRUE,
  rho = 0.01,
  m = 7,
  eps = 1e-04,
  maxiter = 10000,
  abstol = 1e-06,
  reltol = 1e-04,
  eigtol = 1e-09,
  record_objective = FALSE,
  record_weights = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>either a pxp sample covariance/correlation matrix, or a pxn data
matrix, where p is the number of nodes and n is the number of
features (or data points per node)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>is_data_matrix</code></td>
<td>
<p>whether the matrix S should be treated as data matrix
or sample covariance matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of components of the graph</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w0</code></td>
<td>
<p>initial estimate for the weight vector the graph or a string
selecting an appropriate method. Available methods are: "qp": finds w0 that minimizes
||ginv(S) - L(w0)||_F, w0 &gt;= 0; "naive": takes w0 as the negative of the
off-diagonal elements of the pseudo inverse, setting to 0 any elements s.t.
w0 &lt; 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lb</code></td>
<td>
<p>lower bound for the eigenvalues of the Laplacian matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ub</code></td>
<td>
<p>upper bound for the eigenvalues of the Laplacian matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>reweighted l1-norm regularization hyperparameter</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>regularization hyperparameter for the term ||L(w) - U Lambda U'||^2_F</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_max</code></td>
<td>
<p>maximum allowed value for beta</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fix_beta</code></td>
<td>
<p>whether or not to fix the value of beta. In case this parameter
is set to false, then beta will increase (decrease) depending whether the number of
zero eigenvalues is lesser (greater) than k</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>how much to increase (decrease) beta in case fix_beta = FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>in case is_data_matrix = TRUE, then we build an affinity matrix based
on Nie et. al. 2017, where m is the maximum number of possible connections
for a given node</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>small positive constant</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>the maximum number of iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>abstol</code></td>
<td>
<p>absolute tolerance on the weight vector w</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reltol</code></td>
<td>
<p>relative tolerance on the weight vector w</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eigtol</code></td>
<td>
<p>value below which eigenvalues are considered to be zero</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>record_objective</code></td>
<td>
<p>whether to record the objective function values at
each iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>record_weights</code></td>
<td>
<p>whether to record the edge values at each iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>whether to output a progress bar showing the evolution of the
iterations</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list containing possibly the following elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>laplacian</code></td>
<td>
<p>the estimated Laplacian Matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adjacency</code></td>
<td>
<p>the estimated Adjacency Matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>the estimated weight vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>optimization variable accounting for the eigenvalues of the Laplacian matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>eigenvectors of the estimated Laplacian matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>elapsed_time</code></td>
<td>
<p>elapsed time recorded at every iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_seq</code></td>
<td>
<p>sequence of values taken by beta in case fix_beta = FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convergence</code></td>
<td>
<p>boolean flag to indicate whether or not the optimization converged</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj_fun</code></td>
<td>
<p>values of the objective function at every iteration in case record_objective = TRUE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negloglike</code></td>
<td>
<p>values of the negative loglikelihood at every iteration in case record_objective = TRUE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_seq</code></td>
<td>
<p>sequence of weight vectors at every iteration in case record_weights = TRUE</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Ze Vinicius and Daniel Palomar
</p>


<h3>References</h3>

<p>S. Kumar, J. Ying, J. V. M. Cardoso, D. P. Palomar. A unified
framework for structured graph learning via spectral constraints.
Journal of Machine Learning Research, 2020.
http://jmlr.org/papers/v21/19-276.html
</p>


<h3>Examples</h3>

<pre><code class="language-R"># design true Laplacian
Laplacian &lt;- rbind(c(1, -1, 0, 0),
                   c(-1, 1, 0, 0),
                   c(0, 0, 1, -1),
                   c(0, 0, -1, 1))
n &lt;- ncol(Laplacian)
# sample data from multivariate Gaussian
Y &lt;- MASS::mvrnorm(n * 500, rep(0, n), MASS::ginv(Laplacian))
# estimate graph on the basis of sampled data
graph &lt;- learn_k_component_graph(cov(Y), k = 2, beta = 10)
graph$laplacian
</code></pre>


</div>