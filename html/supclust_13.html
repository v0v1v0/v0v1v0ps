<div class="container">

<table style="width: 100%;"><tr>
<td>dlda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Classification with Wilma's Clusters</h2>

<h3>Description</h3>

<p>The four functions <code>nnr</code> (nearest neighbor rule),
<code>dlda</code> (diagonal linear discriminant analysis), <code>logreg</code>
(logistic regression) and <code>aggtrees</code> (aggregated trees) are used
for binary classification with the cluster representatives of Wilma's
output.
</p>


<h3>Usage</h3>

<pre><code class="language-R">dlda    (xlearn, xtest, ylearn)
nnr     (xlearn, xtest, ylearn)
logreg  (xlearn, xtest, ylearn)
aggtrees(xlearn, xtest, ylearn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xlearn</code></td>
<td>
<p>Numeric matrix of explanatory variables (<code class="reqn">q</code>
variables in columns, <code class="reqn">n</code> cases in rows), containing the learning
or training data. Typically, these are the (gene) cluster
representatives of Wilma's output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtest</code></td>
<td>
<p>A numeric matrix of explanatory variables (<code class="reqn">q</code>
variables in columns, <code class="reqn">m</code> cases in rows), containing the test or
validation data. Typically, these are the fitted (gene) cluster
representatives of Wilma's output for the training data, obtained
from <code>predict.wilma</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ylearn</code></td>
<td>
<p>Numeric vector of length <code class="reqn">n</code> containing the class labels
for the training observations. These labels have to be coded by 0 and 1.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>nnr</code> implements the 1-nearest-neighbor-rule with
Euclidean distance function. <code>dlda</code> is linear discriminant
analysis, using the restriction that the covariance matrix is diagonal
with equal variance for all predictors. <code>logreg</code> is default
logistic regression. <code>aggtrees</code> fits a default stump (a
classification tree with two terminal nodes) by <code>rpart</code> for every
predictor variable and uses majority voting to determine the final
classifier.
</p>


<h3>Value</h3>

<p>Numeric vector of length <code class="reqn">m</code>, containing the predicted class
labels for the test observations. The class labels are coded by 0 and
1.</p>


<h3>Author(s)</h3>

<p>Marcel Dettling</p>


<h3>References</h3>

<p>see those in <code>wilma</code>.</p>


<h3>See Also</h3>

<p><code>wilma</code></p>


<h3>Examples</h3>

<pre><code class="language-R">## Generating random learning data: 20 observations and 10 variables (clusters)
set.seed(342)
xlearn &lt;- matrix(rnorm(200), nrow = 20, ncol = 10)

## Generating random test data: 8 observations and 10 variables(clusters)
xtest  &lt;- matrix(rnorm(80),  nrow = 8,  ncol = 10)

## Generating random class labels for the learning data
ylearn &lt;- as.numeric(runif(20)&gt;0.5)

## Predicting the class labels for the test data
nnr(xlearn, xtest, ylearn)
dlda(xlearn, xtest, ylearn)
logreg(xlearn, xtest, ylearn)
aggtrees(xlearn, xtest, ylearn)
</code></pre>


</div>