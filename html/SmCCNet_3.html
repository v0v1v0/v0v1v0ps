<div class="container">

<table style="width: 100%;"><tr>
<td>classifierEval</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluation of Binary Classifier with Different Evaluation Metrics</h2>

<h3>Description</h3>

<p>Evaluate binary classifier's performance with respect to user-selected
metric (accuracy, auc score, precision, recall, f1 score) for binary phenotype.
</p>


<h3>Usage</h3>

<pre><code class="language-R">classifierEval(
  obs,
  pred,
  EvalMethod = "accuracy",
  BinarizeThreshold = 0.5,
  print_score = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>obs</code></td>
<td>
<p>Observed phenotype, vector consists of 0, 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>Predicted probability of the phenotype, vector consists of any value between 0 and 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>EvalMethod</code></td>
<td>
<p>Binary classifier evaluation method, should be one of the following:
'accuracy' (default), 'auc', 'precision', 'recall', and 'f1'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BinarizeThreshold</code></td>
<td>
<p>Cutoff threshold to binarize the predicted probability, default is set 
to 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print_score</code></td>
<td>
<p>Whether to print out the evaluation score, default is set to <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An evaluation score corresponding to the selected metric.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># simulate observed binary phenotype
obs &lt;- rbinom(100,1,0.5)
# simulate predicted probability
pred &lt;- runif(100, 0,1)
# calculate the score
pred_score &lt;- classifierEval(obs, pred, EvalMethod = 'f1', print_score = FALSE)

</code></pre>


</div>