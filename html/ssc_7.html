<div class="container">

<table style="width: 100%;"><tr>
<td>democraticG</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Democratic generic method</h2>

<h3>Description</h3>

<p>Democratic is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains N classifiers with different learning schemes defined in 
list <code>gen.learners</code>. During the iterative process, the multiple classifiers with
different inductive biases label data for each other.
</p>


<h3>Usage</h3>

<pre><code class="language-R">democraticG(y, gen.learners, gen.preds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value <code>NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gen.learners</code></td>
<td>
<p>A list of functions for training N different supervised base classifiers.
Each function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gen.preds</code></td>
<td>
<p>A list of functions for predicting the probabilities per classes.
Each function must be two parameters, model and indexes, where the model
is a classifier trained with <code>gen.learner</code> function and
indexes indicates the instances to predict.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>democraticG can be helpful in those cases where the method selected as 
base classifier needs a <code>learner</code> and <code>pred</code> functions with other
specifications. For more information about the general democratic method,
please see <code>democratic</code> function. Essentially, <code>democratic</code>
function is a wrapper of <code>democraticG</code> function.
</p>


<h3>Value</h3>

<p>A list object of class "democraticG" containing:
</p>

<dl>
<dt>W</dt>
<dd>
<p>A vector with the confidence-weighted vote assigned to each classifier.</p>
</dd>
<dt>model</dt>
<dd>
<p>A list with the final N base classifiers trained using the 
enlarged labeled set.</p>
</dd>
<dt>model.index</dt>
<dd>
<p>List of N vectors of indexes related to the training instances 
used per each classifier. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>instances.index</dt>
<dd>
<p>The indexes of all training instances used to
train the N <code>models</code>. These indexes include the initial labeled instances
and the newly labeled instances. These indexes are relative to the <code>y</code> argument.</p>
</dd>
<dt>model.index.map</dt>
<dd>
<p>List of three vectors with the same information in <code>model.index</code>
but the indexes are relative to <code>instances.index</code> vector.</p>
</dd>
<dt>classes</dt>
<dd>
<p>The levels of <code>y</code> factor.</p>
</dd>
</dl>
<h3>References</h3>

<p>Yan Zhou and Sally Goldman.<br><em>Democratic co-learning.</em><br>
In IEEE 16th International Conference on Tools with Artificial Intelligence (ICTAI),
pages 594-602. IEEE, Nov 2004. doi: 10.1109/ICTAI.2004.48.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
# this is a long running example

library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example A: 
# Training from a set of instances with 
# 1-NN and C-svc (SVM) as base classifiers.

### Define knn base classifier using knn3 from caret package
library(caret)
# learner function
knn &lt;- function(indexes, cls) {
  knn3(x = xtrain[indexes, ], y = cls, k = 1)
}
# function to predict probabilities
knn.prob &lt;- function(model, indexes) {
  predict(model, xtrain[indexes, ])
}

### Define svm base classifier using ksvm from kernlab package
library(kernlab)
library(proxy)
# learner function
svm &lt;- function(indexes, cls) {
  rbf &lt;- function(x, y) {
    sigma &lt;- 0.048
    d &lt;- dist(x, y, method = "Euclidean", by_rows = FALSE)
    exp(-sigma *  d * d)
  }
  class(rbf) &lt;- "kernel"
  ksvm(x = xtrain[indexes, ], y = cls, scaled = FALSE,
       type = "C-svc",  C = 1,
       kernel = rbf, prob.model = TRUE)
}
# function to predict probabilities
svm.prob &lt;- function(model, indexes) {
  predict(model, xtrain[indexes, ], type = "probabilities")
}

### Train
m1 &lt;- democraticG(y = ytrain, 
                  gen.learners = list(knn, svm),
                  gen.preds = list(knn.prob, svm.prob))
### Predict
# predict labels using each classifier
m1.pred1 &lt;- predict(m1$model[[1]], xitest, type = "class")
m1.pred2 &lt;- predict(m1$model[[2]], xitest)
# combine predictions
m1.pred &lt;- list(m1.pred1, m1.pred2)
cls1 &lt;- democraticCombine(m1.pred, m1$W, m1$classes)
table(cls1, yitest)

## Example B: 
# Training from a distance matrix and a kernel matrix with 
# 1-NN and C-svc (SVM) as base classifiers.

### Define knn2 base classifier using oneNN from ssc package
library(ssc)
# Compute distance matrix D
# D is used in knn2.prob
D &lt;- as.matrix(dist(x = xtrain, method = "euclidean", by_rows = TRUE))
# learner function
knn2 &lt;- function(indexes, cls) {
  model &lt;- oneNN(y = cls)
  attr(model, "tra.idxs") &lt;- indexes
  model
}
# function to predict probabilities
knn2.prob &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  predict(model, D[indexes, tra.idxs], distance.weighting = "none")
}

### Define svm2 base classifier using ksvm from kernlab package
library(kernlab)

# Compute kernel matrix K
# K is used in svm2 and svm2.prob functions
sigma &lt;- 0.048
K &lt;- exp(- sigma * D * D)

# learner function
svm2 &lt;- function(indexes, cls) {
  model &lt;- ksvm(K[indexes, indexes], y = cls, 
                type = "C-svc", C = 1,
                kernel = "matrix", 
                prob.model = TRUE)
  attr(model, "tra.idxs") &lt;- indexes
  model
}
# function to predict probabilities
svm2.prob &lt;- function(model, indexes)  {
  tra.idxs &lt;- attr(model, "tra.idxs")
  sv.idxs &lt;- tra.idxs[SVindex(model)]
  predict(model, 
          as.kernelMatrix(K[indexes, sv.idxs]),
          type = "probabilities") 
}


## End(Not run)

</code></pre>


</div>