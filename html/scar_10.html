<div class="container">

<table style="width: 100%;"><tr>
<td>scair</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Maximizing the likelihood of the generalised additive index model 
with shape constraints
</h2>

<h3>Description</h3>

<p>This function searches for a maximum likelihood estimator (mle) of the 
generalised additive index regression with shape constraints. A stochastic search 
strategy is used here. 
</p>
<p>Each index is a linear combination of some (or all) the covariates. 
Each additive component function of these index predictors is assumed to belong 
to one of the nine possible shape restrictions. 
</p>
<p>The output is an object of class <code>scair</code> which contains all the information 
needed to plot the estimator using the <code>plot</code> method, or 
to evaluate it using the <code>predict</code> method.
</p>


<h3>Usage</h3>

<pre><code class="language-R">scair(x,y,shape=rep("l",1), family=gaussian(), weights=rep(1,length(y)), 
  epsilon=1e-8, delta=0.1, indexgen=c("unif", "norm"), iter = 200, 
  allnonneg = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Observed covariates in <code class="reqn">R^d</code>, in the form of an <code class="reqn">n \times d</code> 
numeric <code>matrix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Observed responses, in the form of a numeric <code>vector</code> of length <code class="reqn">n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shape</code></td>
<td>
<p>A vector that specifies the shape restrictions for additive component function
of each index (also called the ridge function), in the form of a string vector of 
length <code class="reqn">m</code>. Here for the sake of identifiability, we require the number of 
indices <code class="reqn">m \le d</code>. The shape constraints we considered (with their 
coresponding abbreviations used in <code>shape</code>) are listed below:
</p>
<p><code>l</code>:	 linear
</p>
<p><code>in</code>:	 monotonically increasing
</p>
<p><code>de</code>:	 monotonically decreasing
</p>
<p><code>cvx</code>:	 convex
</p>
<p><code>cvxin</code>:	 convex and increasing	
</p>
<p><code>cvxde</code>:	 convex and decreasing	
</p>
<p><code>ccv</code>:	 concave
</p>
<p><code>ccvin</code>:	 concave and increasing	
</p>
<p><code>ccvde</code>:	 concave and decreasing</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A description of the error distribution and link function to
be used in the model. This can be a character string naming a
family function, a family function or the result of a call to
a family function.  Currently only the following five common 
exponential families are allowed: Gaussian, Binomial, Poisson,
and Gamma. By default the canonical link function is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>An optional vector of prior weights to be used when maximising the 
likelihood. It is a numeric vector of length <code class="reqn">n</code>. By default 
equal weights are used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Positive convergence tolerance epsilon when performing the 
iteratively reweighted least squares (IRLS) method at each iteration of 
the active set algorithm in <code>scar</code>.  See <code>scar</code> 
for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>A tuning parameter used to avoid the perfect fit phenomenon, and to 
ensure identifiability. It represents the lower bound of the minimum 
eigenvalue of all possible <code class="reqn">A^T A</code> subject to identiability 
conditions, where <code class="reqn">A</code> is an index matrix. It should be smaller than 1. 
This parameter is NOT needed when <code class="reqn">d=1</code>, or the prediction function is
convex or concave, or all the entries of the index matrix are non-negative
if all ridge functions are increasing or decreasing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indexgen</code></td>
<td>
<p>It determines how the index matrices are generated in the stochastic
search. If its value is "<code>unif</code>", then entries of the index matrices are drawn 
from uniform distribution; otherwise, if its value is "<code>norm</code>", entries are 
drawn from normal.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of iterations of the stochastic search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>allnonneg</code></td>
<td>
<p>A boolean variable that specifies whether all the entries of the 
index matrices are non-negative. If it is true, then <code>delta</code> is no
longer needed in case the ridge functions are either all increasing, or all 
decreasing.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For <code class="reqn">i = 1,\ldots,n</code>, let <code class="reqn">X_i</code> be the <code class="reqn">d</code>-dimensional
covariates, <code class="reqn">Y_i</code> be the corresponding one-dimensional response and 
<code class="reqn">w_i</code> be its weight. The generalised additive index model can be written as 
</p>
<p style="text-align: center;"><code class="reqn">g(\mu) = f(x),</code>
</p>
<p> where <code class="reqn">x=(x_1,\ldots,x_d)^T</code>,
<code class="reqn">g</code> is a known link function, <code class="reqn">A</code> is an <code class="reqn">d \times m</code> index matrix,  and 
<code class="reqn">f</code> is an additive function. Our task is to estimate both the index matrix and the 
additive function. 
</p>
<p>Assume the canonical link function is used here, then the maximum likelihood estimator 
of the generalised additive index model based on observations 
<code class="reqn">(X_1,Y_1), \ldots, (X_n,Y_n)</code> 
is the function that maximises 
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n} \sum_{i=1}^n w_i \{Y_i f(A^T X_i) - B(f(A^T X_i))\}</code>
</p>
 
<p>subject to the restrictions that for every <code class="reqn">j = 1,\ldots,m</code>, 
the <code class="reqn">j</code>-th additive component of <code class="reqn">f</code> satisfies the constraint indicated by the  
<code class="reqn">j</code>-th element of <code>shape</code>. Here <code class="reqn">B(.)</code> is the log-partition function of 
the specified exponential family distribution, and <code class="reqn">w_i</code> are the weights. For i.i.d. data, 
<code class="reqn">w_i</code> should be <code class="reqn">1</code> for each <code class="reqn">i</code>.
</p>
<p>For any given <code class="reqn">A</code>, the optimization problem can solved using the active set algorithm 
implemented in <code>scar</code>. Therefore, this problem can be reduced to a finite-dimensional 
optimisation problem. Here we apply a simple stochastic search strategy is proposed, though other methods, 
such as downhill simplex, is also possible (and sometimes offers competitive performance).
All the implementaton details can be found in <cite>Chen and Samworth (2016)</cite>, where theoretical 
justification of our estimator (i.e. uniform consistency) is also given.
</p>
<p>For the identifiability of additive index models, we refer to <cite>Yuan (2011)</cite>.
</p>


<h3>Value</h3>

<p>An object of class <code>scair</code>, with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Covariates copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shape</code></td>
<td>
<p>Shape vector copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Vector of weights copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>The exponential family copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>componentfit</code></td>
<td>
<p>Value of the fitted component function at each observed 
index (computed using the estimated index matrix), in the form of an 
<code class="reqn">n \times m</code> numeric <code>matrix</code>, where the element at 
the <code class="reqn">i</code>-th row and the <code class="reqn">j</code>-th column is the value of <code class="reqn">f_j</code> 
at the <code class="reqn">j</code>-th coordinate of <code class="reqn">A^T X_i</code>,
with the identifiability condition satisfied (see details of <code>scar</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constant</code></td>
<td>
<p>The estimated value of the constant <code class="reqn">c</code> in the 
additive function <code class="reqn">f</code> (see details of <code>scar</code>)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deviance</code></td>
<td>
<p>Up to a constant, minus twice the maximised log-likelihood.
Where applicable, the constant is chosen to make the saturated
model to have zero deviance. See also <code>glm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nulldeviance</code></td>
<td>
<p>The deviance for the null model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>A parameter copied from input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Total number of iterations of the stochastic search algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>allnonneg</code></td>
<td>
<p>specifies whether all entris of the index matrix is non-negative, 
copied from input.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yining Chen and Richard Samworth</p>


<h3>References</h3>

<p>Chen, Y. and Samworth, R. J. (2016).  Generalized additive and index models with shape constraints.
Journal of the Royal Statistical Society: Series B, 78, 729-754.
</p>
<p>Yuan, M. (2011). On the identifiability of additive index models. Statistica Sinica, 21, 1901-1911.
</p>


<h3>See Also</h3>

<p><code>plot.scair</code>, <code>predict.scair</code>, <code>scar</code>, <code>decathlon</code>   
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## An example in the Gaussian additive index regression setting:
## Define the additive function f on the scale of the predictors
f&lt;-function(x){
  return((0.5*x[,1]+0.25*x[,2]-0.25*x[,3])^2) 
}

## Simulate the covariates and the responses
## covariates are drawn uniformly from [-1,1]^3
set.seed(10)
d = 3
n = 500
x = matrix(runif(n*d)*2-1,nrow=n,ncol=d) 
y = f(x) + rnorm(n,sd=0.5)

## Single index model so no delta is required here
shape=c("cvx")
object = scair(x,y,shape=shape, family=gaussian(),iter = 100)

## Estimated index matrix
object$index

## Root squared error for the estimated index
sqrt(sum((object$index - c(0.5,0.25,-0.25))^2))

## Plot the estimatied additive function for the single index
plot(object)

## Evaluate the estimated prediction function at 10^4 random points 
## drawing from the interior of the support
testx = matrix((runif(10000*d)*1.96-0.98),ncol=d)
testf = predict(object,testx)

## and calculate the (estimated) absolute prediction error
mean(abs(testf-f(testx))) 

## Here we can treat the obtained index matrix as a warm start and perform 
## further optimization (on the second and third entry of the index)
## using e.g. the default R optimisation routine.
fn&lt;-function(w){
    dev = Inf
    if (abs(w[1])+abs(w[2])&gt;1) return(dev)
    else {
      wnew = matrix(c(1-abs(w[1])-abs(w[2]),w[1],w[2]),ncol=1)
      dev = scar(x %*% wnew, y, shape = "cvx")$deviance
      return (dev)
    } 
}
index23 = optim(object$index[2:3],fn)$par
newindex = matrix(c(1-sum(abs(index23)),index23),ncol=1); newindex

## Root squared error for the new estimated index
sqrt(sum((newindex - c(0.5,0.25,-0.25))^2))

## A further example is provided in decathlon dataset

</code></pre>


</div>