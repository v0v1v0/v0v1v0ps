<div class="container">

<table style="width: 100%;"><tr>
<td>rm.coef</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes the RM coefficient for variable subset selection</h2>

<h3>Description</h3>

<p>Computes the RM coefficient, measuring the similarity of the
spectral decompositions of a p-variable data matrix, and of the matrix which
results from regressing all the variables on a subset of only k variables. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">rm.coef(mat, indices)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mat</code></td>
<td>
<p>the full data set's covariance (or correlation) matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indices</code></td>
<td>
<p>a numerical vector, matrix or 3-d array of integers
giving the indices of the variables in the subset. If a matrix is
specified, each row is taken to represent a different
<em>k</em>-variable subset. If a 3-d array is given, it is assumed
that the third dimension corresponds to different cardinalities.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes the RM coefficient that measures the similarity of the
spectral decompositions of a p-variable data matrix, and of the matrix which
results from regressing those variables on a subset (given by "indices") of
the variables.  Input data is expected in the form of a (co)variance or
correlation matrix. If a non-square matrix is given, it is assumed to
be a data matrix, and its correlation matrix is used as input.
</p>
<p>The definition of the RM coefficient is as follows:
</p>
<p style="text-align: center;"><code class="reqn">RM = \sqrt{\frac{\mathrm{tr}(X^t P_v X)}{\mathrm{X^t X}}} </code>
</p>

<p>where <code class="reqn">X</code> is the full 
(column-centered) data matrix and <code class="reqn">P_v</code> is the matrix of 
orthogonal projections on the subspace spanned by a k-variable subset.
</p>
<p>This definition is equivalent to:
</p>
<p style="text-align: center;"><code class="reqn">RM = \sqrt{\frac{\sum\limits_{i=1}^{p}\lambda_i
(r)_i^2}{\sum\limits_{j=1}^{p}\lambda_j}} </code>
</p>

<p>where <code class="reqn">\lambda_i</code> stands for the <code class="reqn">i</code>-th largest
eigenvalue of the covariance matrix defined by X and
<code class="reqn">r</code> stands for the multiple correlation between the
<code>i</code>-th Principal Component and the k-variable subset.
</p>
<p>These definitions are also equivalent to the expression used in the
code, which only requires the covariance (or correlation) matrix of
the data under consideration.
</p>
<p>The fact that <code>indices</code> can be a matrix or 3-d array allows for
the computation of the RM values of subsets produced by the search
functions <code>anneal</code>, <code>genetic</code> and
<code>improve</code> (whose output option <code>$subsets</code> are
matrices or 3-d arrays), using a different criterion (see the example
below).
</p>


<h3>Value</h3>

<p>The value of the RM coefficient.
</p>


<h3>References</h3>

<p>Cadima, J. and Jolliffe, I.T. (2001), "Variable Selection and the
Interpretation of Principal Subspaces", <em>Journal of Agricultural,
Biological and Environmental Statistics</em>, Vol. 6, 62-79.
</p>
<p>McCabe, G.P. (1986) "Prediction of Principal Components by Variable
Subsets", <em>Technical Report 86-19, Department of Statistics,
Purdue University</em>.
</p>
<p>Ramsay, J.O., ten Berge, J. and Styan, G.P.H. (1984), "Matrix
Correlation", <em>Psychometrika</em>, 49, 403-423. 
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## An example with a very small data set.

data(iris3) 
x&lt;-iris3[,,1]
rm.coef(var(x),c(1,3))
## [1] 0.8724422

## An example computing the RMs of three subsets produced when the
## anneal function attempted to optimize the RV criterion (using an
## absurdly small number of iterations).

data(swiss)
rvresults&lt;-anneal(cor(swiss),2,nsol=4,niter=5,criterion="Rv")
rm.coef(cor(swiss),rvresults$subsets)

##              Card.2
##Solution 1 0.7982296
##Solution 2 0.7945390
##Solution 3 0.7649296
##Solution 4 0.7623326
</code></pre>


</div>