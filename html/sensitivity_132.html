<div class="container">

<table style="width: 100%;"><tr>
<td>lmg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>LMG <code class="reqn">R^2</code> decomposition for linear and logistic regression models</h2>

<h3>Description</h3>

<p><code>lmg</code> computes the Lindeman, Merenda and Gold (LMG) indices for correlated
input relative importance by <code class="reqn">R^2</code> decomposition for linear and logistic
regression models. These indices allocates  a share of <code class="reqn">R^2</code> to each input
based on the Shapley attribution system, in the case of dependent or correlated inputs.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lmg(X, y, logistic = FALSE,  rank = FALSE, nboot = 0, 
    conf = 0.95, max.iter = 1000, parl = NULL)
## S3 method for class 'lmg'
print(x, ...)
## S3 method for class 'lmg'
plot(x, ylim = c(0,1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a matrix or data frame containing the observed covariates
(i.e., features, input variables...).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a numeric vector containing the observed outcomes (i.e.,
dependent variable). If <code>logistic=TRUE</code>, can be a numeric vector
of zeros and ones, or a logical vector, or a factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logistic</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done via a
logistic regression(binomial GLM).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rank</code></td>
<td>
<p>logical. If <code>TRUE</code>, the analysis is done on the
ranks.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nboot</code></td>
<td>
<p>the number of bootstrap replicates for the computation
of confidence intervals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf</code></td>
<td>
<p>the confidence level of the bootstrap confidence intervals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>if <code>logistic=TRUE</code>, the maximum number of iterative 
optimization steps allowed for the logistic regression. Default is <code>1000</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parl</code></td>
<td>
<p>number of cores on which to parallelize the computation. If
<code>NULL</code>, then no parallelization is done.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the object returned by <code>lmg</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ylim</code></td>
<td>
<p>the y-coordinate limits of the plot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical
parameters (see <code>par</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The computation is done using the subset procedure, defined in Broto, Bachoc
and Depecker (2020), that is computing all the <code class="reqn">R^2</code> for all possible
sub-models first, and then affecting the Shapley weights according to the Lindeman,
Merenda and Gold (1980) definition.
</p>
<p>For logistic regression (<code>logistic=TRUE</code>), the <code class="reqn">R^2</code>
value is equal to:
</p>
<p style="text-align: center;"><code class="reqn">R^2 = 1-\frac{\textrm{model deviance}}{\textrm{null deviance}}</code>
</p>

<p>If either a logistic regression model (<code>logistic = TRUE</code>), or any column
of <code>X</code> is categorical (i.e., of class <code>factor</code>), then the rank-based
indices cannot be computed. In both those cases, <code>rank = FALSE</code> is forced
by default (with a <code>warning</code>).
</p>
<p>If too many cores for the machine are passed on to the <code>parl</code> argument,
the chosen number of cores is defaulted to the available cores minus one.
</p>


<h3>Value</h3>

<p><code>lmg</code> returns a list of class <code>"lmg"</code>, containing the following
components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the matched call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmg</code></td>
<td>
<p>a data frame containing the estimations of the LMG indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R2s</code></td>
<td>
<p>the estimations of the <code class="reqn">R^2</code> for all possible sub-models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indices</code></td>
<td>
<p>list of all subsets corresponding to the structure of R2s.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>the Shapley weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf_int</code></td>
<td>
<p>a matrix containing the estimations, biais and confidence
intervals by bootstrap (if <code>nboot&gt;0</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>the observed covariates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>the observed outcomes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logistic</code></td>
<td>
<p>logical. <code>TRUE</code> if the analysis has been made by
logistic regression.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>boot</code></td>
<td>
<p>logical. <code>TRUE</code> if bootstrap estimates have been produced.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nboot</code></td>
<td>
<p>number of bootstrap replicates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rank</code></td>
<td>
<p>logical. <code>TRUE</code> if a rank analysis has been made.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parl</code></td>
<td>
<p>number of chosen cores for the computation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf</code></td>
<td>
<p>level for the confidence intervals by bootstrap.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Marouane Il Idrissi
</p>


<h3>References</h3>

<p>Broto B., Bachoc F. and Depecker M. (2020) <em>Variance Reduction for Estimation
of Shapley Effects and Adaptation to Unknown Input Distribution.</em> SIAM/ASA Journal
on Uncertainty Quantification, 8(2).
</p>
<p>D.V. Budescu (1993). <em>Dominance analysis: A new approach to the problem of relative
importance of predictors in multiple regression.</em> Psychological Bulletin, 114:542-551.
</p>
<p>L. Clouvel, B. Iooss, V. Chabridon, M. Il Idrissi and F. Robin, 2024,
<em>An overview of variance-based importance measures in the linear regression context: 
comparative analyses and numerical tests</em>, Preprint.
<a href="https://hal.science/hal-04102053">https://hal.science/hal-04102053</a>
</p>
<p>U. Gromping (2006). <em>Relative importance for linear regression in R: the Package
relaimpo.</em>  Journal of Statistical Software, 17:1-27.
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Developments and applications
of Shapley effects   to reliability-oriented sensitivity analysis with correlated 
inputs</em>, Environmental Modelling &amp; Software, 143, 105115, 2021
</p>
<p>M. Il Idrissi, V. Chabridon and B. Iooss (2021). <em>Mesures d'importance relative  
par decompositions de la performance de modeles de regression,</em> Actes des 52emes
Journees   de Statistiques de la Societe Francaise de Statistique (SFdS), pp 497-502,
Nice, France, Juin 2021
</p>
<p>B. Iooss, V. Chabridon and V. Thouvenot, <em>Variance-based importance 
measures for machine learning model interpretability</em>, Congres lambda-mu23,
Saclay, France, 10-13 octobre 2022
<a href="https://hal.science/hal-03741384">https://hal.science/hal-03741384</a>
</p>
<p>Lindeman RH, Merenda PF, Gold RZ (1980). <em>Introduction to Bivariate and Multivariate
Analysis.</em> Scott, Foresman, Glenview, IL.
</p>


<h3>See Also</h3>

<p><code>pcc</code>, <code>src</code>, <code>johnson</code>, <code>shapleyPermEx</code>, <code>shapleysobol_knn</code>, <code>pmvd</code>, <code>pme_knn</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(parallel)
library(doParallel)
library(foreach)
library(gtools)
library(boot)

library(mvtnorm)

set.seed(1234)
n &lt;- 1000
beta&lt;-c(1,-1,0.5)
sigma&lt;-matrix(c(1,0,0,
                0,1,-0.8,
                0,-0.8,1),
              nrow=3,
              ncol=3)

############################
# Gaussian correlated inputs

X &lt;-rmvnorm(n, rep(0,3), sigma)
colnames(X)&lt;-c("X1","X2", "X3")

#############################
# Linear Model

y &lt;- X%*%beta + rnorm(n,0,2)

# Without Bootstrap confidence intervals
x&lt;-lmg(X, y)
print(x)
plot(x)

# With Boostrap confidence intervals
x&lt;-lmg(X, y, nboot=100, conf=0.95)
print(x)
plot(x)

# Rank-based analysis
x&lt;-lmg(X, y, rank=TRUE, nboot=100, conf=0.95)
print(x)
plot(x)

############################
# Logistic Regression
y&lt;-as.numeric(X%*%beta + rnorm(n)&gt;0)
x&lt;-lmg(X,y, logistic = TRUE)
plot(x)
print(x)

# Parallel computing
#x&lt;-lmg(X,y, logistic = TRUE, parl=2)
#plot(x)
#print(x)

</code></pre>


</div>