<div class="container">

<table style="width: 100%;"><tr>
<td>twinSIR</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Fit an Additive-Multiplicative Intensity Model for SIR Data
</h2>

<h3>Description</h3>

<p><code>twinSIR</code> is used to fit additive-multiplicative intensity models for
epidemics as described in Höhle (2009).  Estimation is driven 
by (penalized) maximum likelihood in the point process frame work.  Optimization 
(maximization) of the (penalized) likelihood function is performed by means of 
<code>optim</code>.
The implementation is illustrated in Meyer et al. (2017, Section 4),
see <code>vignette("twinSIR")</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">twinSIR(formula, data, weights, subset,
        knots = NULL, nIntervals = 1, lambda.smooth = 0, penalty = 1,
        optim.args = list(), model = TRUE, keep.data = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>an object of class <code>"formula"</code> (or one that can be coerced to
that class): a symbolic description of the intensity model to be estimated.  
The details of the model specification are given below.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>an object inheriting from class <code>"epidata"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>

<p>an optional vector of weights to be used in the fitting process.  Should be
<code>NULL</code> (the default, i.e. all observations have unit weight) or a
numeric vector.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in the
fitting process.  The subset <code>atRiskY == 1</code> is automatically chosen,
because the likelihood only depends on those observations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots</code></td>
<td>

<p>numeric vector or <code>NULL</code> (the default).  Specification of the knots,
where we suppose a step of the log-baseline.  With the current 
implementation, these must be existing <code>"stop"</code> time points in the
selected <code>subset</code> of the <code>data</code>, which is always
restricted to <code>atRiskY == 1</code> rows.
The intervals of constant log-baseline 
hazard rate then are <code class="reqn">(minTime;knots_1]</code>, <code class="reqn">(knots_1;knots_2]</code>,
..., <code class="reqn">(knots_K;maxTime]</code>.
By default, the <code>knots</code> are automatically chosen at the quantiles of
the infection time points such that <code>nIntervals</code> intervals result.
Non-NULL <code>knots</code> take precedence over <code>nIntervals</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nIntervals</code></td>
<td>

<p>the number of intervals of constant log-baseline hazard.  Defaults to 1,
which means an overall constant log-baseline hazard will be fitted.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.smooth</code></td>
<td>

<p>numeric, the smoothing parameter <code class="reqn">\lambda</code>.  By default it is 0 which
leads to unpenalized likelihood inference.
In case <code>lambda.smooth=-1</code>, the automatic smoothing parameter
selection based on a mixed model approach is used (cf.
Höhle, 2009).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>

<p>either a single number denoting the order of the difference used to penalize
the log-baseline coefficients (defaults to 1), or a more specific penalty
matrix <code class="reqn">K</code> for the parameter sub-vector <code class="reqn">\beta</code>. In case of
non-equidistant knots – usually the case when using quantile based
knot locations – only a 1st order differences penalty matrix as in
Fahrmeir and Lang (2001) is implemented.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optim.args</code></td>
<td>

<p>a list with arguments passed to the <code>optim</code> function.
Especially useful are the following ones:
</p>

<dl>
<dt>
<code>par</code>:</dt>
<dd>
<p>to specify initial parameter values.  Those must be in the order
<code>c(alpha, h0, beta)</code>, i.e. first the coefficients of the epidemic
covariates in the same order as they appear in the <code>formula</code>, then
the log-baseline levels in chronological order and finally the
coefficients of the endemic covariates in the same order
as they appear in the <code>cox</code> terms of the <code>formula</code>.  The default
is to start with 1's for <code>alpha</code> and 0's for <code>h0</code> and
<code>beta</code>.
</p>
</dd>
<dt>
<code>control</code>:</dt>
<dd>
<p>for more detailed <code>trace</code>-ing (default: 1), another <code>REPORT</code>-ing
frequency if <code>trace</code> is positive (default: 10), higher <code>maxit</code>
(maximum number of iterations, default: 300) or another <code>factr</code> value
(default: 1e7, a lower value means higher precision).
</p>
</dd>
<dt>
<code>method</code>:</dt>
<dd>
<p>the optimization algorithm defaults to <code>"L-BFGS-B"</code> (for
box-constrained optimization), if there are any epidemic (non-<code>cox</code>)
variables in the model, and to <code>"BFGS"</code> otherwise.
</p>
</dd>
<dt>
<code>lower</code>:</dt>
<dd>
<p>if <code>method = "L-BFGS-B"</code> this defines the lower bounds for the
model coefficients.  By default, all effects <code class="reqn">\alpha</code> of epidemic
variables are restricted to be non-negative.  Normally, this is exactly
what one would like to have, but there might be reasons for other lower
bounds, see the Note below.
</p>
</dd>
<dt>
<code>hessian</code>:</dt>
<dd>
<p>An estimation of the Expected Fisher Information matrix is always 
part of the return value of the function.  It might be interesting to see 
the Observed Fisher Information (= negative Hessian at the maximum), too. 
This will be additionally returned if <code>hessian = TRUE</code>.
</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>

<p>logical indicating if the model frame, the <code>weights</code>,
<code>lambda.smooth</code>, the penalty matrix <code class="reqn">K</code> and the list of used
distance functions <code>f</code> (from <code>attributes(data)</code>) should be
returned for further computation.  This defaults to <code>TRUE</code> as this
information is necessary e.g. in the <code>profile</code> and <code>plot</code> 
methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.data</code></td>
<td>

<p>logical indicating if the <code>"epidata"</code> object (<code>data</code>)
should be part of the return value. This is only necessary for use of the
<code>simulate</code>-method for <code>"twinSIR"</code>
objects.  The reason is that the <code>twinSIR</code> function only uses and
stores the rows with <code>atRiskY == 1</code> in the <code>model</code> component, but
for the simulation of new epidemic data one needs the whole data set with
all individuals in every time block.  The default value is <code>FALSE</code>, so
if you intent to use <code>simulate.twinSIR</code>, you have to set this to
<code>TRUE</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

  
<p>A model is specified through the <code>formula</code>, which has the form
</p>
<p><code>~ epidemicTerm1 + epidemicTerm2 + cox(endemicVar1) *
    cox(endemicVar2)</code>,
</p>
<p>i.e. the right hand side has the usual form as in <code>lm</code> with
some variables marked as being endemic by the special function
<code>cox</code>.  The left hand side of the formula is empty and will be
set internally to <code>cbind(start, stop, event)</code>, which is similar to
<code>Surv(start, stop, event, type="counting")</code> in package <span class="pkg">survival</span>.
</p>
<p>Basically, the additive-multiplicative model for the infection intensity
<code class="reqn">\lambda_i(t)</code> for individual <code class="reqn">i</code> is
</p>
<p style="text-align: center;"><code class="reqn">\lambda_i(t) = Y_i(t) * (e_i(t) + h_i(t))</code>
</p>

<p>where
</p>

<dl>
<dt>Y_i(t)</dt>
<dd>
<p>is the at-risk indicator, indicating if individual <code class="reqn">i</code> is
“at risk” of becoming infected at time point <code class="reqn">t</code>.
This variable is part of the event history <code>data</code>.
</p>
</dd>
<dt>e_i(t)</dt>
<dd>
<p>is the epidemic component of the infection intensity, defined as
</p>
<p style="text-align: center;"><code class="reqn">e_i(t) = \sum_{j \in I(t)} f(||s_i - s_j||)</code>
</p>

<p>where <code class="reqn">I(t)</code> is the set of infectious individuals just before time
point <code class="reqn">t</code>, <code class="reqn">s_i</code> is the coordinate vector of individual <code class="reqn">i</code>
and the function <code class="reqn">f</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn">f(u) = \sum_{m=1}^p \alpha_m B_m(u)</code>
</p>

<p>with unknown transmission parameters <code class="reqn">\alpha</code> and known distance
functions <code class="reqn">B_m</code>. This set of distance functions results in the set of
epidemic variables normally calculated by the converter function
<code>as.epidata</code>, considering the equality
</p>
<p style="text-align: center;"><code class="reqn">e_i(t) = \sum_{m=1}^p \alpha_m x_{im}(t)</code>
</p>

<p>with <code class="reqn">x_{im}(t) = \sum_{j \in I(t)} B_m(||s_i - s_j||)</code> being the
<code class="reqn">m</code>'th epidemic variable for individual <code class="reqn">i</code>.
</p>
</dd>
<dt>h_i(t)</dt>
<dd>
<p>is the endemic (<code>cox</code>) component of the infection intensity, defined
as
</p>
<p style="text-align: center;"><code class="reqn">h_i(t) = \exp(h_0(t) + z_i(t)' \beta)</code>
</p>

<p>where <code class="reqn">h_0(t)</code> is the log-baseline hazard function, <code class="reqn">z_i(t)</code>
is the vector of endemic covariates of individual <code class="reqn">i</code> and <code class="reqn">\beta</code>
is the vector of unknown coefficients.
To fit the model, the log-baseline hazard function is approximated by a
piecewise constant function with known knots, but unknown levels,
which will be estimated. The approximation is specified by the arguments
<code>knots</code> or <code>nIntervals</code>.
</p>
</dd>
</dl>
<p>If a big number of <code>knots</code> (or <code>nIntervals</code>) is chosen, the
corresponding log-baseline parameters can be rendered identifiable by
the use of penalized likelihood inference.  At present, it is the job
of the user to choose an adequate value of the smoothing parameter
<code>lambda.smooth</code>. Alternatively, a data driven
<code>lambda.smooth</code> smoothing parameter selection based on a mixed
model representation of an equivalent truncated power spline is offered (see
reference for further details). The following two steps are iterated
until convergence:
</p>

<ol>
<li>
<p> Given fixed smoothing parameter, the penalized
likelihood is optimized for the regression components using a L-BFGS-B
approach
</p>
</li>
<li>
<p> Given fixed regression parameters, a Laplace approximation of the
marginal likelihood for the smoothing parameter is numerically
optimized.  
</p>
</li>
</ol>
<p>Depending on the data, convergence might take a couple of iterations.
</p>
<p>Note also that it is unwise to include endemic covariates with huge values,
as they affect the intensities on the exponential scale (after
multiplication by the parameter vector <code class="reqn">\beta</code>).
With large covariate values, the
<code>optim</code> method "L-BFGS-B" will likely terminate due to an infinite 
log-likelihood or score function in some iteration.
</p>


<h3>Value</h3>

<p><code>twinSIR</code> returns an object of class
<code>"twinSIR"</code>, which is a list containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>a named vector of coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loglik</code></td>
<td>
<p>the maximum of the (penalized) log-likelihood function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>counts</code></td>
<td>
<p>the number of log-likelihood and score function evaluations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>logical indicating convergence of the optimization
algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fisherinfo.observed</code></td>
<td>
<p>if requested, the negative Hessian from
<code>optim</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fisherinfo</code></td>
<td>
<p>an estimation of the Expected Fisher Information matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the optimization algorithm used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intervals</code></td>
<td>
<p>a numeric vector (<code>c(minTime, knots, maxTime)</code>)
representing the consecutive intervals of constant log-baseline.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nEvents</code></td>
<td>
<p>a numeric vector containing the number of infections in each of
the above <code>intervals</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>if requested, the model information used. This is a list with
components <code>"survs"</code> (data.frame with the id, start, stop and event
columns), <code>"X"</code> (matrix of the epidemic variables), <code>"Z"</code> (matrix
of the endemic variables), <code>"weights"</code> (the specified <code>weights</code>), 
<code>"lambda.smooth"</code> (the specified <code>lambda.smooth</code>), <code>"K"</code>
(the penalty matrix used), and <code>"f"</code> and <code>"w"</code>
(the functions to generate the used epidemic covariates).
Be aware that the model only contains those rows with <code>atRiskY == 1</code>!</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>if requested, the supplied <code>"epidata"</code> <code>data</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the matched call.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>the specified <code>formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>terms</code></td>
<td>
<p>the <code>terms</code> object used.</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>There are some restrictions to modelling the infection intensity
without a baseline hazard rate, i.e. without an intercept in the
<code>formula</code>.  
Reason: At some point, the optimization algorithm L-BFGS-B tries to set all 
transmission parameters <code class="reqn">\alpha</code> to the boundary value 0 and to calculate
the (penalized) score function with this set of parameters (all 0).  The problem
then is that the values of the infection intensities <code class="reqn">lambda_i(t)</code> are 0
for all <code class="reqn">i</code> and <code class="reqn">t</code> and especially at observed event times, which is 
impossible.  Without a baseline, it is not allowed to have all alpha's set to 0, 
because then we would not observe any infections.  Unfortunately, L-BFGS-B can 
not consider this restriction.  Thus, if one wants to fit a model without 
baseline hazard, the control parameter <code>lower</code> must be specified in 
<code>optim.args</code> so that some alpha is strictly positive, e.g.
<code>optim.args = list(lower = c(0,0.001,0.001,0))</code> and the initial parameter
vector <code>par</code> must not be the zero vector.
</p>


<h3>Author(s)</h3>

<p>Michael Höhle and Sebastian Meyer
</p>


<h3>References</h3>

<p>Höhle, M. (2009),
Additive-multiplicative regression models for spatio-temporal
epidemics, <em>Biometrical Journal</em>, <b>51</b> (6), 961-978.
</p>
<p>Meyer, S., Held, L. and Höhle, M. (2017):
Spatio-temporal analysis of epidemic phenomena using the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> package
<span class="pkg">surveillance</span>.
<em>Journal of Statistical Software</em>, <b>77</b> (11), 1-55.
<a href="https://doi.org/10.18637/jss.v077.i11">doi:10.18637/jss.v077.i11</a>
</p>


<h3>See Also</h3>

<p><code>as.epidata</code> for the necessary data input structure,
<code>plot.twinSIR</code> for plotting the path of the infection intensity,
<code>profile.twinSIR</code> for profile likelihood estimation.
and <code>simulate.twinSIR</code> for the simulation of epidemics following
the fitted model.
</p>
<p>Furthermore, the standard extraction methods
<code>vcov</code>, <code>logLik</code>,
<code>AIC</code> and
<code>extractAIC</code> are implemented for
objects of class <code>"twinSIR"</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("hagelloch")
summary(hagelloch)

# simple model with an overall constant baseline hazard rate
fit1 &lt;- twinSIR(~ household + cox(AGE), data = hagelloch)
fit1
summary(fit1)   # see also help("summary.twinSIR")
plot(fit1)      # see also help("plot.twinSIR")
checkResidualProcess(fit1)   # could be better

# fit a piecewise constant baseline hazard rate with 3 intervals using 
# _un_penalized ML and estimated coefs from fit1 as starting values 
fit2 &lt;- twinSIR(~ household, data = hagelloch, nIntervals = 3,
                optim.args = list(par = coef(fit1)[c(1,2,2,2)]))
summary(fit2)

# fit a piecewise constant baseline hazard rate with 7 intervals
# using _penalized_ ML
fit3 &lt;- twinSIR(~ household, data = hagelloch, nIntervals = 7,
                lambda.smooth = 0.1, penalty = 1)
summary(fit3)
checkResidualProcess(fit3)

# plot the estimated log-baseline levels
plot(x=fit2$intervals, y=coef(fit2)[c(2,2:4)], type="S", ylim=c(-6, -1))
lines(x=fit3$intervals, y=coef(fit3)[c(2,2:8)], type="S", col=2)
legend("right", legend=c("unpenalized 3", "penalized 7"), lty=1, col=1:2, bty="n")


## special use case: fit the model to a subset of the events only,
## while preserving epidemic contributions from the remainder
## (maybe some buffer area nodes)
fit_subset &lt;- twinSIR(~ household, data = hagelloch, subset = CL=="preschool")
summary(fit_subset)


</code></pre>


</div>