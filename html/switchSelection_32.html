<div class="container">

<table style="width: 100%;"><tr>
<td>test_msel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tests and confidence intervals for the parameters estimated by 
the msel function</h2>

<h3>Description</h3>

<p>This function conducts various statistical tests and calculates
confidence intervals for the parameters of the model estimated via the 
<code>msel</code> function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">test_msel(
  object,
  fn,
  fn_args = list(),
  test = "t",
  method = "classic",
  ci = "classic",
  cl = 0.95,
  se_type = "dm",
  trim = 0,
  vcov = object$cov,
  iter = 100,
  generator = rnorm,
  bootstrap = NULL,
  par_ind = 1:object$control_lnL$n_par,
  eps = max(1e-04, sqrt(.Machine$double.eps) * 10),
  n_sim = 1000,
  n_cores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>an object of class 'msel'. It also may be a list of two 
objects. Then <code>object[[1]]</code> and <code>object[[2]]</code> are supplied to the
arguments <code>model1</code> and <code>model2</code> of the 
<code>lrtest_msel</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn</code></td>
<td>
<p>a function which returns a numeric vector and should depend on the 
elements of <code>object</code>. These elements should be accessed via 
<code>coef.msel</code> or 
<code>predict.msel</code> functions.
The first argument of <code>fn</code> should be an <code>object</code>.
Therefore <code>coef</code> and <code>predict</code> functions in <code>fn</code> should also
depend on <code>object</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn_args</code></td>
<td>
<p>a list of additional arguments of <code>fn</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>
<p>a character representing the test to be used.
If <code>test = "t"</code> then t-test is used.
If <code>test = "wald"</code> then Wald test is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character representing a method used to conduct a test.
If <code>test = "t"</code> or <code>test = "wald"</code> and <code>method = "classic"</code> 
then p-values are calculated by using the quantiles of the standard normal 
distribution.
If <code>test = "t"</code> or <code>test = "wald"</code> and <code>method = "bootstrap"</code> 
then p-values are calculated by using the bootstrap as described 
in Hansen (2022).
If <code>test = "wald"</code> and <code>method = "score"</code> then score 
bootstrap Wald test of P. Kline and A. Santos (2012) is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ci</code></td>
<td>
<p>a character representing the type of the confidence interval
used. Available only if <code>test = "t"</code>.
If <code>ci = "classic"</code> then quantiles of the standard normal distribution
are used to build an asymptotic confidence interval.
If <code>ci = "percentile"</code> then percentile bootstrap interval is applied.
If <code>ci = "bc"</code> then the function constructs a bias-corrected 
percentile bootstrap confidence interval of Efron (1982) as described in
Hansen (2022).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cl</code></td>
<td>
<p>a numeric value between <code>0</code> and <code>1</code> representing
a confidence level of the confidence interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se_type</code></td>
<td>
<p>a character representing a method used to estimate 
the standard errors of the outputs of <code>fn</code>. 
If <code>se_type = "dm"</code> then delta method is used.
If <code>se_type = "bootstrap"</code> then bootstrap is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trim</code></td>
<td>
<p>a numeric value between <code>0</code> and <code>1</code> representing
the share of bootstrap estimates to be nullified when standard errors are
estimated for <code>se_type = "bootstrap"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vcov</code></td>
<td>
<p>an estimate of the asymptotic covariance matrix of the 
parameters of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>the number of iterations used by the score bootstrap Wald test.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>generator</code></td>
<td>
<p>function which is used by the score bootstrap to generate
random weights. It should have an argument <code>n</code> representing the
number of random weights to generate. Other arguments are ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bootstrap</code></td>
<td>
<p>an object of class <code>'bootstrap_msel'</code> which is an 
output of the <code>bootstrap_msel</code> function.
This object is used to retrieve the estimates of the bootstrap samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>par_ind</code></td>
<td>
<p>a vector of indexes of the model parameters used 
in the calculation of <code>fn</code>. If only necessary indexes are included then
in some cases estimation time may greatly decrease. Set <code>ind = TRUE</code>
in <code>summary.msel</code> to see the indexes of the 
model parameters. If <code>eps</code> is a vector then <code>eps[i]</code> determines 
the increment used to differentiate <code>fn</code> respect to the parameter with 
<code>par_ind[i]</code>-th index.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>a positive numeric value representing the increment used for the
numeric differentiation of <code>fn</code>. It may also be a numeric vector
such that <code>eps[i]</code> is an increment used to differentiate the 
<code>fn</code> respect to the <code>par_ind[i]</code>-th parameter of the model.
Set <code>ind = TRUE</code> in <code>summary.msel</code>, to see 
the indexes of the model parameters. If <code>eps[i] = 0</code> then derivative of 
<code>fn</code> respect to <code>par_ind[i]</code>-th parameter is assumed to be zero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_sim</code></td>
<td>
<p>the value passed to the <code>n_sim</code> argument of the
<code>msel</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_cores</code></td>
<td>
<p>the value passed to the <code>n_cores</code> argument of the
<code>msel</code> function.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Suppose that <code class="reqn">\theta</code> is a vector of parameters of the model 
estimated via the <code>msel</code> function and 
<code class="reqn">g(\theta)</code> is a differentiable function representing <code>fn</code> which 
returns a <code class="reqn">m</code>-dimensional vector of real values:
</p>
<p style="text-align: center;"><code class="reqn">g(\theta) = (g_{1}(\theta),...,g_{m}(\theta))^{T}.</code>
</p>

<p><strong>Classic and bootstrap t-test</strong>
</p>
<p>If <code>test = "t"</code> then for each <code class="reqn">j\in \{1,...,m\}</code> the following 
hypotheses is tested:
</p>
<p style="text-align: center;"><code class="reqn">H_{0}:g_{j}(\theta) = 0,\qquad H_{1}:g_{j}(\theta)\ne 0.</code>
</p>

<p>The test statistic is: 
</p>
<p style="text-align: center;"><code class="reqn">T = g_{j}(\hat{\theta})/\hat{\sigma}_{j},</code>
</p>

<p>where <code class="reqn">\hat{\sigma}</code> is a standard error of <code class="reqn">g_{j}(\hat{\theta})</code>.
</p>
<p>If <code>se_type = "dm"</code> then delta method is used to estimate 
this standard error:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{j} = \sqrt{\nabla g_{j}(\hat{\theta})^{T} 
\widehat{As.Cov}(\hat{\theta})
\nabla g_{j}(\hat{\theta})},
</code>
</p>

<p>where <code class="reqn">\nabla g_{j}(\hat{\theta})</code> is a gradient as a column vector and
the estimate of the asymptotic covariance matrix of the estimates 
<code class="reqn">\widehat{As.Cov}(\hat{\theta})</code> is provided via the <code>vcov</code> 
argument. Numeric differentiation is used to calculate 
<code class="reqn">\nabla g_{j}(\hat{\theta})</code>.
</p>
<p>If <code>se_type = "bootstrap"</code> then bootstrap is 
applied to estimate the standard error:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\sigma}_{j} = \sqrt{\frac{1}{B - 1}\sum\limits_{b = 1}^{B} 
(g_{j}(\hat{\theta}^{(b)}) - \overline{g_{j}(\hat{\theta}^{(b)}}))^2},</code>
</p>

<p>where <code class="reqn">B</code> is the number of the bootstrap iterations 
<code>bootstrap$iter</code>, <code class="reqn">\hat{\theta}^{(b)}</code> is the estimate 
associated with the <code class="reqn">b</code>-th of these iterations <code>bootstrap$par[b, ]</code>,
and <code class="reqn">g_{j}(\overline{\hat{\theta}^{(b)}})</code> is a sample mean of the bootstrap 
estimates:
</p>
<p style="text-align: center;"><code class="reqn">\overline{g_{j}(\hat{\theta}^{(b)}}) = 
\frac{1}{B}\sum\limits_{b = 1}^{B}g_{j}(\hat{\theta}^{(b)}).</code>
</p>

<p>If <code>method = "classic"</code> it is assumed that if the null hypothesis is 
true then the asymptotic distribution of the test statistic is standard 
normal. This distribution is used for the calculation of the p-value:
</p>
<p style="text-align: center;"><code class="reqn">p-value = 2\min(\Phi(T), 1 - \Phi(T)),</code>
</p>

<p>where <code class="reqn">\Phi()</code> is a cumulative distribution function of the standard
normal distribution.
</p>
<p>If <code>method = "bootstrap"</code> then p-value is calculated via the bootstrap 
as suggested by Hansen (2022):
</p>
<p style="text-align: center;"><code class="reqn">
p-value = \frac{1}{B}\sum\limits_{b=1}^{B} I(|T_{b} - T| &gt; |T|),
</code>
</p>

<p>where <code class="reqn">T_{b} = g_{j}(\hat{\theta}^{(b)})/\hat{\sigma}_{j}</code> is the value of 
the test statistic estimated on the <code>b</code>-th bootstrap sample and 
<code class="reqn">I(q)</code> is an indicator function which equals <code class="reqn">1</code> when <code class="reqn">q</code> is a 
true statement and <code class="reqn">0</code> - otherwise.
</p>
<p><strong>Classic and bootstrap Wald test</strong>
</p>
<p>Suppose that <code>method = "classic"</code> or <code>method = "bootstrap"</code>.
If <code>test = "wald"</code> then the null hypothesis is:
</p>
<p style="text-align: center;"><code class="reqn">H_{0}:
\begin{cases}
g_{1}(\theta) = 0\\
g_{2}(\theta) = 0\\
\vdots\\
g_{m}(\theta) = 0\\
\end{cases}.
</code>
</p>

<p>The alternative hypothesis is that there is such <code class="reqn">j\in\{1,...,m\}</code> that:
</p>
<p style="text-align: center;"><code class="reqn">H_{1}:g_{j}(\theta)\ne 0.</code>
</p>

<p>The test statistic is:
</p>
<p style="text-align: center;"><code class="reqn">T = g(\hat{\theta})^{T}\widehat{As.Cov}(g(\hat{\theta}))^{-1}
g(\hat{\theta}),</code>
</p>

<p>where <code class="reqn">\widehat{As.Cov}(g(\hat{\theta}))</code> is the estimate of the 
asymptotic covariance matrix of <code class="reqn">g(\hat{\theta})</code>. 
</p>
<p>If <code>se_type = "dm"</code> then delta method is used to estimate this matrix:
</p>
<p style="text-align: center;"><code class="reqn">\widehat{As.Cov}(g(\hat{\theta})) 
= g'(\hat{\theta})\widehat{As.Cov}(\hat{\theta}) g'(\hat{\theta})^{T},</code>
</p>

<p>where <code class="reqn">g'(\hat{\theta})</code> is a Jacobian matrix. A numeric differentiation
is used to calculate its elements:
</p>
<p style="text-align: center;"><code class="reqn">g'(\hat{\theta})_{ij} = 
\frac{\partial g_{i}(\theta)}{\partial \theta_{j}}|_{\theta = \hat{\theta}}.</code>
</p>

<p>If <code>se_type = "bootstrap"</code> then bootstrap is used to estimate this 
matrix:
</p>
<p style="text-align: center;"><code class="reqn">\widehat{As.Cov}(g(\hat{\theta}))=
\frac{1}{B-1}\sum\limits_{i=1}^{B}q_{b}q_{b}^{T},</code>
</p>

<p>where:
</p>
<p style="text-align: center;"><code class="reqn">q_{b} = (g(\hat{\theta}^{(b)}) - 
\overline{g(\hat{\theta}^{(b)})}),</code>
</p>

<p style="text-align: center;"><code class="reqn">\overline{g(\hat{\theta}^{(b)})} = \frac{1}{B}\sum\limits_{i=1}^{B}
g(\hat{\theta}^{(b)}).</code>
</p>

<p>If <code>method = "classic"</code> then it is assumed that if the null hypothesis 
is true then the asymptotic distribution of the test statistic is chi-squared
with <code class="reqn">m</code> degrees of freedom. This distribution is used for the 
calculation of the p-value:
</p>
<p style="text-align: center;"><code class="reqn">p-value = 1 - F_{m}(T),</code>
</p>

<p>where <code class="reqn">F_{m}</code> is a cumulative distribution function of the chi-squared
distribution with <code class="reqn">m</code> degrees of freedom.
</p>
<p>If <code>method = "bootstrap"</code> then p-value is calculated via the bootstrap 
as suggested by Hansen (2022):
</p>
<p style="text-align: center;"><code class="reqn">
p-value = \frac{1}{B}\sum\limits_{b=1}^{B} I(T_{b} &gt; T),
</code>
</p>

<p>where:
</p>
<p style="text-align: center;"><code class="reqn">T_{b} = s_{b}^{T}\widehat{As.Cov}(g(\hat{\theta}))^{-1}s_{b},</code>
</p>

<p style="text-align: center;"><code class="reqn">s_{b} = g(\hat{\theta}^{(b)}) - g(\hat{\theta}).</code>
</p>

<p><strong>Score bootstrap Wald test</strong>
</p>
<p>If <code>method = "score"</code> and <code>test = "Wald"</code> then score bootstrap
Wald test of Kline and Santos (2012) is used.
</p>
<p>Consider <code class="reqn">B</code> independent samples of <code class="reqn">n</code> independent identically 
distributed random weights with zero mean and unit variance. 
Let <code class="reqn">w_{ib}</code> denote the <code class="reqn">i</code>-th weight of the <code class="reqn">b</code>-th sample. 
Argument <code>generator</code> is used to supply a function which generates these 
weights <code class="reqn">w_{ib}</code> and <code>iter</code> argument represents <code class="reqn">B</code>. 
Also <code class="reqn">n</code> is the number of observations in the model 
<code>object$other$n_obs</code>.
</p>
<p>Let <code class="reqn">J</code> denote a matrix of sample scores <code>object$J</code>. 
Further, denote by <code class="reqn">J_{b}</code> a matrix such that its <code class="reqn">b</code>-th row is a 
product of the <code class="reqn">w_{ib}</code> and the <code class="reqn">b</code>-th row of <code class="reqn">J</code>. 
Also, denote by <code class="reqn">H</code> a matrix of mean values of the derivatives of 
sample scores respect to the estimated parameters <code>object$H</code>.
</p>
<p>In addition consider the following notations:
</p>
<p style="text-align: center;"><code class="reqn">A = g'(\theta) H^{-1}, \qquad S_{b} = A J^{(c)}_{b},</code>
</p>

<p>where <code class="reqn">J^{(c)}_{b}</code> is a vector of the column sums of <code class="reqn">J_{b}</code>.
</p>
<p>The test statistic is as follows:
</p>
<p style="text-align: center;"><code class="reqn">T = g(\hat{\theta})^{T}(A\widehat{Cov}(J)
A^{T})^{-1}g(\hat{\theta}) / n,</code>
</p>

<p>where <code class="reqn">\widehat{Cov}(J)</code> is a sample covariance matrix of the sample
scores of the model <code>cov(object$J)</code>.
</p>
<p>The test statistic on the <code class="reqn">b</code>-th bootstrap sample is similar:
</p>
<p style="text-align: center;"><code class="reqn">T_{b} = S^{T}(A\widehat{Cov}(J_{b})
A^{T})^{-1}S / n.</code>
</p>

<p>The p-value is estimated as follows:
</p>
<p style="text-align: center;"><code class="reqn">
p-value = \frac{1}{B}\sum\limits_{b=1}^{B} I(T_{b} \geq T).
</code>
</p>

<p><strong>Confidence intervals</strong>
</p>
<p>If <code>test = "t"</code> then the function also returns the realizations of the 
lower and upper bounds of the <code class="reqn">100 \times</code><code>cl</code> percent symmetric 
asymptotic confidence interval of <code class="reqn">g_{j}(\theta)</code>.
</p>
<p>If <code>ci = "classic"</code> then classic confidence interval is used which
assumes asymptotic normality of <code class="reqn">g_{j}(\hat{\theta})</code>:
</p>
<p style="text-align: center;"><code class="reqn">(g_{j}(\hat{\theta}) + z_{(1 - cl) / 2}\hat{\sigma}_{j}, 
g_{j}(\hat{\theta}) + z_{1 - (1 - cl) / 2}\hat{\sigma}_{j}),</code>
</p>

<p>where <code class="reqn">z_{q}</code> is a <code class="reqn">q</code>-th quantile of the standard normal 
distribution and <code class="reqn">cl</code> is a confidence level <code>cl</code>. The method used 
to estimate <code class="reqn">\hat{\sigma}_{j}</code> depends on the <code>se_type</code> argument as 
described above.
</p>
<p>If <code>ci = "percentile"</code> then percentile bootstrap confidence interval 
is used. Therefore the sample quantiles of <code class="reqn">g_{j}(\hat{\theta}^{(b)})</code> 
are used as the realizations of the lower and upper bounds of the confidence 
interval.
</p>
<p>If <code>ci = "bc"</code> then bias corrected percentile bootstrap confidence 
interval of Efron (1982) is used as described in Hansen (2022). The default
percentile bootstrap confidence interval uses sample quantiles of levels 
<code class="reqn">(1 - cl)/2</code> and <code class="reqn">1 - (1 - cl)/2</code>. 
Bias corrected version uses the sample quantiles of the
following levels:
</p>
<p style="text-align: center;"><code class="reqn">(1 - cl)/2 + \Phi(\Phi^{-1}((1 - cl)/2) + s),</code>
</p>

<p style="text-align: center;"><code class="reqn">1 - (1 - cl)/2 + \Phi(\Phi^{-1}(1 - (1 - cl)/2) + s),</code>
</p>

<p>where:
</p>
<p style="text-align: center;"><code class="reqn">s = 2\Phi^{-1}(\frac{1}{B}\sum\limits_{b = 1}^{B}
I(g_{j}(\hat{\theta}^{(b)})\leq g_{j}(\hat{\theta}))).</code>
</p>

<p><strong>Trimming</strong>
</p>
<p>If <code>se_type = "bootstrap"</code> and <code>trim &gt; 0</code> then trimming is used as
described in Hansen (2022) to estimate <code class="reqn">\hat{\sigma}_{j}</code> and 
<code class="reqn">\widehat{As.Cov}(g(\hat{\theta}))</code>. The algorithm is as follows. 
First, nullify 100<code>trim</code> percent of <code class="reqn">g(\hat{\theta}^{(b)})</code> with the 
greatest values of the L2-norm of <code class="reqn">q_{b}</code> (defined above). 
Then use this 'trimmed' sample to estimate the standard error and the 
asymptotic covariance matrix.
</p>


<h3>Value</h3>

<p>This function returns an object of class <code>'test_msel'</code> which is
a list. It may have the following elements:
</p>

<ul>
<li> <p><code>tbl</code> - a list with the elements described below.
</p>
</li>
<li> <p><code>is_bootstrap</code> - a logical value which equals <code>TRUE</code> if 
bootstrap has been used.
</p>
</li>
<li> <p><code>is_ci</code> - a logical value which equals <code>TRUE</code> if
confidence intervals were used.
</p>
</li>
<li> <p><code>test</code> - the same as the input argument <code>test</code>.
</p>
</li>
<li> <p><code>method</code> - the same as the input argument <code>method</code>.
</p>
</li>
<li> <p><code>se_type</code> - the same as the input argument <code>method</code>.
</p>
</li>
<li> <p><code>ci</code> - the same as the input argument <code>ci</code>.
</p>
</li>
<li> <p><code>cl</code> - the same as the input argument <code>cl</code>.
</p>
</li>
<li> <p><code>iter</code> - the same as the input argument <code>iter</code>.
</p>
</li>
<li> <p><code>n_bootstrap</code> - an integer representing the number of the
bootstrap iterations used.
</p>
</li>
<li> <p><code>n_val</code> - the length of the vector returned by <code>fn</code>.
</p>
</li>
</ul>
<p>A list <code>tbl</code> may have the following elements:
</p>

<ul>
<li> <p><code>val</code> - an output of the <code>fn</code> function.
</p>
</li>
<li> <p><code>se</code> - a numeric vector such that <code>se[i]</code> represents 
a standard error associated with <code>val[i]</code>.
</p>
</li>
<li> <p><code>p_value</code> - a numeric vector of p-values.
</p>
</li>
<li> <p><code>lwr</code> - a numeric vector such that <code>lwr[i]</code> is the 
realization of the lower (left) bound of the confidence interval for the
true value of <code>val[i]</code>.
</p>
</li>
<li> <p><code>upr</code> - a numeric vector such that <code>upr[i]</code> is the 
realization of the upper (right) bound of the confidence interval for the
true value of <code>val[i]</code>.
</p>
</li>
<li> <p><code>stat</code> - a numeric vector of values of the test statistics.
</p>
</li>
</ul>
<p>An object of class <code>'test_msel'</code> has an implementation of the
<code>summary</code> method
<code>summary.test_msel</code>.
</p>
<p>In a special case when <code>object</code> is a list of length <code>2</code> the 
function returns an object of class <code>'lrtest_msel'</code> since the function
<code>lrtest_msel</code> is called internally.
</p>


<h3>References</h3>

<p>B. Efron (1982). The Jackknife, the Bootstrap, and Other 
Resampling Plans. Society for Industrial and Applied Mathematics.
</p>
<p>B. Hansen (2022). Econometrics. Princeton University Press.
</p>
<p>P. Kline, A. Santos (2012). 
A Score Based Approach to Wild Bootstrap Inference.
Journal of Econometric Methods, vol. 67, no. 1, pages 23-41.
</p>


<h3>Examples</h3>

<pre><code class="language-R">

# -------------------------------
# CPS data example
# -------------------------------

# Set seed for reproducibility
set.seed(123)

# Upload the data
data(cps)

# Estimate the employment model
model &lt;- msel(work ~ age + I(age ^ 2) + bachelor + master, data = cps)
summary(model)    

# Use Wald test to test the hypothesis that age has no 
# effect on the conditional probability of employment:
# H0: coef age     = 0
#     coef age ^ 2 = 0
age_fn &lt;- function(object)
{
  lwage_coef &lt;- coef(object, type = "coef")[[1]]
  val        &lt;- c(lwage_coef["age"], lwage_coef["I(age^2)"])
  return(val)
}
age_test &lt;- test_msel(object = model, fn = age_fn, test = "wald")
summary(age_test)

# Use t-test to test for each individual the hypothesis:
# P(work = 1 | x) = 0.8
prob_fn &lt;- function(object)
{
  prob &lt;- predict(object, group = 1, type = "prob")
  val  &lt;- prob - 0.8
  return(val)
}
prob_test &lt;- test_msel(object = model, fn = prob_fn, test = "t")
summary(prob_test)

# -------------------------------
# Simulated data example
# Model with continuous outcome
# and ordinal selection
# -------------------------------

# ---
# Step 1
# Simulation of the data
# ---

# Set seed for reproducibility
set.seed(123)

# Load required package
library("mnorm")

# The number of observations
n &lt;- 10000

# Regressors (covariates)
s1 &lt;- runif(n = n, min = -1, max = 1)
s2 &lt;- runif(n = n, min = -1, max = 1)
s3 &lt;- runif(n = n, min = -1, max = 1)
s4 &lt;- runif(n = n, min = -1, max = 1)

# Random errors
sigma &lt;- matrix(c(1,    0.4,  0.45, 0.7,
                  0.4,  1,    0.54, 0.8,
                  0.45, 0.54, 0.81, 0.81,
                  0.7,  0.8,  0.81, 1), nrow = 4)
errors &lt;- mnorm::rmnorm(n = n, mean = c(0, 0, 0, 0), sigma = sigma)
u1   &lt;- errors[, 1]
u2   &lt;- errors[, 2]
eps0 &lt;- errors[, 3]
eps1 &lt;- errors[, 4]

# Coefficients
gamma1     &lt;- c(-1, 2)
gamma2     &lt;- c(1, 1)
gamma1_het &lt;- c(0.5, -1)
beta0      &lt;- c(1, -1, 1, -1.2)
beta1      &lt;- c(2, -1.5, 0.5, 1.2)
# Linear index of the ordinal equations
# mean part
li1 &lt;- gamma1[1] * s1 + gamma1[2] * s2
li2 &lt;- gamma2[1] * s1 + gamma2[2] * s3
# variance part
li1_het &lt;- gamma1_het[1] * s2 + gamma1_het[2] * s3

# Linear index of the continuous equation
# regime 0
li_y0 &lt;- beta0[1] + beta0[2] * s1 + beta0[3] * s3 + beta0[4] * s4
# regime 1
li_y1 &lt;- beta1[1] + beta1[2] * s1 + beta1[3] * s3 + beta1[4] * s4

# Latent variables
z1_star &lt;- li1 + u1 * exp(li1_het)
z2_star &lt;- li2 + u2
y0_star &lt;- li_y0 + eps0
y1_star &lt;- li_y1 + eps1

# Cuts
cuts1 &lt;- c(-1)
cuts2 &lt;- c(0, 1)

# Observable ordinal outcome
# first
z1                     &lt;- rep(0, n)
z1[z1_star &gt; cuts1[1]] &lt;- 1
# second
z2                                               &lt;- rep(0, n)
z2[(z2_star &gt; cuts2[1]) &amp; (z2_star &lt;= cuts2[2])] &lt;- 1
z2[z2_star &gt; cuts2[2]]                           &lt;- 2
z2[z1 == 0]                                      &lt;- NA

# Observable continuous outcome
y                 &lt;- rep(NA, n)
y[which(z2 == 0)] &lt;- y0_star[which(z2 == 0)]
y[which(z2 != 0)] &lt;- y1_star[which(z2 != 0)]
y[which(z1 == 0)] &lt;- NA

# Data
data &lt;- data.frame(s1 = s1, s2 = s2, s3 = s3, s4 = s4,
                   z1 = z1, z2 = z2, y = y)

# ---
# Step 2
# Estimation of the parameters
# ---

# Assign the groups
groups  &lt;- matrix(c(1, 2,
                    1, 1,
                    1, 0,
                    0, -1), 
                  byrow = TRUE, ncol = 2)
groups2 &lt;- matrix(c(1, 1, 0, -1), ncol = 1)

# Estimate the model
model &lt;- msel(list(z1 ~ s1 + s2 | s2 + s3,
                   z2 ~ s1 + s3),
              list(y  ~ s1 + s3 + s4),
              groups = groups, groups2 = groups2, 
              data   = data)
                   
# ---
# Step 3
# Hypotheses testing
# ---

# Use t-test to test for each observation the hypothesis
# H0: P(z1 = 0, z2 = 2 | Xi) = 0 
prob02_fn &lt;- function(object)
{
   val &lt;- predict(object, group = c(1, 0))
   
   return(val)
}
prob02_test &lt;- test_msel(object = model, fn = prob02_fn, test = "t")
summary(prob02_test)

# Use t-test to test the hypothesis
# H0: E(y1|z1=0, z2=2) - E(y0|z1=0, z2=2)
ATE_fn &lt;- function(object)
{
   val1 &lt;- predict(object, group = c(0, 2), group2 = 1)
   val0 &lt;- predict(object, group = c(0, 2), group2 = 0)
   val &lt;- mean(val1 - val0)
   
   return(val)
}
ATE_test &lt;- test_msel(object = model, fn = ATE_fn)
summary(ATE_test)

# Use Wald to test the hypothesis
# H0: beta1 = beta0
coef_fn &lt;- function(object)
{
   coef1 &lt;- coef(object, regime = 1, type = "coef2")
   coef0 &lt;- coef(object, regime = 0, type = "coef2")
   coef_difference &lt;- coef1 - coef0
   
   return(coef_difference)
}
coef_test &lt;- test_msel(object = model, fn = coef_fn, test = "wald")
summary(coef_test)

# Use t-test to test for each 'k' the hypothesis
# H0: beta1k = beta0k
coef_test2 &lt;- test_msel(object = model, fn = coef_fn, test = "t")
summary(coef_test2)

# Use Wald test to test the hypothesis
# H0: beta11 + beta12 - 0.5 = 0
#     beta11 * beta13 - beta03 = 0
test_fn &lt;- function(object)
{
  coef1 &lt;- coef(object, regime = 1, type = "coef2")
  coef0 &lt;- coef(object, regime = 0, type = "coef2")
  val   &lt;- c(coef1[1] + coef1[2] - 0.5, 
  coef1[1] * coef1[3] - coef0[3])
     
  return(val)
}
# classic Wald test
wald1 &lt;- test_msel(object = model,  fn     = test_fn, 
                   test   = "wald", method = "classic")
summary(wald1)
# score bootstrap Wald test
wald2 &lt;- test_msel(object = model,  fn     = test_fn, 
                   test   = "wald", method = "score")
summary(wald2)

# Replicate the latter test with the 2-step estimator
model2 &lt;- msel(list(z1 ~ s1 + s2 | s2 + s3,
                         z2 ~ s1 + s3),
               list(y ~ s1 + s3 + s4),
               groups    = groups, groups2   = groups2, 
               data      = data,   estimator = "2step")
# classic Wald test
wald1_2step &lt;- test_msel(object = model2, fn     = test_fn, 
                         test   = "wald", method = "classic")
summary(wald1_2step)
# score bootstrap Wald test
wald2_2step &lt;- test_msel(object = model2, fn     = test_fn, 
                         test   = "wald", method = "score")
summary(wald2_2step)    

             
</code></pre>


</div>