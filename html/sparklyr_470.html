<div class="container">

<table style="width: 100%;"><tr>
<td>spark_write</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Write Spark DataFrame to file using a custom writer</h2>

<h3>Description</h3>

<p>Run a custom R function on Spark worker to write a Spark DataFrame
into file(s). If Spark's speculative execution feature is enabled (i.e.,
'spark.speculation' is true), then each write task may be executed more than
once and the user-defined writer function will need to ensure no concurrent
writes happen to the same file path (e.g., by appending UUID to each file name).
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_write(x, writer, paths, packages = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A Spark Dataframe to be saved into file(s)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>writer</code></td>
<td>
<p>A writer function with the signature function(partition, path)
where <code>partition</code> is a R dataframe containing all rows from one partition
of the original Spark Dataframe <code>x</code> and path is a string specifying the
file to write <code>partition</code> to</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paths</code></td>
<td>
<p>A single destination path or a list of destination paths, each one
specifying a location for a partition from <code>x</code> to be written to. If
number of partition(s) in <code>x</code> is not equal to <code>length(paths)</code> then
<code>x</code> will be re-partitioned to contain <code>length(paths)</code> partition(s)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>packages</code></td>
<td>
<p>Boolean to distribute <code>.libPaths()</code> packages to each node,
a list of packages to distribute, or a package bundle created with</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

library(sparklyr)

sc &lt;- spark_connect(master = "local[3]")

# copy some test data into a Spark Dataframe
sdf &lt;- sdf_copy_to(sc, iris, overwrite = TRUE)

# create a writer function
writer &lt;- function(df, path) {
  write.csv(df, path)
}

spark_write(
  sdf,
  writer,
  # re-partition sdf into 3 partitions and write them to 3 separate files
  paths = list("file:///tmp/file1", "file:///tmp/file2", "file:///tmp/file3"),
)

spark_write(
  sdf,
  writer,
  # save all rows into a single file
  paths = list("file:///tmp/all_rows")
)

## End(Not run)

</code></pre>


</div>