<div class="container">

<table style="width: 100%;"><tr>
<td>cerardat</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
cerardat
</h2>

<h3>Description</h3>

<p>The methodology is based on a statistical and visual approach using two estimated density curves to date each archaeological context. The statistical procedure required two steps, each leading to the construction of a density curve. The first allowed us to estimate a date corresponding to the terminus post quem of the context, a cursor reflecting an event dated in calendar time. This statistical tool allows the archaeologist to easily visualise and analyse chronological patterns.
</p>


<h3>Usage</h3>

<pre><code class="language-R">cerardat(df, row.sup, date, nf = NULL, confidence = 0.95, graph = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>

<p>The data (data.frame) is a contingency table with the observations in the rows and the technical groups in the columns.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.sup</code></td>
<td>

<p>Index of supplementary rows in df (vector).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>date</code></td>
<td>

<p>The dates of each observation or NA (vector).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nf</code></td>
<td>

<p>an integer representing the number of axes retained in the correspondence analysis. If NULL, it is automatically chosen to keep a number corresponding to at least 60% of the inertia.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>confidence</code></td>
<td>

<p>The desired confidence interval (0.95 for 95%).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>graph</code></td>
<td>

<p>logical to display the plots or not.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The corpus data is a contingency table with the observations in the rows and the technical groups in the columns. There are two types of observations: the reference corpus observations and the supplementary observations. The supplementary rows (observations) are identified by the argument 'row.sup'.
</p>
<p><strong>step 1: modelling events dated in calendar time (dateEv)</strong><br>
This step involves estimating the date of an event recorded in the ground (an archaeological context for the archaeologist) from the pottery assemblage of which it is composed, by fitting a regression model that relates a known date in calendar time, such as the date of issue of a coin, to its pottery profile. The reference corpus used to fit the regression model. We then used the previously fitted model to calculate a predicted value for contexts not included in the reference corpus, sometimes stratigraphically separated or poorly documented, with a 95% confidence interval for the predicted date.
</p>
<p>A correspondence analysis (CA) was carried out to summarize the information in the reference corpus data. We then kept only the first factorial axes. In this way, our contingency table becomes a reduced size table, an incomplete reconstruction of the data. This principle is used in many factor analysis techniques to reduce the number of explanatory variables in the linear regression model.
</p>
<p>After estimating the beta parameters of the model using the classical results of multiple regression analysis and checking that the model fits the data correctly, we can deduce the estimated date of an observation and also predict the date of another observation that has no coins and is therefore not dated.
</p>
<p><strong>step 2: from event time (dateEv) to accumulation time (dateAc)</strong><br>
We used the results of the first step and the properties of the CA to obtain an estimate of the date of each fabric. We could then define the archaeological time represented as dateAc, in other words the accumulation time of a context, as the weighted sum of the fabric dates; the weights being the proportions of MINVC of each fabric in the context. Assuming that the random variables dateEvj are independent, the distribution of the accumulation time of each context can be approximated by the Gaussian mixture. In this way, for each context, we obtained a plurimodal density curve representing the estimated law of accumulation time based on the mixture of normal densities (dates of each tissue). By definition, the area under the density curve has a value of 1 (i.e. 100%).
</p>
<p><strong>date</strong><br>
In order to estimate a date for the context, it is essential to refer to objects that have been dated by another source, such as coins. These contexts were selected on a very strict basis for their chronostratigraphic reliability, level of domestic occupation or enclosures with long urban stratigraphic sequences, thereby minimising any bias associated with the disparity between the date of the coin and that of the context.
</p>


<h3>Value</h3>



<table>
<tr style="vertical-align: top;">
<td><code>prediction </code></td>
<td>
<p>Estimated date for archaeological context (event: dateEV and accumulation: dateAc) with confidence interval. The first two columns show the total count and the number of categories per line (only for columns used in CA). Then a date column shows the known dates. Each dateEv and dateAC model has three columns (value, lower bound, upper bound).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>date_gt </code></td>
<td>
<p>Estimated date for technical groups with confidence interval (use for dateAc). The first column show the total count per category in the reference data (only for rows used in CA).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lm </code></td>
<td>
<p>Linear model on the components of the correspondance analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predict_obj_row </code></td>
<td>
<p>date prediction of archaeological contexts (rows) using predict.lm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predict_obj_col </code></td>
<td>
<p>date prediction of technical groups (columns) using predict.lm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cont_gt </code></td>
<td>
<p>Contingency table of the reference corpus.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>statistical.summary </code></td>
<td>
<p>Statistical summary of the model:<br>
Adjusted R-squared<br>
R-squared<br>
sigma (Residual standard error)<br>
The Shapiro-Wilks test is used to verify the normality of the residuals.<br>
The Durbin-Watson test checks for first order autocorrelation.<br>
The Breusch-Pagan test checks for heteroscedasticity.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obs_ca_eval </code></td>
<td>
<p>Quality of row representation in the correspondence analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check_ref </code></td>
<td>
<p>Plot of estimated dates (with confidence interval) and real dates of reference data. Only when the real date is known.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check_sup </code></td>
<td>
<p>Plot of estimated dates (with confidence interval) and real dates of supplementary data. Only when the real date is known.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Shapiro_Wilks </code></td>
<td>
<p>Summary of the Shapiro-Wilks test. see shapiro.test.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Durbin_Watson </code></td>
<td>
<p>Summary of the Durbin-Watson test. see dwtest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Breusch_Pagan </code></td>
<td>
<p>Summary of the Breusch-Pagan test. see bptest.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>References</h3>

<p>Bellanger L. and Husi P. (2012) Statistical tool for dating and interpreting archaeological contexts using pottery. Journal of Archaeological Science, Elsevier, 39 (4), pp.777-790. doi:10.1016/j.jas.2011.06.031.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data("datacerardat")

resultat = cerardat(df = datacerardat$df,
           row.sup = datacerardat$row.sup,
           date = datacerardat$date,
           nf = NULL,
           confidence = 0.95,
           graph = TRUE
        )

resultat
#The Shapiro-Wilks test is used to verify the normality of the residuals.
#The Durbin-Watson test checks for first order autocorrelation.
#The Breusch-Pagan test checks for heteroscedasticity.



#See the first plot
plot(resultat,
     which = 1,
     col1=rgb(0.93,0.23,0.23,0.5),
     col2="black",
     xlim=NULL,
     ylim=c(0,0.03)
    )

#See the first ten plots
#plot(resultat,
#     which = 1:10,
#     col1=rgb(0.93,0.23,0.23,0.5),
#     col2="black",
#     xlim=NULL,
#     ylim=c(0,0.03)
#    )

#See all plots
#plot(resultat,
#     which = NULL,
#     col1=rgb(0.93,0.23,0.23,0.5),
#     col2="black",
#     xlim=NULL,
#     ylim=c(0,0.03)
#    )

#You can extract the plots and find them in the directory :
paste0(getwd(),"/figures")
#With the 'extract_results' function
#extract_results(resultat,width=480, height=480, path="figures")

</code></pre>


</div>