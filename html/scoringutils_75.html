<div class="container">

<table style="width: 100%;"><tr>
<td>score_quantile</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate forecasts in a Quantile-Based Format</h2>

<h3>Description</h3>

<p>Evaluate forecasts in a Quantile-Based Format
</p>


<h3>Usage</h3>

<pre><code class="language-R">score_quantile(
  data,
  forecast_unit,
  metrics,
  weigh = TRUE,
  count_median_twice = FALSE,
  separate_results = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code>score()</code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li>
</ul>
<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul><li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>
<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul><li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>
<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
"unspecified model").
</p>
<p>You can check the format of your data using <code>check_forecasts()</code> and there
are examples for each format (example_quantile, example_continuous,
example_integer, and example_binary).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>forecast_unit</code></td>
<td>
<p>A character vector with the column names that define
the unit of a single forecast, i.e. a forecast was made for a combination
of the values in <code>forecast_unit</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p>the metrics you want to have in the output. If <code>NULL</code> (the
default), all available metrics will be computed. For a list of available
metrics see <code>available_metrics()</code>, or  check the metrics data set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weigh</code></td>
<td>
<p>if TRUE, weigh the score by alpha / 2, so it can be averaged
into an interval score that, in the limit, corresponds to CRPS. Alpha is the
decimal value that  represents how much is outside a central prediction
interval (e.g. for a 90 percent central prediction interval, alpha is 0.1)
Default: <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>count_median_twice</code></td>
<td>
<p>logical that controls whether or not to count the
median twice when summarising (default is <code>FALSE</code>). Counting the
median twice would conceptually treat it as a 0\
the median is the lower as well as the upper bound. The alternative is to
treat the median as a single quantile forecast instead of an interval. The
interval score would then be better understood as an average of quantile
scores.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>separate_results</code></td>
<td>
<p>if <code>TRUE</code> (default is <code>FALSE</code>), then the separate
parts of the interval score (dispersion penalty, penalties for over- and
under-prediction get returned as separate elements of a list). If you want a
<code>data.frame</code> instead, simply call <code>as.data.frame()</code> on the output.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A data.table with appropriate scores. For more information see
<code>score()</code>
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
(2019) Assessing the performance of real-time epidemic forecasts: A
case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


</div>