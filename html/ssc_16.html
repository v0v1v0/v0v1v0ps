<div class="container">

<table style="width: 100%;"><tr>
<td>selfTraining</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Self-training method</h2>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples. 
Self-training follows a wrapper methodology using a base supervised 
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class="language-R">selfTraining(x, y, x.inst = TRUE, learner, learner.pars = NULL,
  pred = "predict", pred.pars = NULL, max.iter = 50,
  perc.full = 0.7, thr.conf = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A object that can be coerced as matrix. This object has two possible 
interpretations according to the value set in the <code>x.inst</code> argument:
a matrix with the training instances where each row represents a single instance
or a precomputed (distance or kernel) matrix between the training examples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector with the labels of the training instances. In this vector 
the unlabeled instances are specified with the value <code>NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.inst</code></td>
<td>
<p>A boolean value that indicates if <code>x</code> is or not an instance matrix.
Default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learner</code></td>
<td>
<p>either a function or a string naming the function for 
training a supervised base classifier, using a set of instances
(or optionally a distance matrix) and it's corresponding classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learner.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>learner</code> function if necessary.
Default is <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>either a function or a string naming the function for
predicting the probabilities per classes,
using the base classifier trained with the <code>learner</code> function.
Default is <code>"predict"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred.pars</code></td>
<td>
<p>A list with additional parameters for the
<code>pred</code> function if necessary.
Default is <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process. 
Default is 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence threshold.
At each iteration, only the newly labelled examples with a confidence greater than 
this value (<code>thr.conf</code>) are added to the training set.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For predicting the most accurate instances per iteration, <code>selfTraining</code>
uses the predictions obtained with the learner specified. To train a model 
using the <code>learner</code> function, it is required a set of instances 
(or a precomputed matrix between the instances if <code>x.inst</code> parameter is <code>FALSE</code>)
in conjunction with the corresponding classes. 
Additionals parameters are provided to the <code>learner</code> function via the 
<code>learner.pars</code> argument. The model obtained is a supervised classifier
ready to predict new instances through the <code>pred</code> function. 
Using a similar idea, the additional parameters to the <code>pred</code> function
are provided using the <code>pred.pars</code> argument. The <code>pred</code> function returns 
the probabilities per class for each new instance. The value of the 
<code>thr.conf</code> argument controls the confidence of instances selected 
to enlarge the labeled set for the next iteration.
</p>
<p>The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of the unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the labeled set. In some cases, the process stops and no instances 
are added to the original labeled set. In this case, the user must assign a more 
flexible value to the <code>thr.conf</code> parameter.
</p>


<h3>Value</h3>

<p>A list object of class "selfTraining" containing:
</p>

<dl>
<dt>model</dt>
<dd>
<p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt>
<dd>
<p>The indexes of the training instances used to 
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt>
<dd>
<p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt>
<dd>
<p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt>
<dd>
<p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>
<h3>References</h3>

<p>David Yarowsky.<br><em>Unsupervised word sense disambiguation rivaling supervised methods.</em><br>
In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,
pages 189-196. Association for Computational Linguistics, 1995.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(ssc)

## Load Wine data set
data(wine)

cls &lt;- which(colnames(wine) == "Wine")
x &lt;- wine[, -cls] # instances without classes
y &lt;- wine[, cls] # the classes
x &lt;- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50% of instances for training
tra.idx &lt;- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain &lt;- x[tra.idx,] # training instances
ytrain &lt;- y[tra.idx]  # classes of training instances
# Use 70% of train instances as unlabeled set
tra.na.idx &lt;- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] &lt;- NA # remove class information of unlabeled instances

# Use the other 50% of instances for inductive testing
tst.idx &lt;- setdiff(1:length(y), tra.idx)
xitest &lt;- x[tst.idx,] # testing instances
yitest &lt;- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN as base classifier.
m1 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = caret::knn3, 
                   learner.pars = list(k = 1),
                   pred = "predict")
pred1 &lt;- predict(m1, xitest)
table(pred1, yitest)

## Example: Training from a distance matrix with 1-NN as base classifier.
dtrain &lt;- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
m2 &lt;- selfTraining(x = dtrain, y = ytrain, x.inst = FALSE,
                   learner = ssc::oneNN, 
                   pred = "predict",
                   pred.pars = list(distance.weighting = "none"))
ditest &lt;- proxy::dist(x = xitest, y = xtrain[m2$instances.index,],
                      method = "euclidean", by_rows = TRUE)
pred2 &lt;- predict(m2, ditest)
table(pred2, yitest)

## Example: Training from a set of instances with SVM as base classifier.
learner &lt;- e1071::svm
learner.pars &lt;- list(type = "C-classification", kernel="radial", 
                     probability = TRUE, scale = TRUE)
pred &lt;- function(m, x){
  r &lt;- predict(m, x, probability = TRUE)
  prob &lt;- attr(r, "probabilities")
  prob
}
m3 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = learner, 
                   learner.pars = learner.pars, 
                   pred = pred)
pred3 &lt;- predict(m3, xitest)
table(pred3, yitest)

## Example: Training from a set of instances with Naive-Bayes as base classifier.
m4 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = function(x, y) e1071::naiveBayes(x, y), 
                   pred = "predict",
                   pred.pars = list(type = "raw"))
pred4 &lt;- predict(m4, xitest)
table(pred4, yitest)

## Example: Training from a set of instances with C5.0 as base classifier.
m5 &lt;- selfTraining(x = xtrain, y = ytrain, 
                   learner = C50::C5.0, 
                   pred = "predict",
                   pred.pars = list(type = "prob"))
pred5 &lt;- predict(m5, xitest)
table(pred5, yitest)


</code></pre>


</div>