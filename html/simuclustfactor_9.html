<div class="container">

<table style="width: 100%;"><tr>
<td>fit.twcfta</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>TWCFTA model</h2>

<h3>Description</h3>

<p>Implements K-means clustering and afterwards factorial reduction
in a sequential fashion.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fit.twcfta(model, X_i_jk, full_tensor_shape, reduced_tensor_shape)

## S4 method for signature 'tandem'
fit.twcfta(model, X_i_jk, full_tensor_shape, reduced_tensor_shape)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Initialized tandem model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X_i_jk</code></td>
<td>
<p>Matricized tensor along mode-1 (I objects).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>full_tensor_shape</code></td>
<td>
<p>Dimensions of the tensor in full space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduced_tensor_shape</code></td>
<td>
<p>Dimensions of tensor in the reduced space.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<p>The procedure requires sequential clustering and factorial decomposition.
</p>

<ul>
<li>
<p> The K-means clustering algorithm is initially applied to the
matricized tensor X_i_jk to obtain the centroids matrix X_g_jk and the
membership matrix U_i_g.
</p>
</li>
<li>
<p> The Tucker2 decomposition technique is then implemented on the
centroids matrix X_g_jk to yield the core centroids matrix Y_g_qr and
the component weights matrices B_j_q and C_k_r.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Output attributes accessible via the '@' operator.
</p>

<ul>
<li>
<p> U_i_g0 - Initial object membership function matrix.
</p>
</li>
<li>
<p> B_j_q0 - Initial factor/component matrix for the variables.
</p>
</li>
<li>
<p> C_k_r0 - Initial factor/component matrix for the occasions.
</p>
</li>
<li>
<p> U_i_g - Final/updated object membership function matrix.
</p>
</li>
<li>
<p> B_j_q - Final/updated factor/component matrix for the variables.
</p>
</li>
<li>
<p> C_k_r - Final/updated factor/component matrix for the occasions.
</p>
</li>
<li>
<p> Y_g_qr - Derived centroids in the reduced space (data matrix).
</p>
</li>
<li>
<p> X_i_jk_scaled - Standardized dataset matrix.
</p>
</li>
<li>
<p> BestTimeElapsed - Execution time for the best iterate.
</p>
</li>
<li>
<p> BestLoop - Loop that obtained the best iterate.
</p>
</li>
<li>
<p> BestKmIteration - Number of iteration until best iterate for the K-means.
</p>
</li>
<li>
<p> BestFaIteration - Number of iteration until best iterate for the FA.
</p>
</li>
<li>
<p> FaConverged - Flag to check if algorithm converged for the K-means.
</p>
</li>
<li>
<p> KmConverged - Flag to check if algorithm converged for the Factor Decomposition.
</p>
</li>
<li>
<p> nKmConverges - Number of loops that converged for the K-means.
</p>
</li>
<li>
<p> nFaConverges - Number of loops that converged for the Factor decomposition.
</p>
</li>
<li>
<p> TSS_full - Total deviance in the full-space.
</p>
</li>
<li>
<p> BSS_full - Between deviance in the reduced-space.
</p>
</li>
<li>
<p> RSS_full - Residual deviance in the reduced-space.
</p>
</li>
<li>
<p> PF_full - PseudoF in the full-space.
</p>
</li>
<li>
<p> TSS_reduced - Total deviance in the reduced-space.
</p>
</li>
<li>
<p> BSS_reduced - Between deviance in the reduced-space.
</p>
</li>
<li>
<p> RSS_reduced - Residual deviance in the reduced-space.
</p>
</li>
<li>
<p> PF_reduced - PseudoF in the reduced-space.
</p>
</li>
<li>
<p> PF - Actual PseudoF value to obtain best loop.
</p>
</li>
<li>
<p> Labels - Object cluster assignments.
</p>
</li>
<li>
<p> FsKM - Objective function values for the KM best iterate.
</p>
</li>
<li>
<p> FsFA - Objective function values for the FA best iterate.
</p>
</li>
<li>
<p> Enorm - Average l2 norm of the residual norm.
</p>
</li>
</ul>
<h3>Note</h3>



<ul>
<li>
<p> This procedure is useful to further interpret the between clusters
variability of the data and to understand the variables and/or occasions
that most contribute to discriminate the clusters. However, the application
of this technique could lead to the masking of variables that are not
informative of the clustering structure.
</p>
</li>
<li>
<p> since the Tucker2 model is applied after the clustering, this
cannot help select the most relevant information for the clustering in
the dataset.
</p>
</li>
</ul>
<h3>References</h3>

<p>Arabie P, Hubert L (1996).
“Advances in Cluster Analysis Relevant to Marketing Research.”
In Gaul W, Pfeifer D (eds.), <em>From Data to Knowledge</em>, 3–19.
Tucker L (1966).
“Some mathematical notes on three-mode factor analysis.”
<em>Psychometrika</em>, <b>31</b>(3), 279-311.
<a href="https://doi.org/10.1007/BF02289464">doi:10.1007/BF02289464</a>, <a href="https://ideas.repec.org/a/spr/psycho/v31y1966i3p279-311.html">https://ideas.repec.org/a/spr/psycho/v31y1966i3p279-311.html</a>.
</p>


<h3>See Also</h3>


<p><code>fit.twfcta</code> <code>tandem</code>

</p>


<h3>Examples</h3>

<pre><code class="language-R">X_i_jk = generate_dataset()$X_i_jk
model = tandem()
twcfta = fit.twcfta(model, X_i_jk, c(8,5,4), c(3,3,2))

</code></pre>


</div>