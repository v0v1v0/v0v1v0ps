<div class="container">

<table style="width: 100%;"><tr>
<td>sentopicmodel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create a sentopic model</h2>

<h3>Description</h3>

<p>The set of functions <code>LDA()</code>, <code>JST()</code>, <code>rJST()</code> and
<code>sentopicmodel()</code> are all wrappers to an unified C++ routine and attempt to
replicate their corresponding model. This function is the lower level
wrapper to the C++ routine.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sentopicmodel(
  x,
  lexicon = NULL,
  L1 = 5,
  L2 = 3,
  L1prior = 1,
  L2prior = 5,
  beta = 0.01,
  L1cycle = 0,
  L2cycle = 0,
  reversed = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>tokens object containing the texts. A coercion will be attempted if <code>x</code> is not a tokens.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lexicon</code></td>
<td>
<p>a <code>quanteda</code> dictionary with positive and negative categories</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L1</code></td>
<td>
<p>the number of labels in the first document mixture layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L2</code></td>
<td>
<p>the number of labels in the second document mixture layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L1prior</code></td>
<td>
<p>the first layer hyperparameter of document mixtures</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L2prior</code></td>
<td>
<p>the second layer hyperparameter of document mixtures</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the hyperparameter of vocabulary distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L1cycle</code></td>
<td>
<p>integer specifying the cycle size between two updates of the hyperparameter L1prior</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L2cycle</code></td>
<td>
<p>integer specifying the cycle size between two updates of the hyperparameter L2prior</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reversed</code></td>
<td>
<p>indicates on which dimension should <code>lexicon</code> apply. When
<code>reversed=FALSE</code>, the lexicon is applied on the first layer of the document
mixture (as in a JST model). When <code>reversed=TRUE</code>, the lexicon is applied to
the second layer of the document mixture (as in a reversed-JST model).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An S3 list containing the model parameter and the estimated mixture.
This object corresponds to a Gibbs sampler estimator with zero iterations.
The MCMC can be iterated using the <code>fit()</code>
function.
</p>

<ul>
<li> <p><code>tokens</code> is the tokens object used to create the model
</p>
</li>
<li> <p><code>vocabulary</code> contains the set of words of the corpus
</p>
</li>
<li> <p><code>it</code> tracks the number of Gibbs sampling iterations
</p>
</li>
<li> <p><code>za</code> is the list of topic assignment, aligned to the <code>tokens</code> object with
padding removed
</p>
</li>
<li> <p><code>logLikelihood</code> returns the measured log-likelihood at each iteration,
with a breakdown of the likelihood into hierarchical components as
attribute
</p>
</li>
</ul>
<p>The <code>topWords()</code> function easily extract the most probables words of each
topic/sentiment.
</p>


<h3>Author(s)</h3>

<p>Olivier Delmarcelle
</p>


<h3>See Also</h3>

<p>Fitting a model: <code>fit()</code>,
extracting top words: <code>topWords()</code>
</p>
<p>Other topic models: 
<code>JST()</code>,
<code>LDA()</code>,
<code>rJST()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">LDA(ECB_press_conferences_tokens)
rJST(ECB_press_conferences_tokens, lexicon = LoughranMcDonald)
</code></pre>


</div>