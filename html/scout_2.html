<div class="container">

<table style="width: 100%;"><tr>
<td>crossProdLasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performs the lasso on the cross product matrices X'X and X'y</h2>

<h3>Description</h3>

<p>Perform L1-regularized regression of y onto X using only
the cross-product matrices X'X and X'y. In the case of
covariance-regularized regression, this is useful if you would like to
try out something other than L1 or L2 regularization of the inverse
covariance matrix.
</p>
<p>Suppose you
use your own method to regularize X'X. Then let Sigma denote your estimate of the population covariance matrix.  Now say you want to minimize
beta' Sigma  beta - 2 beta' X'y + lambda ||beta||_1
in order to get the regression estimate beta, which maximizes the
second scout criterion when an L_1 penalty is used. You can do this by
calling crossProdLasso(Sigma, X'y,rho).
</p>
<p>If you run crossProdLasso(X'X,X'y,rho) then it should give the same
result as lars(X,y)
</p>
<p>Notice that the xtx that you pass into this function must be POSITIVE
SEMI DEFINITE (or positive definite) or the problem is not convex and
the algorithm will not converge.
</p>


<h3>Usage</h3>

<pre><code class="language-R">crossProdLasso(xtx,xty,rho,thr=1e-4,maxit=100,beta.init=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xtx</code></td>
<td>
<p>A pxp matrix, which should be an estimate of a covariance
matrix. This matrix must be POSITIVE
SEMI DEFINITE (or positive definite) or the problem is not convex and
the algorithm will not converge.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xty</code></td>
<td>
<p>A px1 vector, which is generally obtained via X'y.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>Must be non-negative; the regularization parameter you are
using. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thr</code></td>
<td>
<p>Convergence threshold.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>How many iterations to perform?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta.init</code></td>
<td>
<p>If you're running this over a range of rho values,
then set beta.init equal to the solution you got for a previous rho
value. It will speed things up.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If your xtx is simply X'X for some X, and your xty is simple X'y
with some y, then the results will be the same as running lars on data
(X,y) for a single shrinkage parameter value.
</p>
<p>Note that when you use the scout function with p2=1, the crossProdLasso
function is
called internally to give the regression coefficients, after the
regularized inverse covariance matrix is estimated. It is provided
here in case it is useful to the user in other settings.
</p>


<h3>Value</h3>

<table><tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>A px1 vector with the regression coefficients.</p>
</td>
</tr></table>
<h3>Note</h3>

<p>The FORTRAN code that this function links to was kindly written
and provided by Jerry Friedman.
</p>


<h3>Author(s)</h3>

<p>FORTRAN code by Jerry Friedman.   R interface by Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten, DM and Tibshirani, R (2008) Covariance-regularized
regression and classification for high-dimensional problems. Journal
of the Royal Statistical Society, Series B 71(3): 615-636. &lt;http://www-stat.stanford.edu/~dwitten&gt;</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1)
#data(diabetes)
#attach(diabetes)
x2 &lt;- matrix(rnorm(10*20),ncol=20)
y &lt;- rnorm(10)
# First, let's do scout(2,1) the usual way).
scout.out &lt;- scout(x2,y,p1=2,p2=1)
print(scout.out)



# Now, suppose I want to do develop a covariance-regularized regression
# method as in Section 3.2 of Witten and Tibshirani (2008). It will work
# like this:
# 1. Develop some positive definite estimate of Sigma
# 2. Find \beta by minimize \beta^T \Sigma \beta - 2 \beta^T X^T y +
# \lamda ||\beta||_1
# 3. Re-scale \beta.

# Step 1:
regcovx &lt;- cov(x2)*(abs(cov(x2))&gt;.005) + diag(ncol(x2))*.01

# Step 2:
betahat &lt;-  crossProdLasso(regcovx, cov(x2,y), rho=.02)$beta
# Step 3:
betahat.sc &lt;- betahat*lsfit(x2%*%betahat, y, intercept=FALSE)$coef
print(betahat.sc)

# Try a different value of rho:
betahat2 &lt;- crossProdLasso(regcovx,cov(x2,y),rho=.04,beta.init=betahat)$beta
plot(betahat,betahat2, xlab="rho=.02",ylab="rho=.04")
#detach(diabetes)
</code></pre>


</div>