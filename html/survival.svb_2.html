<div class="container">

<table style="width: 100%;"><tr>
<td>svb.fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit sparse variational Bayesian proportional hazards models.</h2>

<h3>Description</h3>

<p>Fit sparse variational Bayesian proportional hazards models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">svb.fit(
  Y,
  delta,
  X,
  lambda = 1,
  a0 = 1,
  b0 = ncol(X),
  mu.init = NULL,
  s.init = rep(0.05, ncol(X)),
  g.init = rep(0.5, ncol(X)),
  maxiter = 1000,
  tol = 0.001,
  alpha = 1,
  center = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Failure times.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>Censoring indicator, 0: censored, 1: uncensored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Design matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Penalisation parameter, default: <code>lambda=1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a0</code></td>
<td>
<p>Beta distribution parameter, default: <code>a0=1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>Beta distribution parameter, default: <code>b0=ncol(X)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu.init</code></td>
<td>
<p>Initial value for the mean of the Gaussian component of 
the variational family (<code class="reqn">\mu</code>), default taken from LASSO fit.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.init</code></td>
<td>
<p>Initial value for the standard deviations of the Gaussian 
component of the variational family (<code class="reqn">s</code>), default: 
<code>rep(0.05, ncol(X))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g.init</code></td>
<td>
<p>Initial value for the inclusion probability (<code class="reqn">\gamma</code>), 
default: <code>rep(0.5, ncol(X))</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>Maximum number of iterations, default: <code>1000</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence tolerance, default: <code>0.001</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The elastic-net mixing parameter used for initialising <code>mu.init</code>. 
When <code>alpha=1</code> the lasso penalty is used and <code>alpha=0</code> the ridge 
penalty, values between 0 and 1 give a mixture of the two penalties, default:
<code>1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>Center X prior to fitting, increases numerical stability, 
default: <code>TRUE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Print additional information: default: <code>TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns a list containing: <br></p>
<table>
<tr style="vertical-align: top;">
<td><code>beta_hat</code></td>
<td>
<p>Point estimate for the coefficients <code class="reqn">\beta</code>, taken as 
the mean under the variational approximation.
<code class="reqn">\hat{\beta}_j = E_{\tilde{\Pi}} [ \beta_j ] = \gamma_j \mu_j</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inclusion_prob</code></td>
<td>
<p>Posterior inclusion probabilities. Used to describe
the posterior probability a coefficient is non-zero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>Final value for the means of the Gaussian component of the variational 
family <code class="reqn">\mu</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>Final value for the standard deviation of the Gaussian component of 
the variational family <code class="reqn">s</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g</code></td>
<td>
<p>Final value for the inclusion probability (<code class="reqn">\gamma</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Value of lambda used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a0</code></td>
<td>
<p>Value of <code class="reqn">\alpha_0</code> used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>Value of <code class="reqn">\beta_0</code> used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>Describes whether the algorithm converged.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Rather than compute the posterior using MCMC, we turn to approximating it
using variational inference. Within variational inference we re-cast
Bayesian inference as an optimisation problem, where we minimize the 
Kullback-Leibler (KL) divergence between a family of tractable distributions 
and the posterior, <code class="reqn">\Pi</code>. <br><br> In our case we use a mean-field variational 
family,
</p>
<p style="text-align: center;"><code class="reqn">Q = \{ \prod_{j=1}^p \gamma_j N(\mu_j, s_j^2) + (1 - \gamma_j) \delta_0 \}</code>
</p>

<p>where <code class="reqn">\mu_j</code> is the mean and <code class="reqn">s_j</code> the std. dev for the Gaussian 
component, <code class="reqn">\gamma_j</code> the inclusion probabilities, <code class="reqn">\delta_0</code> a Dirac mass 
at zero and <code class="reqn">p</code> the number of coefficients.<br><br> The components of the
variational family (<code class="reqn">\mu, s, \gamma</code>) are then optimised by minimizing the 
Kullback-Leibler divergence between the variational family and the posterior,
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\Pi} = \arg \min KL(Q \| \Pi).</code>
</p>
<p> We use co-ordinate ascent
variational inference (CAVI) to solve the above optimisation problem. <br><br></p>


<h3>Examples</h3>

<pre><code class="language-R">n &lt;- 125                        # number of sample
p &lt;- 250                        # number of features
s &lt;- 5                          # number of non-zero coefficients
censoring_lvl &lt;- 0.25           # degree of censoring


# generate some test data
set.seed(1)
b &lt;- sample(c(runif(s, -2, 2), rep(0, p-s)))
X &lt;- matrix(rnorm(n * p), nrow=n)
Y &lt;- log(1 - runif(n)) / -exp(X %*% b)
delta  &lt;- runif(n) &gt; censoring_lvl   		# 0: censored, 1: uncensored
Y[!delta] &lt;- Y[!delta] * runif(sum(!delta))	# rescale censored data


# fit the model
f &lt;- survival.svb::svb.fit(Y, delta, X, mu.init=rep(0, p))


## Larger Example
n &lt;- 250                        # number of sample
p &lt;- 1000                       # number of features
s &lt;- 10                         # number of non-zero coefficients
censoring_lvl &lt;- 0.4            # degree of censoring


# generate some test data
set.seed(1)
b &lt;- sample(c(runif(s, -2, 2), rep(0, p-s)))
X &lt;- matrix(rnorm(n * p), nrow=n)
Y &lt;- log(1 - runif(n)) / -exp(X %*% b)
delta  &lt;- runif(n) &gt; censoring_lvl   		# 0: censored, 1: uncensored
Y[!delta] &lt;- Y[!delta] * runif(sum(!delta))	# rescale censored data


# fit the model
f &lt;- survival.svb::svb.fit(Y, delta, X)


# plot the results
plot(b, xlab=expression(beta), main="Coefficient value", pch=8, ylim=c(-2,2))
points(f$beta_hat, pch=20, col=2)
legend("topleft", legend=c(expression(beta), expression(hat(beta))),
       pch=c(8, 20), col=c(1, 2))
plot(f$inclusion_prob, main="Inclusion Probabilities", ylab=expression(gamma))

</code></pre>


</div>