<div class="container">

<table style="width: 100%;"><tr>
<td>sdf_expand_grid</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create a Spark dataframe containing all combinations of inputs</h2>

<h3>Description</h3>

<p>Given one or more R vectors/factors or single-column Spark dataframes,
perform an expand.grid operation on all of them and store the result in
a Spark dataframe
</p>


<h3>Usage</h3>

<pre><code class="language-R">sdf_expand_grid(
  sc,
  ...,
  broadcast_vars = NULL,
  memory = TRUE,
  repartition = NULL,
  partition_by = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Each input variable can be either a R vector/factor or a Spark
dataframe. Unnamed inputs will assume the default names of 'Var1', 'Var2',
etc in the result, similar to what 'expand.grid' does for unnamed inputs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>broadcast_vars</code></td>
<td>
<p>Indicates which input(s) should be broadcasted to all
nodes of the Spark cluster during the join process (default: none).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p>Boolean; whether the resulting Spark dataframe should be
cached into memory (default: TRUE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>repartition</code></td>
<td>
<p>Number of partitions the resulting Spark dataframe should
have</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partition_by</code></td>
<td>
<p>Vector of column names used for partitioning the
resulting Spark dataframe, only supported for Spark 2.0+</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
sc &lt;- spark_connect(master = "local")
grid_sdf &lt;- sdf_expand_grid(sc, seq(5), rnorm(10), letters)

## End(Not run)

</code></pre>


</div>