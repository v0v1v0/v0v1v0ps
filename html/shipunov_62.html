<div class="container">

<table style="width: 100%;"><tr>
<td>Gradd</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Classification grid and decision boundaries</h2>

<h3>Description</h3>

<p>Adds to the 2D ordination either colored points to make classification grid, or lines to show decision boundaries</p>


<h3>Usage</h3>

<pre><code class="language-R">Gradd(model2var, data2var, spacing=75, what="points",
 trnsp=0.2, pch=16, cex=0.8, lwd=2, lty=2, lcol="grey", palette=NULL,
 type="ids", User.Predict=function(model2var, X) {}, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model2var</code></td>
<td>
<p>Model based on 'data2var' (see below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data2var</code></td>
<td>
<p>Data with _exactly_ 2 variables, e.g., result of PCA.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>spacing</code></td>
<td>
<p>Density of points to predict.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>what</code></td>
<td>
<p>What to draw: either "points" for classification grid, or "lines" for decision boundaries</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trnsp</code></td>
<td>
<p>Transparency of points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pch</code></td>
<td>
<p>Type of points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cex</code></td>
<td>
<p>Scale of points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lwd</code></td>
<td>
<p>Width of lines.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lty</code></td>
<td>
<p>Type of lines.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lcol</code></td>
<td>
<p>Color of lines.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>palette</code></td>
<td>
<p>Palette to use.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Type of the model: "ids", "lda", "tree", or "user" (see examples).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>User.Predict</code></td>
<td>
<p>Function to define in case of 'type="user"'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to points() or contour().</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Gradd() takes model and its 2D data, makes new data with the same range
but made of dense equidistantly spaced (grid-like) points, then predicts
class labels from this new data, probably also calculates decision
boundaries, and finally plots either points colored by predition, or
lines along boundaries.
</p>
<p>Before you run Gradd(), make the model. This model should have 'predict'
method, and use ids (to make colors) and exactly 2 variables with names
same as 'data2var' column names, e.g:
</p>
<p>model2var &lt;- somemodel(ids ~ ., data=cbind(ids, data2var))
</p>
<p>If the model type is "user", the Gradd() uses predefined
'User.Predict(model2var, X)' function which must return factor ids from
testing X data (see examples).
</p>
<p>To plot both lines and grid, use Gradd() twice.
</p>
<p>Gradd() is mainly a teching demo. It is useful if the goal is to
illustrate the general properties of the supervised method and/or
underlying data. It uses the entire 2D dataset to learn new data but
learning from training subset is also possible, see the Naive Bayes
example below.
</p>


<h3>Author(s)</h3>

<p>Alexey Shipunov</p>


<h3>Examples</h3>

<pre><code class="language-R">## SVM:
library(e1071)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.svm.pca &lt;- svm(Species ~ ., data=cbind(iris[5], iris.p))
plot(iris.p, type="n", main="SVM")
Gradd(iris.svm.pca, iris.p) # type="ids" (default)
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## LDA:
library(MASS)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.lda.pca &lt;- lda(Species ~ . , data=cbind(iris[5], iris.p))
plot(iris.p, type="n", main="LDA")
Gradd(iris.lda.pca, iris.p, type="lda")
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## tree::tree() (note how to draw decision boundaries):
library(tree)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.tree.pca &lt;- tree(Species ~ . , data=cbind(iris[5], iris.p))
plot(iris.p, type="n", main="tree")
Gradd(iris.tree.pca, iris.p, type="tree", what="lines")
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## randomForest:
library(randomForest)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.rf.pca &lt;- randomForest(Species ~ ., data=cbind(iris[5], iris.p))
plot(iris.p, type="n", main="randomForest")
Gradd(iris.rf.pca, iris.p) # type="ids" (default)
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## naiveBayes (note how to use training subsample):
library(e1071)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
sel &lt;- 1:nrow(iris) 
plot(iris.p, col=iris$Species, pch=ifelse(sel, 19, 1), main="naiveBayes")
iris.nb2 &lt;- naiveBayes(Species ~ ., data=cbind(iris[5], iris.p)[sel, ])
Gradd(iris.nb2, iris.p[sel, ], what="lines")

## rpart (note how to use MDS for the base plot):
iris.dist &lt;- dist(iris[, -5], method="manhattan")
iris.dist[iris.dist == 0] &lt;- abs(jitter(0))
library(MASS)
iris.m &lt;- isoMDS(iris.dist)$points
colnames(iris.m) &lt;- c("Dim1", "Dim2")
library(rpart)
iris.rpart.mds &lt;- rpart(Species ~ . , data=cbind(iris[5], iris.m))
plot(iris.m, type="n", main="rpart + MDS")
Gradd(iris.rpart.mds, iris.m, type="tree")
text(iris.m, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## QDA:
library(MASS)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.qda.pca &lt;- qda(Species ~ . , data=cbind(iris[5], iris.p))
plot(iris.p, type="n", main="QDA")
Gradd(iris.qda.pca, iris.p, type="lda")
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## nnet:
library(nnet)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
iris.nnet.pca &lt;- nnet(Species ~ . , data=cbind(iris[5], iris.p), size=4)
plot(iris.p, type="n", main="nnet")
Gradd(iris.nnet.pca, iris.p, type="tree")
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))

## kNN (note how to employ User.Predict()):
library(class)
iris.p &lt;- prcomp(iris[, -5])$x[, 1:2]
plot(iris.p, type="n", main="kNN")
Gradd(cbind(iris[5], iris.p), iris.p, type="user",
 User.Predict=function(model2var, X) knn(model2var[, 2:3], X, model2var[, 1], k=5))
text(iris.p, col=as.numeric(iris[, 5]), labels=abbreviate(iris[, 5], 1,
 method="both.sides"))
</code></pre>


</div>