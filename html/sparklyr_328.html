<div class="container">

<table style="width: 100%;"><tr>
<td>sdf_copy_to</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Copy an Object into Spark</h2>

<h3>Description</h3>

<p>Copy an object into Spark, and return an <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object wrapping the
copied object (typically, a Spark DataFrame).
</p>


<h3>Usage</h3>

<pre><code class="language-R">sdf_copy_to(sc, x, name, memory, repartition, overwrite, struct_columns, ...)

sdf_import(x, sc, name, memory, repartition, overwrite, struct_columns, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p>The associated Spark connection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> object from which a Spark DataFrame can be generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>The name to assign to the copied table in Spark.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p>Boolean; should the table be cached into memory?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>repartition</code></td>
<td>
<p>The number of partitions to use when distributing the
table across the Spark cluster. The default (0) can be used to avoid
partitioning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overwrite</code></td>
<td>
<p>Boolean; overwrite a pre-existing table with the name <code>name</code>
if one already exists?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>struct_columns</code></td>
<td>
<p>(only supported with Spark 2.4.0 or higher) A list of
columns from the source data frame that should be converted to Spark SQL
StructType columns.
The source columns can contain either json strings or nested lists.
All rows within each source column should have identical schemas (because
otherwise the conversion result will contain unexpected null values or
missing values as Spark currently does not support schema discovery on
individual rows within a struct column).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments, passed to implementing methods.</p>
</td>
</tr>
</table>
<h3>Advanced Usage</h3>

<p><code>sdf_copy_to</code> is an S3 generic that, by default, dispatches to
<code>sdf_import</code>. Package authors that would like to implement
<code>sdf_copy_to</code> for a custom object type can accomplish this by
implementing the associated method on <code>sdf_import</code>.
</p>


<h3>See Also</h3>

<p>Other Spark data frames: 
<code>sdf_distinct()</code>,
<code>sdf_random_split()</code>,
<code>sdf_register()</code>,
<code>sdf_sample()</code>,
<code>sdf_sort()</code>,
<code>sdf_weighted_sample()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
sc &lt;- spark_connect(master = "spark://HOST:PORT")
sdf_copy_to(sc, iris)

## End(Not run)

</code></pre>


</div>