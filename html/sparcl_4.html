<div class="container">

<table style="width: 100%;"><tr>
<td>HierarchicalSparseCluster.permute</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Choose tuning parameter for sparse hierarchical clustering</h2>

<h3>Description</h3>

<p>The tuning parameter controls the L1 bound on w, the feature weights. A permutation approach is used to select the tuning parameter.
</p>


<h3>Usage</h3>

<pre><code class="language-R">HierarchicalSparseCluster.permute(x, nperms = 10, wbounds = NULL,
dissimilarity=c("squared.distance",
"absolute.value"),standardize.arrays=FALSE)
## S3 method for class 'HierarchicalSparseCluster.permute'
plot(x,...) 
## S3 method for class 'HierarchicalSparseCluster.permute'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A nxp data matrix, with n observations and p feaures.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nperms</code></td>
<td>
<p>The number of permutations to perform.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wbounds</code></td>
<td>
<p>The sequence of tuning parameters to consider. The
tuning parameters are the L1 bound on w, the feature weights. If
NULL, then a default sequence will be used. If non-null, should be
greater than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dissimilarity</code></td>
<td>
<p>How should dissimilarity be computed? Default is
squared.distance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize.arrays</code></td>
<td>
<p>Should the arrays first be standardized?
Default is FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> not used. </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Let $d_ii'j$ denote the dissimilarity between observations i and i'
along feature j.
</p>
<p>Sparse hierarchical clustering seeks a p-vector of weights w (one per
feature) and a nxn  matrix U that optimize
$maximize_U,w sum_j w_j  sum_ii' d_ii'j U_ii'$ subject to $||w||_2 &lt;= 1,
||w||_1 &lt;= s, w_j &gt;= 0, sum_ii' U_ii'^2 &lt;= 1$,
where  s is a value for the L1 bound on w. Let O(s) denote the
objective function with tuning parameter s: i.e. $O(s)=sum_j w_j
sum_ii' d_ii'j U_ii'$.
</p>
<p>We permute the data as follows: within each feature, we permute the
observations. Using the permuted data, we can run sparse hierarchical clustering
with tuning parameter s, yielding the objective function O*(s). If we do
this repeatedly we can get a number of O*(s) values.
</p>
<p>Then, the Gap statistic is given by
$Gap(s)=log(O(s))-mean(log(O*(s)))$. The
optimal s is that which results in the highest Gap
statistic. Or, we
can choose the smallest s such that its Gap statistic is within
$sd(log(O*(s)))$ of the largest Gap statistic.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>gaps</code></td>
<td>
<p>The gap statistics obtained (one for each of the tuning
parameters tried). If O(s) is the objective function evaluated at
the tuning parameter s, and O*(s) is the same quantity but for the
permuted data, then Gap(s)=log(O(s))-mean(log(O*(s))).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sdgaps</code></td>
<td>
<p>The standard deviation of log(O*(s)), for each value of the
tuning parameter s.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nnonzerows</code></td>
<td>
<p>The number of features with non-zero weights, for
each value of the tuning parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>wbounds</code></td>
<td>
<p>The tuning parameters considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bestw</code></td>
<td>
<p>The value of the tuning parameter corresponding to the
highest gap statistic.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Daniela M. Witten and Robert Tibshirani</p>


<h3>References</h3>

<p>Witten and Tibshirani (2009) A framework for feature
selection in clustering.</p>


<h3>See Also</h3>

<p>HierarchicalSparseCluster, KMeansSparseCluster, KMeansSparseCluster.permute</p>


<h3>Examples</h3>

<pre><code class="language-R">  # Generate 2-class data
  set.seed(1)
  x &lt;- matrix(rnorm(100*50),ncol=50)
  y &lt;- c(rep(1,50),rep(2,50))
  x[y==1,1:25] &lt;- x[y==1,1:25]+2
  # Do tuning parameter selection for sparse hierarchical clustering
  perm.out &lt;- HierarchicalSparseCluster.permute(x, wbounds=c(1.5,2:6),
nperms=5)
  print(perm.out)
  plot(perm.out)
  # Perform sparse hierarchical clustering
  sparsehc &lt;- HierarchicalSparseCluster(dists=perm.out$dists, wbound=perm.out$bestw, 
method="complete")
  par(mfrow=c(1,2))
  plot(sparsehc)
  plot(sparsehc$hc, labels=rep("", length(y)))
  print(sparsehc)
  # Plot using knowledge of class labels in order to compare true class
  #   labels to clustering obtained
  par(mfrow=c(1,1))
  ColorDendrogram(sparsehc$hc,y=y,main="My Simulated
Data",branchlength=.007)
</code></pre>


</div>