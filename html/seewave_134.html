<div class="container">

<table style="width: 100%;"><tr>
<td>sh</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Shannon and Renyi spectral entropy</h2>

<h3>Description</h3>

<p>This function computes the Shannon or Renyi entropy of a frequency
spectrum</p>


<h3>Usage</h3>

<pre><code class="language-R">sh(spec, alpha = "shannon")</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>spec</code></td>
<td>
<p>a data set resulting of a spectral analysis obtained
with <code>spec</code> or <code>meanspec</code> (not in dB).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>a character string, by default <code>"shannon"</code> to compute Shannon
entropy, <code>"simpson"</code> to compute Simpson entropy otherwise a
numeric vector of length 1 with a value superior to 0 but
different to 1 to compute Renyi entropy. See the examples.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>. Shannon spectral entropy is calculated according to:<br></p>
<p style="text-align: center;"><code class="reqn">S = -\frac{\sum_{i = 1}^{N} y_i log_2(y_i)}{log_2(N)}</code>
</p>

<p>. Simpson or Gini-Simpson spectral entropy (or index) is computed according to:<br></p>
<p style="text-align: center;"><code class="reqn">GS = 1 - \sum_{i=1}^{N} y_{i}^2</code>
</p>

<p>. Renyi spectral entropy of order alpha is calucalted according to:<br></p>
<p style="text-align: center;"><code class="reqn">R = \frac{1}{1-\alpha} \times log_2(\sum_{i = 1}^{N} y_{i}^{\alpha})</code>
</p>

<p>with </p>
<p style="text-align: center;"><code class="reqn">\alpha \geq 0</code>
</p>

<p style="text-align: center;"><code class="reqn">\alpha \neq 1</code>
</p>

<p><em>y</em> = relative amplitude of the <em>i</em> frequency,<br></p>
<p style="text-align: center;"><code class="reqn">\sum_{i = 1}^{N} y_i = 1</code>
</p>
<p><br> 
and <em>N</em> = number of frequencies.<br></p>


<h3>Value</h3>

<p>A numeric vector of length 1 is returned.
The value has no unit.</p>


<h3>Note</h3>

<p>The Shannon entropy scaled between 0 and 1 is also known as Pielou's evenness index</p>


<h3>Note</h3>

<p>The Shannon spectral entropy of a noisy signal will tend towards 1 whereas
the Shannon spectral entropy of a pure tone signal will tend towards
0. See Han <em>et al</em>. for details regarding the Renyi entropy.</p>


<h3>Author(s)</h3>

<p>Jerome Sueur and Laurent Lellouch</p>


<h3>References</h3>

<p>Han, NC, Muniandy SV, Dayou J (2011) Acoustic classification of
Australian anurans based on hybrid spectral-entropy approach.  <em>Applied
Acoustics</em>. <br></p>
<p>Nunes RR, Almeida de MP, Sleigh JW (2004) -
Spectral entropy: a new method for anesthetic adequacy.
<em>Revista Brasileira de Anestesiologia</em>, <b>54</b>, 413-422.<br></p>
<p>Renyi A (1961) - On measures of information and entropy. Proceedings
of the 4th Berkeley Symposium on Mathematics, Statistics and
Probability 1960. pp. 547-561.<br></p>
<p>Simpson EH (1949) - Measurement of diversity. <em>Nature</em>, <b>163</b>, 688.<br></p>


<h3>See Also</h3>

<p><code>csh</code>,<code>th</code>, <code>H</code>, <code>sfm</code></p>


<h3>Examples</h3>

<pre><code class="language-R">a&lt;-synth(f=8000,d=1,cf=2000,plot=FALSE)
speca&lt;-spec(a,f=8000,at=0.5,plot=FALSE)
## Shannon spectral entropy
sh(speca)
# [1] 0.2336412
b&lt;-noisew(d=1,f=8000)
specb&lt;-spec(b,f=8000,at=0.5,plot=FALSE)
sh(specb)
# close to 1
## Renyi spectral entropy
sh(speca, alpha=2)
sh(speca, alpha=3)
</code></pre>


</div>