<div class="container">

<table style="width: 100%;"><tr>
<td>splsda</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit SPLSDA classification models</h2>

<h3>Description</h3>

<p>Fit a SPLSDA classification model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">splsda( x, y, K, eta, kappa=0.5,
    classifier=c('lda','logistic'), scale.x=TRUE, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p> Matrix of predictors. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> Vector of class indices. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p> Number of hidden components. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p> Thresholding parameter. <code>eta</code> should be between 0 and 1. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p> Parameter to control the effect of
the concavity of the objective function
and the closeness of original and surrogate direction vectors.
<code>kappa</code> is relevant only for multicategory classification.
<code>kappa</code> should be between 0 and 0.5. Default is 0.5. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classifier</code></td>
<td>
<p> Classifier used in the second step of SPLSDA.
Alternatives are <code>"logistic"</code> or <code>"lda"</code>.
Default is <code>"lda"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale.x</code></td>
<td>
<p> Scale predictors by dividing each predictor variable
by its sample standard deviation? </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> Other parameters to be passed through to <code>spls</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The SPLSDA method is described in detail in Chung and Keles (2010).
SPLSDA provides a two-stage approach for PLS-based classification with variable selection,
by directly imposing sparsity on the dimension reduction step of PLS
using sparse partial least squares (SPLS) proposed in Chun and Keles (2010).
<code>y</code> is assumed to have numerical values, 0, 1, ..., G,
where G is the number of classes subtracted by one.
The option <code>classifier</code> refers to the classifier used in the second step of SPLSDA
and <code>splsda</code> utilizes algorithms offered by <span class="pkg">MASS</span> and <span class="pkg">nnet</span> packages
for this purpose.
If <code>classifier="logistic"</code>, then either logistic regression or multinomial regression is used.
Linear discriminant analysis (LDA) is used if <code>classifier="lda"</code>.
<code>splsda</code> also utilizes algorithms offered by the <span class="pkg">pls</span> package for fitting <code>spls</code>.
The user should install <span class="pkg">pls</span>, <span class="pkg">MASS</span> and <span class="pkg">nnet</span> packages before using <code>splsda</code> functions.
</p>


<h3>Value</h3>

<p>A <code>splsda</code> object is returned.
print, predict, coef methods use this object.</p>


<h3>Author(s)</h3>

<p> Dongjun Chung and Sunduz Keles. </p>


<h3>References</h3>

<p>Chung D and Keles S (2010), 
"Sparse partial least squares classification for high dimensional data",
<em>Statistical Applications in Genetics and Molecular Biology</em>, Vol. 9, Article 17.
</p>
<p>Chun H and Keles S (2010), "Sparse partial least squares
for simultaneous dimension reduction and variable selection",
<em>Journal of the Royal Statistical Society - Series B</em>, Vol. 72, pp. 3â€“25. </p>


<h3>See Also</h3>

 <p><code>print.splsda</code>, <code>predict.splsda</code>, and <code>coef.splsda</code>. </p>


<h3>Examples</h3>

<pre><code class="language-R">data(prostate)
# SPLSDA with eta=0.8 &amp; 3 hidden components
f &lt;- splsda( prostate$x, prostate$y, K=3, eta=0.8, scale.x=FALSE )
print(f)
# Print out coefficients
coef.f &lt;- coef(f)
coef.f[ coef.f!=0, ]
</code></pre>


</div>