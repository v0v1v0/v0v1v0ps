<div class="container">

<table style="width: 100%;"><tr>
<td>RoughKMeans_SHELL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Rough k-Means Shell</h2>

<h3>Description</h3>

<p>RoughKMeans_SHELL performs rough k-means algorithms with  options for normalization and a 2D-plot of the results.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RoughKMeans_SHELL(clusterAlgorithm, dataMatrix, meansMatrix, nClusters, 
                  normalizationMethod, maxIterations, plotDimensions, 
                  colouredPlot, threshold, weightLower)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>clusterAlgorithm</code></td>
<td>
<p>Select 0 = classic k-means, 1 = Lingras &amp; West's rough k-means, 2 = Peters' rough k-means, 3 = <code class="reqn">\pi</code> rough k-means. Default: clusterAlgorithm = 3 (<code class="reqn">\pi</code> rough k-means).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2. Note: Plotting is limited to a maximum of 5 clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalizationMethod</code></td>
<td>
<p>1 = unity interval, 2 = normal distribution (sample variance), 3 = normal distribution (population variance). Any other value returns the matrix unchanged. Default: meansMatrix = 1 (unity interval).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plotDimensions</code></td>
<td>
<p>An integer vector of the length 2. Defines the to be plotted feature dimensions, i.e., max(plotDimensions = c(1:2)) &lt;= nFeatures. Default: plotDimensions = c(1:2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>colouredPlot</code></td>
<td>
<p>Select TRUE = colouredPlot plot, FALSE = black/white plot.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>Relative threshold in rough k-means algorithms (threshold &gt;= 1.0).  Default: threshold = 1.5. Note: It can be ignored for classic k-means.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weightLower</code></td>
<td>
<p>Weight of the lower approximation in rough k-means algorithms (0.0 &lt;= weightLower &lt;= 1.0).  Default: weightLower = 0.7. Note: It can be ignored for classic k-means and <code class="reqn">\pi</code> rough k-means</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>2D-plot of clustering results. The boundary objects are represented by stars (*).
</p>
<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Lloyd, S.P. (1982) Least squares quantization in PCM. <em>IEEE Transactions on Information Theory</em> <b>28</b>, 128–137. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Lingras, P. and West, C. (2004) Interval Set Clustering of web users with rough k-means. <em>Journal of Intelligent Information Systems</em> <b>23</b>, 5–16. &lt;doi:10.1023/b:jiis.0000029668.88665.1a&gt;.
</p>
<p>Peters, G. (2006) Some refinements of rough k-means clustering. <em>Pattern Recognition</em> <b>39</b>, 1481–1491. &lt;doi:10.1016/j.patcog.2006.02.002&gt;.
</p>
<p>Lingras, P. and Peters, G. (2011) Rough Clustering. <em>WIREs Data Mining and Knowledge Discovery</em> <b>1</b>, 64–72. &lt;doi:10.1002/widm.16&gt;.
</p>
<p>Lingras, P. and Peters, G. (2012) Applying rough set concepts to clustering. In: Peters, G.; Lingras, P.; Slezak, D. and Yao, Y. Y. (Eds.) <em>Rough Sets: Selected Methods and Applications in Management and Engineering</em>, Springer, 23–37. &lt;doi:10.1007/978-1-4471-2760-4_2&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering – fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307–322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G. (2014) Rough clustering utilizing the principle of indifference. <em>Information Sciences</em> <b>277</b>, 358–374. &lt;doi:10.1016/j.ins.2014.02.073&gt;.
</p>
<p>Peters, G. (2015) Is there any need for rough clustering?  <em>Pattern Recognition Letters</em> <b>53</b>, 31–37. &lt;doi:10.1016/j.patrec.2014.11.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># An illustrative example clustering the sample data set DemoDataC2D2a.txt
RoughKMeans_SHELL(3, DemoDataC2D2a, 2, 2, 1, 100, c(1:2), TRUE, 1.5, 0.7)
</code></pre>


</div>