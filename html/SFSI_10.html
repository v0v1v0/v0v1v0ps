<div class="container">

<table style="width: 100%;"><tr>
<td>4. Least Angle Regression (LARS)</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Least Angle Regression to solve LASSO-type problems</h2>

<h3>Description</h3>

<p>Computes the entire LASSO solution for the regression coefficients, starting from zero, to the
least-squares estimates, via the Least Angle Regression (LARS) algorithm (Efron, 2004). It uses as inputs
a variance matrix among predictors and a covariance vector between response and predictors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LARS(Sigma, Gamma, method = c("LAR","LASSO"),
     nsup.max = NULL, steps.max = NULL, 
     eps = .Machine$double.eps*100, scale = TRUE, 
     sdx = NULL, mc.cores = 1L, save.at = NULL,
     precision.format = c("double","single"),
     fileID = NULL, verbose = 1)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Sigma</code></td>
<td>
<p>(numeric matrix) Variance-covariance matrix of predictors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Gamma</code></td>
<td>
<p>(numeric matrix) Covariance between response variable and predictors. If it contains more than one column, the algorithm is applied to each column separately as different response variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>(character) Either:
</p>

<ul>
<li> <p><code>'LAR'</code>: Computes the entire sequence of all coefficients. Values of lambdas are calculated at each step.
</p>
</li>
<li> <p><code>'LASSO'</code>: Similar to <code>'LAR'</code> but solutions when a predictor leaves the solution are also returned.
</p>
</li>
</ul>
<p>Default is <code>method = 'LAR'</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsup.max</code></td>
<td>
<p>(integer) Maximum number of non-zero coefficients in the last LARS solution.
Default <code>nsup.max = NULL</code> will calculate solutions for the entire lambda sequence</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>steps.max</code></td>
<td>
<p>(integer) Maximum number of steps (i.e., solutions) to be computed. Default <code>steps.max = NULL</code> will calculate solutions for the entire lambda sequence</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>(numeric) A numerical zero. Default is the machine precision</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> to scale matrix <code>Sigma</code> for variables with unit variance and scale <code>Gamma</code> by the standard deviation (<code>sdx</code>) of the corresponding predictor
taken from the diagonal of <code>Sigma</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sdx</code></td>
<td>
<p>(numeric vector) Scaling factor that will be used to scale the regression coefficients. When <code>scale = TRUE</code> this scaling factor vector is set to the squared root of the diagonal of <code>Sigma</code>, otherwise a provided value is used assuming that <code>Sigma</code> and <code>Gamma</code> are scaled</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mc.cores</code></td>
<td>
<p>(integer) Number of cores used. When <code>mc.cores</code> &gt; 1, the analysis is run in parallel for each column of <code>Gamma</code>. Default is <code>mc.cores = 1</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save.at</code></td>
<td>
<p>(character) Path where regression coefficients are to be saved (this may include a prefix added to the files). Default <code>save.at = NULL</code> will no save the regression coefficients and they are returned in the output object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fileID</code></td>
<td>
<p>(character) Suffix added to the file name where regression coefficients are to be saved. Default <code>fileID = NULL</code> will automatically add sequential integers from 1 to the number of columns of <code>Gamma</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precision.format</code></td>
<td>
<p>(character) Either 'single' or 'double' for numeric precision and memory occupancy (4 or 8 bytes, respectively) of the regression coefficients. This is only used when <code>save.at</code> is not <code>NULL</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If numeric greater than zero details on each LARS step will be printed</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Finds solutions for the regression coefficients in a linear model
</p>
<p style="text-align:center">y<sub>i</sub> = <b>x</b>'<sub>i</sub><b>β</b> + e<sub>i</sub></p>
<p>where
y<sub>i</sub> is the response for the i<sup>th</sup> observation,
<b>x</b><sub>i</sub> = (x<sub>i1</sub>,...,x<sub>ip</sub>)'
is a vector of <code class="reqn">p</code> predictors assumed to have unit variance,
<b>β</b> = (β<sub>1</sub>,...,β<sub>p</sub>)'
is a vector of regression coefficients, and
e<sub>i</sub>
is a residual.
</p>
<p>The regression coefficients
<b>β</b>
are estimated as function of the variance matrix among
predictors (<b>Σ</b>) and the covariance vector between response and predictors (<b>Γ</b>) by minimizing the penalized mean squared error function
</p>
<p style="text-align:center">-<b>Γ</b>' <b>β</b> + 1/2 <b>β</b>'<b>Σ</b><b>β</b> + 1/2 λ ||<b>β</b>||<sub>1</sub></p>
<p>where λ
is the penalization parameter and
||<b>β</b>||<sub>1</sub> = ∑<sub>j=1</sub>|β<sub>j</sub>|
is the L1-norm.
</p>
<p>The algorithm to find solutions for each β<sub>j</sub> is fully described in Efron (2004) in which the "current correlation" between the predictor
x<sub>ij</sub>
and the residual
e<sub>i</sub> = y<sub>i</sub> - <b>x</b>'<sub>i</sub><b>β</b>
is expressed (up-to a constant) as
</p>
<p style="text-align:center">r<sub>j</sub> = Γ<sub>j</sub> - <b>Σ</b>'<sub>j</sub><b>β</b></p>
<p>where
Γ<sub>j</sub>
is the j<sup>th</sup> element of
<b>Γ</b> and
<b>Σ</b><sub>j</sub>
is the j<sup>th</sup> column of the matrix
<b>Σ</b>
</p>


<h3>Value</h3>

<p>Returns a list object with the following elements:
</p>

<ul>
<li> <p><code>lambda</code>: (vector) all the sequence of values of the LASSO penalty.
</p>
</li>
<li> <p><code>beta</code>: (matrix) regression coefficients for each predictor (in rows) associated to each value of the penalization parameter lambda (in columns).
</p>
</li>
<li> <p><code>nsup</code>: (vector) number of non-zero predictors associated to each value of lambda.
</p>
</li>
</ul>
<p>The returned object is of the class 'LASSO' for which methods <code>coef</code> and <code>predict</code> exist. Function 'path.plot' can be also used
</p>


<h3>Author(s)</h3>

<p>Adapted from the 'lars' function in package 'lars' (Hastie &amp; Efron, 2013)
</p>


<h3>References</h3>

<p>Efron B, Hastie T, Johnstone I, Tibshirani R (2004). Least angle regression. <em>The Annals of Statistics</em>, <b>32</b>(2), 407–499.
</p>
<p>Hastie T, Efron B (2013). lars: least angle regression, Lasso and forward stagewise. <a href="https://cran.r-project.org/package=lars">https://cran.r-project.org/package=lars</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">  require(SFSI)
  data(wheatHTP)
  
  y = as.vector(Y[,"E1"])   # Response variable
  X = scale(X_E1)           # Predictors

  # Training and testing sets
  tst = which(Y$trial %in% 1:10)
  trn = seq_along(y)[-tst]

  # Calculate covariances in training set
  XtX = var(X[trn,])
  Xty = cov(X[trn,],y[trn])
  
  # Run the penalized regression
  fm = LARS(XtX, Xty, method="LASSO")  
  
  # Regression coefficients
  dim(coef(fm))
  dim(coef(fm, ilambda=50)) # Coefficients associated to the 50th lambda
  dim(coef(fm, nsup=25))    # Coefficients with around nsup=25 are non-zero

  # Predicted values
  yHat1 = predict(fm, X=X[trn,])  # training data
  yHat2 = predict(fm, X=X[tst,])  # testing data
  
  # Penalization vs correlation
  plot(-log(fm$lambda[-1]),cor(y[trn],yHat1[,-1]), main="Training", type="l")
  plot(-log(fm$lambda[-1]),cor(y[tst],yHat2[,-1]), main="Testing", type="l")
</code></pre>


</div>