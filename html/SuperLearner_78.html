<div class="container">

<table style="width: 100%;"><tr>
<td>SL.glmnet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Elastic net regression, including lasso and ridge</h2>

<h3>Description</h3>

<p>Penalized regression using elastic net. Alpha = 0 corresponds to ridge
regression and alpha = 1 corresponds to Lasso.
</p>
<p>See <code>vignette("glmnet_beta", package = "glmnet")</code> for a nice tutorial on
glmnet.
</p>


<h3>Usage</h3>

<pre><code class="language-R">SL.glmnet(Y, X, newX, family, obsWeights, id, alpha = 1, nfolds = 10,
  nlambda = 100, useMin = TRUE, loss = "deviance", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Outcome variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Covariate dataframe</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newX</code></td>
<td>
<p>Dataframe to predict the outcome</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>"gaussian" for regression, "binomial" for binary
classification. Untested options: "multinomial" for multiple classification
or "mgaussian" for multiple response, "poisson" for non-negative outcome
with proportional mean and variance, "cox".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obsWeights</code></td>
<td>
<p>Optional observation-level weights</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>id</code></td>
<td>
<p>Optional id to group observations from the same unit (not used
currently).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Elastic net mixing parameter, range [0, 1]. 0 = ridge regression
and 1 = lasso.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>Number of folds for internal cross-validation to optimize lambda.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>Number of lambda values to check, recommended to be 100 or more.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>useMin</code></td>
<td>
<p>If TRUE use lambda that minimizes risk, otherwise use 1
standard-error rule which chooses a higher penalty with performance within
one standard error of the minimum (see Breiman et al. 1984 on CART for
background).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Loss function, can be "deviance", "mse", or "mae". If family =
binomial can also be "auc" or "class" (misclassification error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Any additional arguments are passed through to cv.glmnet.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
</p>
<p>Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge regression: Biased estimation
for nonorthogonal problems. Technometrics, 12(1), 55-67.
</p>
<p>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 267-288.
</p>
<p>Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2), 301-320.
</p>


<h3>See Also</h3>

<p><code>predict.SL.glmnet</code> <code>cv.glmnet</code>
<code>glmnet</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Load a test dataset.
data(PimaIndiansDiabetes2, package = "mlbench")
data = PimaIndiansDiabetes2

# Omit observations with missing data.
data = na.omit(data)

Y = as.numeric(data$diabetes == "pos")
X = subset(data, select = -diabetes)

set.seed(1, "L'Ecuyer-CMRG")

sl = SuperLearner(Y, X, family = binomial(),
                  SL.library = c("SL.mean", "SL.glm", "SL.glmnet"))
sl

</code></pre>


</div>