<div class="container">

<table style="width: 100%;"><tr>
<td>SL.ksvm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Wrapper for Kernlab's SVM algorithm</h2>

<h3>Description</h3>

<p>Wrapper for Kernlab's support vector machine algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">SL.ksvm(Y, X, newX, family, type = NULL, kernel = "rbfdot",
  kpar = "automatic", scaled = T, C = 1, nu = 0.2, epsilon = 0.1,
  cross = 0, prob.model = family$family == "binomial",
  class.weights = NULL, cache = 40, tol = 0.001, shrinking = T, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Outcome variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Training dataframe</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newX</code></td>
<td>
<p>Test dataframe</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Gaussian or binomial</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>ksvm can be used for classification , for regression, or for
novelty detection. Depending on whether y is a factor or not, the default
setting for type is C-svc or eps-svr, respectively, but can be overwritten
by setting an explicit value. See ?ksvm for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p>the kernel function used in training and predicting. This
parameter can be set to any function, of class kernel, which computes the
inner product in feature space between two vector arguments. See ?ksvm for
more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kpar</code></td>
<td>
<p>the list of hyper-parameters (kernel parameters). This is a list
which contains the parameters to be used with the kernel function. See
?ksvm for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scaled</code></td>
<td>
<p>A logical vector indicating the variables to be scaled. If
scaled is of length 1, the value is recycled as many times as needed and
all non-binary variables are scaled. Per default, data are scaled
internally (both x and y variables) to zero mean and unit variance. The
center and scale values are returned and used for later predictions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>C</code></td>
<td>
<p>cost of constraints violation (default: 1) this is the 'C'-constant
of the regularization term in the Lagrange formulation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>parameter needed for nu-svc, one-svc, and nu-svr. The nu parameter
sets the upper bound on the training error and the lower bound on the
fraction of data points to become Support Vectors (default: 0.2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>epsilon in the insensitive-loss function used for eps-svr,
nu-svr and eps-bsvm (default: 0.1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cross</code></td>
<td>
<p>if a integer value k&gt;0 is specified, a k-fold cross validation
on the training data is performed to assess the quality of the model: the
accuracy rate for classification and the Mean Squared Error for regression</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob.model</code></td>
<td>
<p>if set to TRUE builds a model for calculating class
probabilities or in case of regression, calculates the scaling parameter of
the Laplacian distribution fitted on the residuals. Fitting is done on
output data created by performing a 3-fold cross-validation on the training
data. (default: FALSE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class.weights</code></td>
<td>
<p>a named vector of weights for the different classes,
used for asymmetric class sizes. Not all factor levels have to be supplied
(default weight: 1). All components have to be named.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cache</code></td>
<td>
<p>cache memory in MB (default 40)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>tolerance of termination criterion (default: 0.001)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinking</code></td>
<td>
<p>option whether to use the shrinking-heuristics (default: TRUE)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Any additional parameters, not currently passed through.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>List with predictions and the original training data &amp;
hyperparameters.
</p>


<h3>References</h3>

<p>Hsu, C. W., Chang, C. C., &amp; Lin, C. J. (2016). A practical guide to support
vector classification. <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf</a>
</p>
<p>Scholkopf, B., &amp; Smola, A. J. (2001). Learning with kernels: support vector
machines, regularization, optimization, and beyond. MIT press.
</p>
<p>Vapnik, V. N. (1998). Statistical learning theory (Vol. 1). New York: Wiley.
</p>
<p>Zeileis, A., Hornik, K., Smola, A., &amp; Karatzoglou, A. (2004). kernlab-an S4
package for kernel methods in R. Journal of statistical software, 11(9),
1-20.
</p>


<h3>See Also</h3>

<p><code>predict.SL.ksvm</code> <code>ksvm</code>
<code>predict.ksvm</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(Boston, package = "MASS")
Y = Boston$medv
# Remove outcome from covariate dataframe.
X = Boston[, -14]

set.seed(1)

sl = SuperLearner(Y, X, family = gaussian(),
                 SL.library = c("SL.mean", "SL.ksvm"))
sl

pred = predict(sl, X)
summary(pred$pred)

</code></pre>


</div>