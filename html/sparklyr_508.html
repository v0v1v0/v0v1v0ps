<div class="container">

<table style="width: 100%;"><tr>
<td>stream_write_csv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Write files to the stream</h2>

<h3>Description</h3>

<p>Write files to the stream
</p>


<h3>Usage</h3>

<pre><code class="language-R">stream_write_csv(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoint"),
  header = TRUE,
  delimiter = ",",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_text(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_json(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_parquet(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_orc(
  x,
  path,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path(path, "checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_kafka(
  x,
  mode = c("append", "complete", "update"),
  trigger = stream_trigger_interval(),
  checkpoint = file.path("checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)

stream_write_console(
  x,
  mode = c("append", "complete", "update"),
  options = list(),
  trigger = stream_trigger_interval(),
  partition_by = NULL,
  ...
)

stream_write_delta(
  x,
  path,
  mode = c("append", "complete", "update"),
  checkpoint = file.path("checkpoints", random_string("")),
  options = list(),
  partition_by = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A Spark DataFrame or dplyr operation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the ‘<span class="samp">⁠"hdfs://"⁠</span>’, ‘<span class="samp">⁠"s3a://"⁠</span>’ and ‘<span class="samp">⁠"file://"⁠</span>’ protocols.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mode</code></td>
<td>
<p>Specifies how data is written to a streaming sink. Valid values are
<code>"append"</code>, <code>"complete"</code> or <code>"update"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trigger</code></td>
<td>
<p>The trigger for the stream query, defaults to micro-batches
running every 5 seconds. See <code>stream_trigger_interval</code> and
<code>stream_trigger_continuous</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>checkpoint</code></td>
<td>
<p>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>header</code></td>
<td>
<p>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delimiter</code></td>
<td>
<p>The character used to delimit each column, defaults to <code>,</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quote</code></td>
<td>
<p>The character used as a quote. Defaults to ‘<span class="samp">⁠'"'⁠</span>’.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>escape</code></td>
<td>
<p>The character used to escape other characters, defaults to <code>\</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>charset</code></td>
<td>
<p>The character set, defaults to <code>"UTF-8"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>null_value</code></td>
<td>
<p>The character to use for default values, defaults to <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partition_by</code></td>
<td>
<p>Partitions the output by the given list of columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p>Other Spark stream serialization: 
<code>stream_write_memory()</code>,
<code>stream_write_table()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

sc &lt;- spark_connect(master = "local")

dir.create("csv-in")
write.csv(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- file.path("file://", getwd(), "csv-in")

stream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_csv("csv-out")

stream_stop(stream)

## End(Not run)

</code></pre>


</div>