<div class="container">

<table style="width: 100%;"><tr>
<td>calc_stats</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate various statistics from a confusion matrix</h2>

<h3>Description</h3>

<p>Given a frequency table of predictions versus target values,
calculate numerous statistics of interest.
</p>


<h3>Usage</h3>

<pre><code class="language-R">calc_stats(tabble, prevalence = NULL, positive, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tabble</code></td>
<td>
<p>A frequency table created with <code>table</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prevalence</code></td>
<td>
<p>Prevalence value. Default is <code>NULL</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>Positive class</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other, not currently used</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Used within confusion_matrix to calculate various confusion matrix
metrics. This is called by <code>confusion_matrix</code>, but if this is all you
want you can simply supply the table.
</p>
<p>Suppose a 2x2 table with notation
</p>

<table>
<tr>
<td style="text-align: right;"> </td>
<td style="text-align: center;"> target </td>
<td style="text-align: center;"> </td>
</tr>
<tr>
<td style="text-align: right;"> Predicted </td>
<td style="text-align: center;"> Event </td>
<td style="text-align: center;"> No Event
</td>
</tr>
<tr>
<td style="text-align: right;"> Event </td>
<td style="text-align: center;"> A </td>
<td style="text-align: center;"> B </td>
</tr>
<tr>
<td style="text-align: right;"> No Event </td>
<td style="text-align: center;"> C </td>
<td style="text-align: center;"> D </td>
</tr>
<tr>
<td style="text-align: right;"> </td>
</tr>
</table>
<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Positive Predictive Value = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">Negative Predictive Value = (specificity * (1-prevalence))/(((1-sensitivity)*prevalence) + ((specificity)*(1-prevalence)))</code>
</p>
 <p style="text-align: center;"><code class="reqn">Detection Rate = A/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Detection Prevalence = (A+B)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Balanced Accuracy = (sensitivity+specificity)/2</code>
</p>

<p style="text-align: center;"><code class="reqn">Precision = A/(A+B)</code>
</p>

<p style="text-align: center;"><code class="reqn">Recall = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">F1 = harmonic mean of precision and recall = (1+beta^2)*precision*recall/((beta^2 * precision)+recall)</code>
</p>

<p>where <code>beta = 1</code> for this function.
</p>
<p style="text-align: center;"><code class="reqn">False Discovery Rate = 1 - Positive Predictive Value</code>
</p>

<p style="text-align: center;"><code class="reqn">False Omission Rate = 1 - Negative Predictive Value</code>
</p>

<p style="text-align: center;"><code class="reqn">False Positive Rate = 1 - Specificity</code>
</p>

<p style="text-align: center;"><code class="reqn">False Negative Rate = 1 - Sensitivity</code>
</p>

<p style="text-align: center;"><code class="reqn">D' = qnorm(Sensitivity) - qnorm(1 - Specificity)</code>
</p>

<p style="text-align: center;"><code class="reqn">AUC ~= pnorm(D'/sqrt(2))</code>
</p>

<p>See the references for discussions of the first five formulas.
Abbreviations:
</p>

<dl>
<dt>Positive Predictive Value: PPV</dt>
<dd></dd>
<dt>Negative Predictive Value: NPV</dt>
<dd></dd>
<dt>False Discovery Rate: FDR</dt>
<dd></dd>
<dt>False Omission Rate: FOR</dt>
<dd></dd>
<dt>False Positive Rate: FPR</dt>
<dd></dd>
<dt>False Negative Rate: FNR</dt>
<dd></dd>
</dl>
<h3>Value</h3>

<p>A tibble with (at present) columns for sensitivity, specificity, PPV, NPV, F1 score, detection rate, detection prevalence, balanced accuracy, FDR, FOR, FPR, FNR.  For
more than 2 classes, these statistics are provided for each class.
</p>


<h3>Note</h3>

<p>Different names are used for the same statistics.
</p>

<dl>
<dt>Sensitivity: True Positive Rate, Recall, Hit Rate, Power</dt>
<dd></dd>
<dt>Specificity: True Negative Rate</dt>
<dd></dd>
<dt>Positive Predictive Value: Precision</dt>
<dd></dd>
<dt>False Negative Rate: Miss Rate, Type II error rate, beta</dt>
<dd></dd>
<dt>False Positive Rate: Fallout, Type I error rate, alpha</dt>
<dd></dd>
</dl>
<p>This function is called by <code>confusion_matrix</code>, but if this is all you
want, you can simply supply the table to this function.
</p>


<h3>Author(s)</h3>

<p>Michael Clark (see <a href="https://github.com/m-clark/confusionMatrix">m-clark/confusion_matrix</a>).
</p>


<h3>References</h3>

<p>Kuhn, M. (2008), "Building predictive models in R using the
caret package, " <em>Journal of Statistical Software</em>,
(<a href="https://www.jstatsoft.org/article/view/v028i05">https://www.jstatsoft.org/article/view/v028i05</a>).
</p>
<p>Altman, D.G., Bland, J.M. (1994) "Diagnostic tests 1: sensitivity and
specificity", <em>British Medical Journal</em>, vol 308, 1552.
</p>
<p>Altman, D.G., Bland, J.M. (1994) "Diagnostic tests 2: predictive values,"
<em>British Medical Journal</em>, vol 309, 102.
</p>
<p>Velez, D.R., et. al. (2008) "A balanced accuracy function for epistasis
modeling in imbalanced datasets using multifactor dimensionality
reduction.," <em>Genetic Epidemiology</em>, vol 4, 306.
</p>


</div>