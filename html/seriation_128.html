<div class="container">

<table style="width: 100%;"><tr>
<td>seriate</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Seriate Dissimilarity Matrices, Matrices or Arrays</h2>

<h3>Description</h3>

<p>Tries to find a linear order for objects using data in the form of a
dissimilarity matrix (two-way one-mode data), a data matrix (two-way
two-mode data), or a data array (k-way k-mode data). The order can then be
used to reorder the dissimilarity matrix/data matrix using
<code>permute()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">seriate(x, ...)

## S3 method for class 'dist'
seriate(x, method = "Spectral", control = NULL, rep = 1L, ...)

## S3 method for class 'matrix'
seriate(x, method = "PCA", control = NULL, margin = c(1L, 2L), rep = 1L, ...)

## S3 method for class 'array'
seriate(
  x,
  method = "PCA",
  control = NULL,
  margin = seq(length(dim(x))),
  rep = 1L,
  ...
)

## S3 method for class 'data.frame'
seriate(
  x,
  method = "Heatmap",
  control = NULL,
  margin = c(1L, 2L),
  rep = 1L,
  ...
)

## S3 method for class 'table'
seriate(x, method = "CA", control = NULL, margin = c(1L, 2L), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments are added to the <code>control</code> list.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string with the name of the seriation method
(default: varies by data type).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>a list of control options passed on to the seriation
algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rep</code></td>
<td>
<p>number of random restarts for randomized methods.
Uses <code>seriate_rep()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>margin</code></td>
<td>
<p>an integer vector giving the margin indices (dimensions) to be
seriated. For example, for a matrix, <code>1</code> indicates rows, <code>2</code>
indicates columns, <code>c(1 ,2)</code> means rows and columns.
Unseriated margins return the identity seriation order for that margin.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Seriation methods are managed via a registry. See
<code>list_seriation_methods()</code> for help. In the following, we focus on
discussing the
built-in methods that are registered automatically by the package <span class="pkg">seriation</span>.
</p>
<p>The available control options, default settings, and
a description for each algorithm
can be retrieved using <code>get_seriation_method(name = "&lt;seriation method&gt;")</code>.
Some control parameters are also described in more detail below.
</p>
<p>Some methods are very slow, and progress can be printed using the control
parameter <code>verbose = TRUE</code>.
</p>
<p>Many seriation methods (heuristically) optimize (minimize or maximize) an
objective function often called seriation criterion.
The value of the seriation criterion for a given order can be
calculated using <code>criterion()</code>. In this manual page, we
include the criterion, which is optimized by each method using <strong>bold font</strong>.
If no criterion is mentioned, then the method does not directly optimize a criterion.
A definition of the different seriation criteria can be found on the <code>criterion()</code> manual page.
</p>
<p><strong>Seriation methods for distance matrices (dist)</strong>
</p>
<p>One-mode two-way data must be provided as a dist object (not
a symmetric matrix). Similarities have to be transformed into
dissimilarities.
Seriation algorithms fall into different groups based on the approach.
In the following, we describe the currently implemented methods.
A list with all methods and the available parameters is available
<a href="https://mhahsler.github.io/seriation/seriation_methods.html">here</a>.
<a href="https://michael.hahsler.net/research/paper/EJOR_seriation_2016.pdf">Hahsler (2017)</a>
for a more detailed description and an experimental comparison of the most
popular methods.
</p>
<p><strong>Dendrogram leaf order</strong>
</p>
<p>These methods create a dendrogram using hierarchical clustering and then derive
the seriation order from the leaf order in the dendrogram. Leaf reordering
may be applied.
</p>

<ul>
<li> <p><strong>Hierarchical clustering:</strong> <code>"HC"</code>, <code>"HC_single"</code>, <code>"HC_complete"</code>,
<code>"HC_average"</code>, <code>"HC_ward"</code>
</p>
<p>Uses the order of the leaf nodes in a dendrogram obtained by hierarchical
clustering as a simple seriation technique. This method
applies hierarchical clustering (<code>stats::hclust()</code>) to <code>x</code>. The clustering
method can be given using a <code>"linkage"</code> element in the <code>control</code>
list. If omitted, the default <code>"complete"</code> is used.
For convenience, the other methods are provided as shortcuts.
</p>
</li>
<li> <p><strong>Reordered by the Gruvaeus and Wainer heuristic:</strong> <code>"GW"</code>, <code>"GW_single"</code>, <code>"GW_average"</code>,
<code>"GW_complete"</code>, <code>"GW_ward"</code>  (Gruvaeus and Wainer, 1972)
</p>
<p>Method <code>"GW"</code> uses an algorithm developed by Gruvaeus and Wainer (1972)
as implemented <code>gclus::reorder.hclust()</code> (Hurley 2004).  The clusters are
ordered at each level so that the objects at the edge of each cluster are
adjacent to the nearest object outside the cluster. The
method produces a unique order.
</p>
<p>The methods start with a dendrogram created by <code>hclust()</code>. As the
<code>"linkage"</code> element in the <code>control</code> list, a clustering method
(default <code>"average"</code>) can be specified. Alternatively, an stats::hclust
object can be supplied using an element named <code>"hclust"</code>.
</p>
<p>A dendrogram (binary tree) has <code class="reqn">2^{n-1}</code> internal nodes (subtrees) and
the same number of leaf orderings. That is, at each internal node, the left
and right subtree (or leaves) can be swapped or, in terms of a dendrogram,
be flipped. The leaf-node reordering to minimize
</p>
<p>Minimizes the <strong>Hamiltonian path length (restricted by the dendrogram)</strong>.
</p>
</li>
<li> <p><strong>Reordered by optimal leaf ordering:</strong> <code>"OLO"</code>, <code>"OLO_single"</code>,
<code>"OLO_average"</code>, <code>"OLO_complete"</code>, <code>"OLO_ward"</code>  (Bar-Joseph et al., 2001)
</p>
<p>Starts with a dendrogram and
produces an optimal leaf ordering that minimizes the sum of
the distances along the (Hamiltonian) path connecting the leaves in the
given order. The algorithm's time complexity is <code class="reqn">O(n^3)</code>. Note that
non-finite distance values are not allowed.
</p>
<p>Minimizes the <strong>Hamiltonian path length (restricted by the dendrogram)</strong>.
</p>
</li>
<li> <p><strong>Dendrogram seriation:</strong> <code>"DendSer"</code> (Earle and Hurley, 2015)
</p>
<p>Use heuristic dendrogram seriation to optimize for various criteria.
The DendSer code has to be first registered. A
detailed description can be found on the manual page for
<code>register_DendSer()</code>.
</p>
</li>
</ul>
<p><strong>Dimensionality reduction</strong>
</p>
<p>Find a seriation order by reducing the dimensionality to 1 dimension. This is typically
done by minimizing a stress measure or the reconstruction error.
Note that dimensionality reduction to a single dimension is a very
difficult discrete optimization problem.
For example, MDS algorithms used for a single dimension
tend to end up in local optima (see Maier and De Leeuw, 2015).
However, generally, ordering along a single component of MDS provides good results
sufficient for applications like visualization.
</p>

<ul>
<li> <p><strong>Classical metric multidimensional scaling:</strong> <code>"MDS"</code>
</p>
<p>Orders along the 1D classical metric multidimensional scaling.
<code>control</code> parameters are passed on to <code>stats::cmdscale()</code>.
</p>
</li>
<li> <p><strong>Isometric feature mapping:</strong> <code>"isomap"</code> (Tenenbaum, 2000)
</p>
<p>Orders along the 1D isometric feature mapping.
<code>control</code> parameters are passed on to <code>vegan::isomap()</code>
</p>
</li>
<li> <p><strong>Kruskal's non-metric multidimensional scaling:</strong> <code>"isoMDS"</code>, <code>"monoMDS"</code>,
<code>"metaMDS"</code> (Kruskal, 1964)
</p>
<p>Orders along the 1D Kruskal's non-metric multidimensional scaling.
Package <span class="pkg">vegan</span> provides an alternative implementation called <code>monoMDS</code>
and a version that uses random restarts for stability called <code>metaMDS</code>.
<code>control</code> parameters are passed on to <code>MASS::isoMDS()</code>, <code>vegan::monoMDS()</code> or <code>vegan::metaMDS()</code>.
</p>
</li>
<li> <p><strong>Sammon's non-linear mapping:</strong> <code>"Sammon_mapping"</code> (Sammon, 1969)
</p>
<p>Orders along the 1D Sammon's non-linear mapping.
<code>control</code> parameters are passed on to <code>MASS::sammon()</code>.
</p>
</li>
<li> <p><strong>Angular order of the first two eigenvectors:</strong> <code>"MDS_angle"</code>
</p>
<p>Finds a 2D configuration using MDS (<code>stats::cmdscale()</code>)
to approximate the eigenvectors of the covariance matrix in the
original data matrix.
Orders by the angle in this space and splits the order by the
larges gap between adjacent angles. A similar method was used by
Friendly (2002) to order variables in correlation matrices
by angles of first two eigenvectors.
</p>
</li>
<li> <p><strong>Smacof:</strong> <code>"MDS_smacof"</code> (de Leeuw and Mair, 2009)
</p>
<p>Perform seriation using stress majorization with several transformation functions.
This method has to be registered first using <code>register_smacof()</code>.
</p>
</li>
</ul>
<p><strong>Optimization</strong>
</p>
<p>These methods try to optimize a seriation criterion directly, typically using a
heuristic approach.
</p>

<ul>
<li> <p><strong>Anti-Robinson seriation by simulated annealing:</strong> <code>"ARSA"</code> (Brusco et al 2008)
</p>
<p>The algorithm automatically finds a suitable start temperature and calculates
the needed number of iterations. The algorithm gets slow for a large number of
objects. The speed can be improved by lowering the cooling parameter <code>"cool"</code>
or increasing the minimum temperature <code>"tmin"</code>.
However, this will decrease the seriation quality.
</p>
<p>Directly minimizes the <strong>linear seriation criterion (LS).</strong>
</p>
</li>
<li> <p><strong>Complete Enumeration:</strong> <code>"Enumerate"</code>
</p>
<p>This method finds the optimal permutation given a seriation criterion by complete enumeration
of all permutations.
The criterion is specified as the <code>control</code> parameters <code>"criterion"</code>.
Default is the weighted gradient measure. Use <code>"verbose = TRUE"</code> to see
the progress.
</p>
<p>Note: The number of permutations for <code class="reqn">n</code> objects is <code class="reqn">n!</code>.
Complete enumeration is only possible for tiny problems (&lt;10 objects) and is limited on most systems
to a problem size of up to 12 objects.
</p>
</li>
<li> <p><strong>Gradient measure seriation by branch-and-bound:</strong> <code>"BBURCG"</code>, <code>"BBWRCG"</code> (Brusco and Stahl 2005)
</p>
<p>The method uses branch-and-bound to minimize the
<strong>unweighted gradient measure</strong> (<code>"BBURCG"</code>) and the
<strong>weighted gradient measure</strong> (<code>"BBWRCG"</code>).
This type of optimization is only feasible for a small number of objects (&lt; 50 objects).
</p>
<p>For BBURCG, the control parameter <code>"eps"</code> can be used to relax the problem by defining
that a distance needs to be eps larger to count as a violation. This relaxation will improve the speed,
but miss some Robinson events. The default value is 0.
</p>
</li>
<li> <p><strong>Genetic Algorithm:</strong> <code>"GA"</code>
</p>
<p>The GA code has to be first registered. A detailed description can
be found on the manual page for <code>register_GA()</code>.
</p>
</li>
<li> <p><strong>Quadratic assignment problem seriation:</strong>
<code>"QAP_LS"</code>, <code>"QAP_2SUM"</code>, <code>"QAP_BAR"</code>, <code>"QAP_Inertia"</code> (Hahsler, 2017)
</p>
<p>Formulates the seriation problem as a quadratic assignment problem and applies a
simulated annealing solver to find a good solution.
These methods minimize the
<strong>Linear Seriation Problem</strong> (LS) formulation (Hubert and Schultz 1976),
the <strong>2-Sum Problem</strong> formulation (Barnard, Pothen, and Simon 1993), the
<strong>banded anti-Robinson form</strong> (BAR), or the <strong>inertia criterion</strong>.
</p>
<p><code>control</code> parameters are passed on to <code>qap::qap()</code>.
An important parameter is <code>rep</code> to return the best result from the
given number of repetitions with random restarts. The default is 1, but bigger
numbers result in better and more stable results.
</p>
</li>
<li> <p><strong>General Simulated Annealing:</strong> <code>"GSA"</code>
</p>
<p>Implement simulated annealing similar to the ARSA method. However, it
can optimize
for any criterion measure defined in <span class="pkg">seriation</span>. By default, the
algorithm optimizes for the raw gradient measure, and is warm started with the
result of spectral seriation (2-Sum problem) since Hahsler (2017) shows that
2-Sum solutions are similar to solutions for the gradient measure.
Use <code>warmstart = "random"</code> for no warm start.
</p>
<p>The initial temperature <code>t0</code> and minimum temperature <code>tmin</code> can be set. If
<code>t0</code> is not set, then it is estimated by sampling uphill moves and setting
<code>t0</code> such that the median uphill move have a probability
of <code>tinitialaccept</code>.
Using the cooling rate <code>cool</code>, the number of iterations
to go for <code>t0</code> to <code>tmin</code> is calculated.
</p>
<p>Several popular local neighborhood functions are
provided, and new ones can be defined (see LS). Local moves are tried in each
iteration <code>nlocal</code> times the number of objects.
</p>
<p>Note that this is an R implementation repeatedly calling the criterion funciton
which is very slow.
</p>
</li>
<li> <p><strong>Stochastic gradient descent:</strong> <code>"SGD"</code>
</p>
<p>Starts with a solution and then performs stochastic gradient descent to find
a close-by local optimum given a specified criterion.
</p>
<p>Important <code>control</code> parameters:
</p>

<ul>
<li> <p><code>"criterion"</code>: the criterion to optimize
</p>
</li>
<li> <p><code>"init"</code>: initial seriation (an order or the name of a seriation method)
</p>
</li>
<li> <p><code>"max_iter"</code>: number of trials
</p>
</li>
</ul>
</li>
<li> <p><strong>Spectral seriation:</strong> <code>"Spectral"</code>, <code>"Spectral_norm"</code>  (Ding and He, 2004)
</p>
<p>Spectral seriation uses a relaxation to minimize the <strong>2-Sum Problem</strong>
(Barnard, Pothen, and Simon, 1993). It uses the order of the Fiedler vector
of the similarity matrix's (normalized) Laplacian.
</p>
<p>Spectral seriation gives a good trade-off between seriation quality,
and scalability (see Hahsler, 2017).
</p>
</li>
<li> <p><strong>Traveling salesperson problem solver:</strong> <code>"TSP"</code>
</p>
<p>Uses a traveling salesperson problem solver to minimize the
<strong>Hamiltonian path length</strong>. The solvers in <span class="pkg">TSP</span> are used (see
<code>TSP::solve_TSP()</code>). The solver method can be passed on via the <code>control</code>
argument, e.g., <code>control = list(method = "two_opt")</code>. Default is the est
of 10 runs of arbitrary insertion heuristic with 2-opt improvement.
</p>
<p>Since a tour returned by a TSP solver is a connected circle and we are
looking for a path representing a linear order, we need to find the best
cutting point.  Climer and Zhang (2006) suggest adding a dummy city with
equal distance to each other city before generating the tour. The place of
this dummy city in an optimal tour with minimal length is the best cutting
point (it lies between the most distant cities).
</p>
</li>
</ul>
<p><strong>Other Methods</strong>
</p>

<ul>
<li> <p><strong>Identity permutation:</strong> '"Identity"
</p>
</li>
<li> <p><strong>Reverse Identity permutation:</strong> '"Reverse"
</p>
</li>
<li> <p><strong>Random permutation:</strong> <code>"Random"</code>
</p>
</li>
<li> <p><strong>Rank-two ellipse seriation:</strong> <code>"R2E"</code>  (Chen 2002)
</p>
<p>Rank-two ellipse seriation starts with generating a sequence of correlation matrices
<code class="reqn">R^1, R^2, \ldots</code>. <code class="reqn">R^1</code> is the correlation matrix of the original
distance matrix <code class="reqn">D</code> (supplied to the function as <code>x</code>), and
</p>
<p style="text-align: center;"><code class="reqn">R^{n+1} = \phi R^n,</code>
</p>
<p> where <code class="reqn">\phi</code> calculates the correlation
matrix.
</p>
<p>The rank of the matrix <code class="reqn">R^n</code> falls with increasing <code class="reqn">n</code>. The first
<code class="reqn">R^n</code> in the sequence, which has a rank of 2 is found. Projecting all
points in this matrix on the first two eigenvectors, all points fall on an
ellipse. The order of the points on this ellipse is the resulting order.
</p>
<p>The ellipse can be cut at the two interception points (top or bottom) of the
vertical axis with the ellipse. In this implementation, the topmost cutting
point is used.
</p>
</li>
<li> <p><strong>Sorting Points Into Neighborhoods:</strong> <code>"SPIN_STS"</code>, <code>"SPIN_NH"</code> (Tsafrir, 2005)
</p>
<p>Given a weight matrix <code class="reqn">W</code>, the SPIN algorithms try to
minimize the energy for a permutation (matrix <code class="reqn">P</code>) given by </p>
<p style="text-align: center;"><code class="reqn">F(P) =
  tr(PDP^TW),</code>
</p>
<p> where <code class="reqn">tr</code> denotes the matrix trace.
</p>
<p><code>"SPIN_STS"</code> implements the Side-to-Side algorithm, which tries to push
out large distance values. The default weight matrix suggested in the paper
with <code class="reqn">W=XX^T</code> and <code class="reqn">X_i=i-(n+1)/2</code> is used. We run the algorithm form
<code>step</code> (25) iteration and restart the algorithm <code>nstart</code> (10) with
random initial permutations (default values in parentheses).
</p>
<p><code>"SPIN_NH"</code> implements the neighborhood algorithm (concentrate low
distance values around the diagonal) with a Gaussian weight matrix
<code class="reqn">W_{ij} = exp(-(i-j)^2/n\sigma)</code>, where <code class="reqn">n</code> is the size of the
dissimilarity matrix and <code class="reqn">\sigma</code> is the variance around the diagonal
that control the influence of global (large <code class="reqn">\sigma</code>) or local (small
<code class="reqn">\sigma</code>) structure.
</p>
<p>We use the heuristic suggested in the paper for the linear assignment
problem. We do not terminate as indicated in the algorithm but run all the
iterations since the heuristic does not guarantee that the energy is
strictly decreasing. We also implement the heuristic "annealing" scheme
where <code class="reqn">\sigma</code> is successively reduced. The parameters in <code>control</code>
are <code>sigma</code> which can be a single value or a decreasing sequence
(default: 20 to 1 in 10 steps), and <code>step</code>, which defines how many update
steps are performed before for each value of <code>alpha</code>. Via
<code>W_function</code> a custom function to create <code class="reqn">W</code> with the function
signature <code style="white-space: pre;">⁠function(n, sigma, verbose)⁠</code> can be specified.
</p>
</li>
<li> <p><strong>Visual Assessment of (Clustering) Tendency:</strong> <code>"VAT"</code> (Bezdek and Hathaway, 2002).
</p>
<p>Creates an order based on Prim's algorithm for finding a minimum spanning
tree (MST) in a weighted connected graph representing the distance matrix.
The order is given by the order in which the nodes (objects) are added to
the MST.
</p>
</li>
</ul>
<p><strong>Seriation methods for matrices (matrix)</strong>
</p>
<p>Two-mode two-way data are general matrices.
Some methods also require that the matrix is positive.
Data frames and contingency tables (base::table)
are converted into a matrix. However, the
default methods are different.
</p>
<p>Some methods find the row and column order simultaneously,
while others calculate them independently.
Currently, the
following methods are implemented for <code>matrix</code>:
</p>
<p><strong>Seriating rows and columns simultaneously</strong>
</p>
<p>Row and column order influence each other.
</p>

<ul>
<li> <p><strong>Bond Energy Algorithm:</strong> <code>"BEA"</code>  (McCormick, 1972).
</p>
<p>The algorithm tries to maximize a non-negative matrix's
<strong>Measure of Effectiveness.</strong>
Due to the definition of this measure, the tasks of
ordering rows and columns are separable and can be solved independently.
</p>
<p>BEA consists of the following three steps:
</p>

<ol>
<li>
<p> Place one randomly chosen column.
</p>
</li>
<li>
<p> Try to place each remaining column at each possible position left,
right and between the already placed columns and
calculate every time the increase in ME. Choose the column and
position which gives the largest increase in ME and place the column.
Repeat till all columns are placed.
</p>
</li>
<li>
<p> Repeat procedure with rows.
</p>
</li>
</ol>
<p>The overall procedure
amounts to two approximate traveling salesperson problems (TSP)
where the distance is the -1 times the ME increase. The BEA algorithm
is equivalent to a simple suboptimal TSP heuristic called
'cheapest insertion'.
Several consecutive runs of the algorithm might improve the
energy if a better initial column/row is chosen.
</p>
<p>Arabie and Hubert (1990) question its use with non-binary data if
the objective is to find a seriation or one-dimensional orderings of rows
and columns.
</p>
</li>
<li> <p><strong>TSP to optimize the Measure of Effectiveness</strong>: <code>"BEA_TSP"</code> (Lenstra 1974).
</p>
<p>Since BEA is equivalent to a simple TSP heuristic, we can use better TSP
solvers to get better results.
Distances between rows are calculated for a <code class="reqn">M \times N</code> data matrix as
</p>
<p style="text-align: center;"><code class="reqn">d_{jk} = - \sum_{i=1}^{i=M} x_{ij}x_{ik}\ (j,k=0,1,...,N).</code>
</p>

<p>Distances
between columns are calculated the same way from the transposed data matrix.
</p>
<p>Solving the two TSP using these distances optimizes the measure of
effectiveness. With an exact TSP solver, the optimal solution
can be found.
</p>
<p><code>control</code> parameter:
</p>

<ul><li> <p><code>"method"</code>: a TSP solver method (see <code>TSP::solve_TSP()</code>).
</p>
</li></ul>
</li>
<li> <p><strong>Correspondence analysis</strong> <code>"CA"</code> (Friendly, 2023)
</p>
<p>This function is designed to help simplify a mosaic plot or other displays of a
matrix of frequencies.  It calculates a correspondence analysis of the matrix and
an order for rows and columns according to the scores on a correspondence analysis dimension.
</p>
<p>This is the default method for contingency tables.
</p>
<p><code>control</code> parameters:
</p>

<ul>
<li> <p><code>"dim"</code>: CA dimension used for reordering.
</p>
</li>
<li> <p><code>"ca_param"</code>: List with parameters for the call to <code>ca::ca()</code>.
</p>
</li>
</ul>
</li>
</ul>
<p><strong>Seriating rows and columns separately using dissimilarities</strong>
</p>

<ul><li> <p><strong>Heatmap seriation:</strong> <code>"Heatmap"</code>
</p>
<p>Calculates distances between
rows and between columns and then applies seriation so each. This is
the default method for data frames.
</p>
<p><code>control</code> parameter:
</p>

<ul>
<li> <p><code>"seriation_method"</code>: a list with row and column seriation methods.
The special method <code>"HC_Mean"</code> is available to use hierarchical clustering
with reordering the leaves by the row/column means (see <code>stats::heatmap()</code>).
Defaults to optimal leaf ordering <code>"OLO"</code>.
</p>
</li>
<li> <p><code>"seriation_control"</code>: a list with control parameters for row and column
seriation methods.
</p>
</li>
<li> <p><code>"dist_fun"</code>: specify the distance calculation as a function.
</p>
</li>
<li> <p><code>"scale"</code>: <code>"none"</code>, <code>"row"</code>, or <code>"col"</code>.
</p>
</li>
</ul>
</li></ul>
<p><strong>Seriate rows using the data matrix</strong>
</p>
<p>These methods need access to the data matrix instead of dissimilarities to
reorder objects (rows). Columns can also be reorderd by applying the same technique
to the transposed data matrix.
</p>

<ul><li> <p><strong>Order along the 1D locally linear embedding:</strong> <code>"LLE"</code>
</p>
</li></ul>
<p>Performs 1D the non-linear dimensionality reduction method locally linear embedding
(see <code>lle()</code>).
</p>

<ul>
<li> <p><strong>Order along the first principal component:</strong> <code>"PCA"</code>
</p>
<p>Uses the projection of the data on its first principal component (using
<code>stats::princomp()</code>) to
determine the order of rows. Performs the same procedure on the transposed
matrix to obtain the column order.
</p>
<p>Note that for a distance matrix calculated from <code>x</code> with Euclidean
distance, this method minimizes the least square criterion.
</p>
</li>
<li> <p><strong>Angular order of the first two PCA components:</strong> <code>"PCA_angle"</code>
</p>
<p>For rows, projects the data on the first two principal components
and then orders by the angle in this space. The order is split by the larges
gap between adjacent angles. A similar method was suggested by
Friendly (2002) to order variables in correlation matrices
by angles of first two eigenvectors. PCA also computes the eigenvectors
of the covariance matrix of the data.
</p>
<p>Performs the same process on the
transposed matrix for the column order.
</p>
</li>
</ul>
<p><strong>Other methods</strong>
</p>

<ul>
<li> <p><strong>Angular order of the first two eigenvectors:</strong> <code>"AOE"</code> (Friendly 2002)
</p>
<p>This method reordered correlation matrices by the angle in the space
spanned by the two largest eigenvectors of the matrix. The order is split
by the largest angle gap. This is the original method proposed by
Friendly (2002).
</p>
</li>
<li> <p><strong>By row/column mean:</strong> <code>"Mean"</code>
</p>
<p>A transformation can be applied before calculating the means.
The function is specified as control
parameter <code>"transformation"</code>. Any function that takes as an input a
matrix and returns the transformed matrix can be used. Examples
are <code>scale</code> or <code style="white-space: pre;">⁠\(x) x^.5⁠</code>.
</p>
</li>
<li> <p><strong>Identity permutation:</strong> <code>"Identity"</code>
</p>
</li>
<li> <p><strong>Reverse Identity permutation:</strong> <code>"Reverse"</code>
</p>
</li>
<li> <p><strong>Random permutation:</strong> <code>"Random"</code>
</p>
</li>
</ul>
<p>For <strong>general arrays</strong> no built-in methods are currently available.
</p>


<h3>Value</h3>

<p>Returns an object of class ser_permutation.
</p>


<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>Arabie, P. and L.J. Hubert (1990): The bond energy algorithm
revisited, <em>IEEE Transactions on Systems, Man, and Cybernetics,</em>
<strong>20</strong>(1), 268–274.
<a href="https://doi.org/10.1109/21.47829">doi:10.1109/21.47829</a>
</p>
<p>Bar-Joseph, Z., E. D. Demaine, D. K. Gifford, and T. Jaakkola. (2001): Fast
Optimal Leaf Ordering for Hierarchical Clustering. <em>Bioinformatics,</em>
<strong>17</strong>(1), 22–29.
<a href="https://doi.org/10.1093/bioinformatics/17.suppl_1.S22">doi:10.1093/bioinformatics/17.suppl_1.S22</a>
</p>
<p>Barnard, S. T., A. Pothen, and H. D. Simon (1993): A Spectral Algorithm for
Envelope Reduction of Sparse Matrices. <em>In Proceedings of the 1993
ACM/IEEE Conference on Supercomputing,</em> 493–502. Supercomputing '93. New
York, NY, USA: ACM. <a href="https://ieeexplore.ieee.org/document/1263497">https://ieeexplore.ieee.org/document/1263497</a>
</p>
<p>Bezdek, J.C. and Hathaway, R.J. (2002): VAT: a tool for visual assessment of
(cluster) tendency. <em>Proceedings of the 2002 International Joint
Conference on Neural Networks (IJCNN '02),</em> Volume: 3, 2225–2230.
<a href="https://doi.org/10.1109/IJCNN.2002.1007487">doi:10.1109/IJCNN.2002.1007487</a>
</p>
<p>Brusco, M., Koehn, H.F., and Stahl, S. (2008): Heuristic Implementation of
Dynamic Programming for Matrix Permutation Problems in Combinatorial Data
Analysis. <em>Psychometrika,</em> <strong>73</strong>(3), 503–522.
<a href="https://doi.org/10.1007/s11336-007-9049-5">doi:10.1007/s11336-007-9049-5</a>
</p>
<p>Brusco, M., and Stahl, S. (2005): <em>Branch-and-Bound Applications in
Combinatorial Data Analysis.</em> New York: Springer.
<a href="https://doi.org/10.1007/0-387-28810-4">doi:10.1007/0-387-28810-4</a>
</p>
<p>Chen, C. H. (2002): Generalized Association Plots: Information Visualization
via Iteratively Generated Correlation Matrices. <em>Statistica Sinica,</em>
<strong>12</strong>(1), 7–29.
</p>
<p>Ding, C. and Xiaofeng He (2004): Linearized cluster assignment via spectral
ordering. <em>Proceedings of the Twenty-first International Conference on
Machine learning (ICML '04)</em>.
<a href="https://doi.org/10.1145/1015330.1015407">doi:10.1145/1015330.1015407</a>
</p>
<p>Climer, S. and Xiongnu Zhang (2006): Rearrangement Clustering: Pitfalls,
Remedies, and Applications, <em>Journal of Machine Learning Research,</em>
<strong>7</strong>(Jun), 919–943.
</p>
<p>D. Earle, C. B. Hurley (2015): Advances in dendrogram seriation
for application to visualization.
<em>Journal of Computational and Graphical Statistics,</em> <strong>24</strong>(1), 1–25.
</p>
<p>Friendly, M. (2002): Corrgrams: Exploratory Displays for Correlation
Matrices. <em>The American Statistician,</em> <strong>56</strong>(4), 316–324.
<a href="https://doi.org/10.1198/000313002533">doi:10.1198/000313002533</a>
</p>
<p>Friendly, M. (2023). <em>vcdExtra: 'vcd' Extensions and Additions</em>. R
package version 0.8-5, <a href="https://CRAN.R-project.org/package=vcdExtra">https://CRAN.R-project.org/package=vcdExtra</a>.
</p>
<p>Gruvaeus, G. and Wainer, H. (1972): Two Additions to Hierarchical Cluster
Analysis, <em>British Journal of Mathematical and Statistical Psychology,</em>
<strong>25</strong>, 200–206.
<a href="https://doi.org/10.1111/j.2044-8317.1972.tb00491.x">doi:10.1111/j.2044-8317.1972.tb00491.x</a>
</p>
<p>Hahsler, M. (2017): An experimental comparison of seriation methods for
one-mode two-way data. <em>European Journal of Operational Research,</em>
<strong>257</strong>, 133–143.
<a href="https://doi.org/10.1016/j.ejor.2016.08.066">doi:10.1016/j.ejor.2016.08.066</a>
</p>
<p>Hubert, Lawrence, and James Schultz (1976): Quadratic Assignment as a
General Data Analysis Strategy. <em>British Journal of Mathematical and
Statistical Psychology,</em> <strong>29</strong>(2). Blackwell Publishing Ltd. 190–241.
<a href="https://doi.org/10.1111/j.2044-8317.1976.tb00714.x">doi:10.1111/j.2044-8317.1976.tb00714.x</a>
</p>
<p>Hurley, Catherine B. (2004): Clustering Visualizations of Multidimensional
Data. <em>Journal of Computational and Graphical Statistics,</em>
<strong>13</strong>(4), 788–806.
<a href="https://doi.org/10.1198/106186004X12425">doi:10.1198/106186004X12425</a>
</p>
<p>Kruskal, J.B. (1964). Nonmetric multidimensional scaling: a numerical method.
<em>Psychometrika,</em> <strong>29</strong>, 115–129.
</p>
<p>Lenstra, J.K (1974): Clustering a Data Array and the Traveling-Salesman
Problem, <em>Operations Research,</em> <strong>22</strong>(2) 413–414.
<a href="https://doi.org/10.1287/opre.22.2.413">doi:10.1287/opre.22.2.413</a>
</p>
<p>Mair P., De Leeuw J. (2015). Unidimensional scaling. In <em>Wiley
StatsRef: Statistics Reference Online,</em> Wiley, New York.
<a href="https://doi.org/10.1002/9781118445112.stat06462.pub2">doi:10.1002/9781118445112.stat06462.pub2</a>
</p>
<p>McCormick, W.T., P.J. Schweitzer and T.W. White (1972): Problem
decomposition and data reorganization by a clustering technique,
<em>Operations Research,</em> <strong>20</strong>(5), 993–1009.
<a href="https://doi.org/10.1287/opre.20.5.993">doi:10.1287/opre.20.5.993</a>
</p>
<p>Tenenbaum, J.B., de Silva, V. &amp; Langford, J.C. (2000)
A global network framework for nonlinear dimensionality reduction.
<em>Science</em> <strong>290</strong>, 2319-2323.
</p>
<p>Tsafrir, D., Tsafrir, I., Ein-Dor, L., Zuk, O., Notterman, D.A. and Domany,
E. (2005): Sorting points into neighborhoods (SPIN): data analysis and
visualization by ordering distance matrices, <em>Bioinformatics,</em>
<strong>21</strong>(10) 2301–8.
<a href="https://doi.org/10.1093/bioinformatics/bti329">doi:10.1093/bioinformatics/bti329</a>
</p>
<p>Sammon, J. W. (1969) A non-linear mapping for data structure analysis.
<em>IEEE Trans. Comput.</em>, <strong>C-18</strong> 401–409.
</p>


<h3>See Also</h3>

<p>Other seriation: 
<code>register_DendSer()</code>,
<code>register_GA()</code>,
<code>register_optics()</code>,
<code>register_smacof()</code>,
<code>register_tsne()</code>,
<code>register_umap()</code>,
<code>registry_for_seriaiton_methods</code>,
<code>seriate_best()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Show available seriation methods (for dist and matrix)
list_seriation_methods()

# show the description for ARSA
get_seriation_method("dist", name = "ARSA")

### Seriate as distance matrix (for 50 flowers from the iris dataset)
data("iris")
x &lt;- as.matrix(iris[-5])
x &lt;- x[sample(nrow(x), size = 50), ]
d &lt;- dist(x)

order &lt;- seriate(d)
order

pimage(d, main = "Distances (Random Order)")
pimage(d, order, main = "Distances (Reordered)")

# Compare seriation quality
rbind(
        random = criterion(d),
        reordered = criterion(d, order)
     )

# Reorder the distance matrix
d_reordered &lt;-  permute(d, order)
pimage(d_reordered, main = "Distances (Reordered)")


### Seriate a matrix (50 flowers from iris)

# To make the variables comparable, we scale the data
x &lt;- scale(x, center = FALSE)

# The iris flowers are ordered by species in the data set
pimage(x, main = "original data", prop = FALSE)
criterion(x)

# Apply some methods
order &lt;- seriate(x, method = "BEA_TSP")
pimage(x, order, main = "TSP to optimize ME", prop = FALSE)
criterion(x, order)

order &lt;- seriate(x, method = "PCA")
pimage(x, order, main = "First principal component", prop = FALSE)
criterion(x, order)

order &lt;- seriate(x, method = "heatmap")
pimage(x, order, main = "Heatmap seriation", prop = FALSE)
criterion(x, order)

# reorder the matrix
x_reordered &lt;- permute(x, order)

# create a heatmap seriation manually by calculating
# distances between rows and between columns
order &lt;- c(
    seriate(dist(x), method = "OLO"),
    seriate(dist(t(x)), method = "OLO")
)
pimage(x, order, main = "Heatmap seriation", prop = FALSE)
criterion(x, order)

### Seriate a correlation matrix
corr &lt;- cor(x)

# plot in original order
pimage(corr, main = "Correlation matrix")

# reorder the correlation matrix using the angle of eigenvectors
pimage(corr, order = "AOE", main = "Correlation matrix (AOE)")

# we can also define a distance (we used d = sqrt(1 - r)) and
# then reorder the matrix (rows and columns) using any seriation method.
d &lt;- as.dist(sqrt(1 - corr))
o &lt;- seriate(d, method = "R2E")
corr_reordered &lt;- permute(corr, order = c(o, o))
pimage(corr_reordered, main = "Correlation matrix (R2E)")
</code></pre>


</div>