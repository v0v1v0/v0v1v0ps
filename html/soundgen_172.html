<div class="container">

<table style="width: 100%;"><tr>
<td>naiveBayes_likelihood</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Naive Bayes likelihood</h2>

<h3>Description</h3>

<p>Internal soundgen function
</p>


<h3>Usage</h3>

<pre><code class="language-R">naiveBayes_likelihood(
  d,
  nObs = nrow(d),
  mod_train,
  class_names,
  nClasses = length(class_names),
  like_names,
  predictors,
  nPredictors = length(predictors)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>dataframe containing the observations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nObs</code></td>
<td>
<p>the number of observations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mod_train</code></td>
<td>
<p>the output of naiveBayes_train()</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class_names</code></td>
<td>
<p>names of outcome classes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nClasses</code></td>
<td>
<p>the number of outcome classes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>like_names</code></td>
<td>
<p>the names of variables holding likelihoods</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictors</code></td>
<td>
<p>the names of predictor variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nPredictors</code></td>
<td>
<p>the number of predicto variables</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A Helper function called by <code>naiveBayes</code> to calculate the
likelihood of each observation. Algorithm: for each predictor and class, the
likelihood is dnorm(observation, mean_per_class, sd_per_class). I tried
non-Gaussian probability distributions (Student's t to accommodate outliers),
but Gaussian actually seems to be more robust.
</p>


</div>