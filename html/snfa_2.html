<div class="container">

<table style="width: 100%;"><tr>
<td>fit.boundary</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multivariate smooth boundary fitting with additional constraints</h2>

<h3>Description</h3>

<p>Fits boundary of data with kernel smoothing, imposing monotonicity and/or concavity constraints.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fit.boundary(X.eval, y.eval, X.bounded, y.bounded, X.constrained = NA,
  X.fit = NA, y.fit.observed = NA, H.inv = NA, H.mult = 1,
  method = "u", scale.constraints = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.eval</code></td>
<td>
<p>Vector of outputs used for fitting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.bounded</code></td>
<td>
<p>Matrix of inputs where bounding constraints apply</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.bounded</code></td>
<td>
<p>Vector of outputs where bounding constraints apply</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where monotonicity/concavity constraints apply</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit; defaults to X.constrained</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.fit.observed</code></td>
<td>
<p>Vector of outputs corresponding to observations in X.fit; used for efficiency calculation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H.inv</code></td>
<td>
<p>Inverse of the smoothing matrix (must be positive definite); defaults to rule of thumb</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H.mult</code></td>
<td>
<p>Scaling factor for rule of thumb smoothing matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Constraints to apply; "u" for unconstrained, "m" for monotonically increasing, and "mc" for monotonically increasing and concave</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale.constraints</code></td>
<td>
<p>Boolean, whether to scale constraints by their average value, can help with convergence</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This method fits a smooth boundary of the data (with all data points below the boundary)
while imposing specified monotonicity and concavity constraints. The procedure is
derived from Racine et al. (2009), which develops kernel smoothing methods with
bounding, monotonicity and concavity constraints. Specifically, the smoothing procedure
involves finding optimal weights for a Nadaraya-Watson estimator of the form 
</p>
<p style="text-align: center;"><code class="reqn">\hat{y} = m(x) = \sum_{i=1}^N p_i A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">x</code> are inputs, <code class="reqn">y</code> are outputs, <code class="reqn">p</code> are weights, subscripts
index observations, and 
</p>
<p style="text-align: center;"><code class="reqn">A(x, x_i) = \frac{K(x, x_i)}{\sum_{h=1}^N K(x, x_h)}</code>
</p>

<p>for a kernel <code class="reqn">K</code>. This method uses a multivariate normal kernel of the form
</p>
<p style="text-align: center;"><code class="reqn">K(x, x_h) = \exp\left(-\frac12 (x - x_h)'H^{-1}(x - x_h)\right),</code>
</p>

<p>where <code class="reqn">H</code> is a bandwidth matrix. Bandwidth selection is performed via Silverman's
(1986) rule-of-thumb, in the function <code>H.inv.select</code>.
</p>
<p>Optimal weights <code class="reqn">\hat{p}</code> are selected by solving the quadratic programming problem
</p>
<p style="text-align: center;"><code class="reqn">\min_p \mbox{\ \ }-\mathbf{1}'p + \frac12 p'p.</code>
</p>

<p>This method always imposes bounding constraints as specified points, given by
</p>
<p style="text-align: center;"><code class="reqn">m(x_i) - y_i = \sum_{h=1}^N p_h A(x_i, x_h) y_h - y_i \geq 0 \mbox{\ \ \ \ }\forall i.</code>
</p>

<p>Additionally, monotonicity constraints of the following form can be imposed at 
specified points:
</p>
<p style="text-align: center;"><code class="reqn">\frac{\partial m(x)}{\partial x^j} = \sum_{h=1}^N p_h \frac{\partial A(x, x_h)}{\partial x^j} y_h \geq 0 \mbox{\ \ \ \ }\forall x, j,</code>
</p>

<p>where superscripts index inputs. Finally concavity constraints of the following form can also be imposed using Afriat's
(1967) conditions:
</p>
<p style="text-align: center;"><code class="reqn">m(x) - m(z) \leq \nabla_x m(z) \cdot (x - z) \mbox{\ \ \ \ }\forall x, z.</code>
</p>

<p>The gradient of the frontier at a point <code class="reqn">x</code> is given by 
</p>
<p style="text-align: center;"><code class="reqn">\nabla_x m(x) = \sum_{i=1}^N \hat{p}_i \nabla_x A(x, x_i) y_i,</code>
</p>

<p>where <code class="reqn">\hat{p}_i</code> are estimated weights.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements
</p>
<table>
<tr style="vertical-align: top;">
<td><code>y.fit</code></td>
<td>
<p>Estimated value of the frontier at X.fit</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradient.fit</code></td>
<td>
<p>Estimated gradient of the frontier at X.fit</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>efficiency</code></td>
<td>
<p>Estimated efficiencies of y.fit.observed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>solution</code></td>
<td>
<p>Boolean; TRUE if frontier successfully estimated</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.eval</code></td>
<td>
<p>Matrix of inputs used for fitting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.constrained</code></td>
<td>
<p>Matrix of inputs where monotonicity/concavity constraints apply</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X.fit</code></td>
<td>
<p>Matrix of inputs where curve is fit</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>H.inv</code></td>
<td>
<p>Inverse smoothing matrix used in fitting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Method used to fit frontier</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scaling.factor</code></td>
<td>
<p>Factor by which constraints are multiplied before quadratic programming</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Racine JS, Parmeter CF, Du P (2009).
“Constrained nonparametric kernel regression: Estimation and inference.”
Working paper.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(univariate)

#Set up data for fitting

X &lt;- as.matrix(univariate$x)
y &lt;- univariate$y

N.fit &lt;- 100
X.fit &lt;- as.matrix(seq(min(X), max(X), length.out = N.fit))

#Reflect data for fitting
reflected.data &lt;- reflect.data(X, y)
X.eval &lt;- reflected.data$X
y.eval &lt;- reflected.data$y

#Fit frontiers
frontier.u &lt;- fit.boundary(X.eval, y.eval, 
                           X.bounded = X, y.bounded = y,
                           X.constrained = X.fit,
                           X.fit = X.fit,
                           method = "u")
                          
frontier.m &lt;- fit.boundary(X.eval, y.eval, 
                           X.bounded = X, y.bounded = y,
                           X.constrained = X.fit,
                           X.fit = X.fit,
                           method = "m")
                          
frontier.mc &lt;- fit.boundary(X.eval, y.eval, 
                            X.bounded = X, y.bounded = y,
                            X.constrained = X.fit,
                            X.fit = X.fit,
                            method = "mc")

#Plot frontier
library(ggplot2)

frontier.df &lt;- data.frame(X = rep(X.fit, times = 3),
                          y = c(frontier.u$y.fit, frontier.m$y.fit, frontier.mc$y.fit),
                          model = rep(c("u", "m", "mc"), each = N.fit))

ggplot(univariate, aes(X, y)) +
  geom_point() +
  geom_line(data = frontier.df, aes(color = model))

#Plot slopes
slope.df &lt;- data.frame(X = rep(X.fit, times = 3),
                       slope = c(frontier.u$gradient.fit,
                                 frontier.m$gradient.fit,
                                 frontier.mc$gradient.fit),
                       model = rep(c("u", "m", "mc"), each = N.fit))

ggplot(slope.df, aes(X, slope)) +
  geom_line(aes(color = model))
  
</code></pre>


</div>