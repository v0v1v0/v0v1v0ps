<div class="container">

<table style="width: 100%;"><tr>
<td>ml_gbt_classifier</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Spark ML – Gradient Boosted Trees</h2>

<h3>Description</h3>

<p>Perform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ml_gbt_classifier(
  x,
  formula = NULL,
  max_iter = 20,
  max_depth = 5,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  min_instances_per_node = 1L,
  max_bins = 32,
  min_info_gain = 0,
  loss_type = "logistic",
  seed = NULL,
  thresholds = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = random_string("gbt_classifier_"),
  ...
)

ml_gradient_boosted_trees(
  x,
  formula = NULL,
  type = c("auto", "regression", "classification"),
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  checkpoint_interval = 10,
  loss_type = c("auto", "logistic", "squared", "absolute"),
  max_bins = 32,
  max_depth = 5,
  max_iter = 20L,
  min_info_gain = 0,
  min_instances_per_node = 1,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  seed = NULL,
  thresholds = NULL,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  uid = random_string("gradient_boosted_trees_"),
  response = NULL,
  features = NULL,
  ...
)

ml_gbt_regressor(
  x,
  formula = NULL,
  max_iter = 20,
  max_depth = 5,
  step_size = 0.1,
  subsampling_rate = 1,
  feature_subset_strategy = "auto",
  min_instances_per_node = 1,
  max_bins = 32,
  min_info_gain = 0,
  loss_type = "squared",
  seed = NULL,
  checkpoint_interval = 10,
  cache_node_ids = FALSE,
  max_memory_in_mb = 256,
  features_col = "features",
  label_col = "label",
  prediction_col = "prediction",
  uid = random_string("gbt_regressor_"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>Maxmimum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_depth</code></td>
<td>
<p>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p>Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsampling_rate</code></td>
<td>
<p>Fraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>feature_subset_strategy</code></td>
<td>
<p>The number of features to consider for splits at each tree node. See details for options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_instances_per_node</code></td>
<td>
<p>Minimum number of instances each child must
have after split.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_bins</code></td>
<td>
<p>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. More bins give higher granularity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_info_gain</code></td>
<td>
<p>Minimum information gain for a split to be considered
at a tree node. Should be &gt;= 0, defaults to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss_type</code></td>
<td>
<p>Loss function which GBT tries to minimize. Supported: <code>"squared"</code> (L2) and <code>"absolute"</code> (L1) (default = squared) for regression and <code>"logistic"</code> (default) for classification. For <code>ml_gradient_boosted_trees</code>, setting <code>"auto"</code>
will default to the appropriate loss type based on model type.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Seed for random numbers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thresholds</code></td>
<td>
<p>Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable
checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10
iterations, defaults to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cache_node_ids</code></td>
<td>
<p>If <code>FALSE</code>, the algorithm will pass trees to
executors to match instances with nodes. If <code>TRUE</code>, the algorithm will
cache node IDs for each instance. Caching can speed up training of deeper
trees. Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_memory_in_mb</code></td>
<td>
<p>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration, and its aggregates
may exceed this size. Defaults to 256.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code>ft_r_formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label_col</code></td>
<td>
<p>Label column name. The column should be a numeric column. Usually this column is output by <code>ft_r_formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prediction_col</code></td>
<td>
<p>Prediction column name.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>probability_col</code></td>
<td>
<p>Column name for predicted class conditional probabilities.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>raw_prediction_col</code></td>
<td>
<p>Raw prediction (a.k.a. confidence) column name.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional arguments; see Details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>The type of model to fit. <code>"regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. When <code>"auto"</code> is used, the model type is
inferred based on the response variable type – if it is a numeric type,
then regression is used; classification otherwise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response</code></td>
<td>
<p>(Deprecated) The name of the response column (as a length-one character vector.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>features</code></td>
<td>
<p>(Deprecated) The name of features (terms) to use for the model fit.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The supported options for <code>feature_subset_strategy</code> are
</p>

<ul>
<li> <p><code>"auto"</code>: Choose automatically for task: If <code>num_trees == 1</code>, set to <code>"all"</code>. If <code>num_trees &gt; 1</code> (forest), set to <code>"sqrt"</code> for classification and to <code>"onethird"</code> for regression.
</p>
</li>
<li> <p><code>"all"</code>: use all features
</p>
</li>
<li> <p><code>"onethird"</code>: use 1/3 of the features
</p>
</li>
<li> <p><code>"sqrt"</code>: use use sqrt(number of features)
</p>
</li>
<li> <p><code>"log2"</code>: use log2(number of features)
</p>
</li>
<li> <p><code>"n"</code>: when <code>n</code> is in the range (0, 1.0], use n * number of features. When <code>n</code> is in the range (1, number of features), use <code>n</code> features. (default = <code>"auto"</code>)
</p>
</li>
</ul>
<p><code>ml_gradient_boosted_trees</code> is a wrapper around <code>ml_gbt_regressor.tbl_spark</code> and <code>ml_gbt_classifier.tbl_spark</code> and calls the appropriate method based on model type.
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>. If it is a
<code>spark_connection</code>, the function returns a <code>ml_estimator</code> object. If
it is a <code>ml_pipeline</code>, it will return a pipeline with the predictor
appended to it. If a <code>tbl_spark</code>, it will return a <code>tbl_spark</code> with
the predictions added to it.
</p>


<h3>See Also</h3>

<p>Other ml algorithms: 
<code>ml_aft_survival_regression()</code>,
<code>ml_decision_tree_classifier()</code>,
<code>ml_generalized_linear_regression()</code>,
<code>ml_isotonic_regression()</code>,
<code>ml_linear_regression()</code>,
<code>ml_linear_svc()</code>,
<code>ml_logistic_regression()</code>,
<code>ml_multilayer_perceptron_classifier()</code>,
<code>ml_naive_bayes()</code>,
<code>ml_one_vs_rest()</code>,
<code>ml_random_forest_classifier()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
sc &lt;- spark_connect(master = "local")
iris_tbl &lt;- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

gbt_model &lt;- iris_training %&gt;%
  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)

pred &lt;- ml_predict(gbt_model, iris_test)

ml_regression_evaluator(pred, label_col = "Sepal_Length")

## End(Not run)

</code></pre>


</div>