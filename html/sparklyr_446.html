<div class="container">

<table style="width: 100%;"><tr>
<td>spark_read_binary</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Read binary data into a Spark DataFrame.</h2>

<h3>Description</h3>

<p>Read binary files within a directory and convert each file into a record
within the resulting Spark dataframe. The output will be a Spark dataframe
with the following columns and possibly partition columns:
</p>

<ul>
<li>
<p> path: StringType
</p>
</li>
<li>
<p> modificationTime: TimestampType
</p>
</li>
<li>
<p> length: LongType
</p>
</li>
<li>
<p> content: BinaryType
</p>
</li>
</ul>
<h3>Usage</h3>

<pre><code class="language-R">spark_read_binary(
  sc,
  name = NULL,
  dir = name,
  path_glob_filter = "*",
  recursive_file_lookup = FALSE,
  repartition = 0,
  memory = TRUE,
  overwrite = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dir</code></td>
<td>
<p>Directory to read binary files from.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path_glob_filter</code></td>
<td>
<p>Glob pattern of binary files to be loaded
(e.g., "*.jpg").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recursive_file_lookup</code></td>
<td>
<p>If FALSE (default), then partition discovery
will be enabled (i.e., if a partition naming scheme is present, then
partitions specified by subdirectory names such as "date=2019-07-01" will
be created and files outside subdirectories following a partition naming
scheme will be ignored). If TRUE, then all nested directories will be
searched even if their names do not follow a partition naming scheme.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p>Other Spark serialization routines: 
<code>collect_from_rds()</code>,
<code>spark_insert_table()</code>,
<code>spark_load_table()</code>,
<code>spark_read()</code>,
<code>spark_read_avro()</code>,
<code>spark_read_csv()</code>,
<code>spark_read_delta()</code>,
<code>spark_read_image()</code>,
<code>spark_read_jdbc()</code>,
<code>spark_read_json()</code>,
<code>spark_read_libsvm()</code>,
<code>spark_read_orc()</code>,
<code>spark_read_parquet()</code>,
<code>spark_read_source()</code>,
<code>spark_read_table()</code>,
<code>spark_read_text()</code>,
<code>spark_save_table()</code>,
<code>spark_write_avro()</code>,
<code>spark_write_csv()</code>,
<code>spark_write_delta()</code>,
<code>spark_write_jdbc()</code>,
<code>spark_write_json()</code>,
<code>spark_write_orc()</code>,
<code>spark_write_parquet()</code>,
<code>spark_write_source()</code>,
<code>spark_write_table()</code>,
<code>spark_write_text()</code>
</p>


</div>