<div class="container">

<table style="width: 100%;"><tr>
<td>spark_write_tfrecord</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Write a Spark DataFrame to a TFRecord file</h2>

<h3>Description</h3>

<p>Serialize a Spark DataFrame to the TensorFlow TFRecord format for
training or inference.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spark_write_tfrecord(x, path, record_type = c("Example",
  "SequenceExample"), write_locality = c("distributed", "local"),
  mode = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A Spark DataFrame</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the "hdfs://", "s3a://", and "file://" protocols.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>record_type</code></td>
<td>
<p>Output format of TensorFlow records. One of <code>"Example"</code> and 
<code>"SequenceExample"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>write_locality</code></td>
<td>
<p>Determines whether the TensorFlow records are
written locally on the workers or on a distributed file system. One of
<code>"distributed"</code> and <code>"local"</code>. See Details for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mode</code></td>
<td>
<p>A <code>character</code> element. Specifies the behavior when data or
table already exists. Supported values include: 'error', 'append', 'overwrite' and
'ignore'. Notice that 'overwrite' will also change the column structure.
</p>
<p>For more details see also <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
for your version of Spark.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For <code>write_locality = local</code>, each of the workers stores on the
local disk a subset of the data. The subset that is stored on each worker
is determined by the partitioning of the DataFrame. Each of the partitions
is coalesced into a single TFRecord file and written on the node where the
partition lives. This is useful in the context of distributed training, in which
each of the workers gets a subset of the data to work on. When this mode is
activated, the path provided to the writer is interpreted as a base path
that is created on each of the worker nodes, and that will be populated with data
from the DataFrame.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
iris_tbl &lt;- copy_to(sc, iris)
data_path &lt;- file.path(tempdir(), "iris")
df1 &lt;- iris_tbl %&gt;%
ft_string_indexer_model(
  "Species", "label",
  labels = c("setosa", "versicolor", "virginica")
)

df1 %&gt;%
spark_write_tfrecord(
  path = data_path,
  write_locality = "local"
)

## End(Not run)
</code></pre>


</div>