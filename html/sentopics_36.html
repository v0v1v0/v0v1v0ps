<div class="container">

<table style="width: 100%;"><tr>
<td>LDA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create a Latent Dirichlet Allocation model</h2>

<h3>Description</h3>

<p>This function initialize a Latent Dirichlet Allocation model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LDA(x, K = 5, alpha = 1, beta = 0.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>tokens object containing the texts. A coercion will be attempted if <code>x</code> is not a tokens.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>the number of topics</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the hyperparameter of topic-document distribution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>the hyperparameter of vocabulary distribution</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>rJST.LDA</code> methods enable the transition from a previously
estimated LDA model to a sentiment-aware <code>rJST</code> model. The function
retains the previously estimated topics and randomly assigns sentiment to
every word of the corpus. The new model will retain the iteration count of
the initial LDA model.
</p>


<h3>Value</h3>

<p>An S3 list containing the model parameter and the estimated mixture.
This object corresponds to a Gibbs sampler estimator with zero iterations.
The MCMC can be iterated using the <code>fit()</code>
function.
</p>

<ul>
<li> <p><code>tokens</code> is the tokens object used to create the model
</p>
</li>
<li> <p><code>vocabulary</code> contains the set of words of the corpus
</p>
</li>
<li> <p><code>it</code> tracks the number of Gibbs sampling iterations
</p>
</li>
<li> <p><code>za</code> is the list of topic assignment, aligned to the <code>tokens</code> object with
padding removed
</p>
</li>
<li> <p><code>logLikelihood</code> returns the measured log-likelihood at each iteration,
with a breakdown of the likelihood into hierarchical components as
attribute
</p>
</li>
</ul>
<p>The <code>topWords()</code> function easily extract the most probables words of each
topic/sentiment.
</p>


<h3>Author(s)</h3>

<p>Olivier Delmarcelle
</p>


<h3>References</h3>

<p>Blei, D.M., Ng, A.Y. and Jordan, M.I. (2003). <a href="http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf">Latent Dirichlet Allocation</a>.
<em>Journal of Machine Learning Research</em>, 3, 993â€“1022.
</p>


<h3>See Also</h3>

<p>Fitting a model: <code>fit()</code>, extracting
top words: <code>topWords()</code>
</p>
<p>Other topic models: 
<code>JST()</code>,
<code>rJST()</code>,
<code>sentopicmodel()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># creating a model
LDA(ECB_press_conferences_tokens, K = 5, alpha = 0.1, beta = 0.01)

# estimating an LDA model
lda &lt;- LDA(ECB_press_conferences_tokens)
lda &lt;- fit(lda, 100)
</code></pre>


</div>