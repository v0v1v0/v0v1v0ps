<div class="container">

<table style="width: 100%;"><tr>
<td>selfTraining</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>General Interface for Self-training model</h2>

<h3>Description</h3>

<p>Self-training is a simple and effective semi-supervised
learning classification method. The self-training classifier is initially
trained with a reduced set of labeled examples. Then it is iteratively retrained
with its own most confident predictions over the unlabeled examples.
Self-training follows a wrapper methodology using a base supervised
classifier to establish the possible class of unlabeled instances.
</p>


<h3>Usage</h3>

<pre><code class="language-R">selfTraining(learner, max.iter = 50, perc.full = 0.7, thr.conf = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>learner</code></td>
<td>
<p>model from parsnip package for training a supervised base classifier
using a set of instances. This model need to have probability predictions
(or optionally a distance matrix) and it's corresponding classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>maximum number of iterations to execute the self-labeling process.
Default is 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perc.full</code></td>
<td>
<p>A number between 0 and 1. If the percentage
of new labeled examples reaches this value the self-training process is stopped.
Default is 0.7.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thr.conf</code></td>
<td>
<p>A number between 0 and 1 that indicates the confidence threshold.
At each iteration, only the newly labelled examples with a confidence greater than
this value (<code>thr.conf</code>) are added to the training set.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For predicting the most accurate instances per iteration, <code>selfTraining</code>
uses the predictions obtained with the learner specified. To train a model
using the <code>learner</code> function, it is required a set of instances
(or a precomputed matrix between the instances if <code>x.inst</code> parameter is <code>FALSE</code>)
in conjunction with the corresponding classes.
Additionals parameters are provided to the <code>learner</code> function via the
<code>learner.pars</code> argument. The model obtained is a supervised classifier
ready to predict new instances through the <code>pred</code> function.
Using a similar idea, the additional parameters to the <code>pred</code> function
are provided using the <code>pred.pars</code> argument. The <code>pred</code> function returns
the probabilities per class for each new instance. The value of the
<code>thr.conf</code> argument controls the confidence of instances selected
to enlarge the labeled set for the next iteration.
</p>
<p>The stopping criterion is defined through the fulfillment of one of the following
criteria: the algorithm reaches the number of iterations defined in the <code>max.iter</code>
parameter or the portion of the unlabeled set, defined in the <code>perc.full</code> parameter,
is moved to the labeled set. In some cases, the process stops and no instances
are added to the original labeled set. In this case, the user must assign a more
flexible value to the <code>thr.conf</code> parameter.
</p>


<h3>Value</h3>

<p>(When model fit) A list object of class "selfTraining" containing:
</p>

<dl>
<dt>model</dt>
<dd>
<p>The final base classifier trained using the enlarged labeled set.</p>
</dd>
<dt>instances.index</dt>
<dd>
<p>The indexes of the training instances used to
train the <code>model</code>. These indexes include the initial labeled instances
and the newly labeled instances.
Those indexes are relative to <code>x</code> argument.</p>
</dd>
<dt>classes</dt>
<dd>
<p>The levels of <code>y</code> factor.</p>
</dd>
<dt>pred</dt>
<dd>
<p>The function provided in the <code>pred</code> argument.</p>
</dd>
<dt>pred.pars</dt>
<dd>
<p>The list provided in the <code>pred.pars</code> argument.</p>
</dd>
</dl>
<h3>References</h3>

<p>David Yarowsky.<br><em>Unsupervised word sense disambiguation rivaling supervised methods.</em><br>
In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,
pages 189-196. Association for Computational Linguistics, 1995.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(tidyverse)
library(tidymodels)
library(caret)
library(SSLR)

data(wine)

set.seed(1)
train.index &lt;- createDataPartition(wine$Wine, p = .7, list = FALSE)
train &lt;- wine[ train.index,]
test  &lt;- wine[-train.index,]

cls &lt;- which(colnames(wine) == "Wine")

#% LABELED
labeled.index &lt;- createDataPartition(train$Wine, p = .2, list = FALSE)
train[-labeled.index,cls] &lt;- NA

#We need a model with probability predictions from parsnip
#https://tidymodels.github.io/parsnip/articles/articles/Models.html
#It should be with mode = classification

#For example, with Random Forest
rf &lt;-  rand_forest(trees = 100, mode = "classification") %&gt;%
  set_engine("randomForest")


m &lt;- selfTraining(learner = rf,
                  perc.full = 0.7,
                  thr.conf = 0.5, max.iter = 10) %&gt;% fit(Wine ~ ., data = train)

#Accuracy
predict(m,test) %&gt;%
  bind_cols(test) %&gt;%
  metrics(truth = "Wine", estimate = .pred_class)

</code></pre>


</div>